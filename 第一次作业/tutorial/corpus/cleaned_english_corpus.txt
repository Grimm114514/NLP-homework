the coordination of autonomous agents in dynamic environments is hampered by the semantic gap between highlevel mission objectives and lowlevel planner inputs to address this we introduce a framework centered on a knowledge graph kg that functions as an intelligent translation layer the kgs twoplane architecture compiles declarative facts into peragent missionaware worldviews and physicsaware traversal rules decoupling mission semantics from a domainagnostic planner this allows complex coordinated paths to be modified simply by changing facts in the kg a case study involving autonomous underwater vehicles auvs in the gulf of mexico visually demonstrates the endtoend process and quantitatively proves that different declarative policies produce distinct highperforming outcomes this work establishes the kg not merely as a recent studies operationalize selfimprovement through coding agents that edit their own codebases they grow a tree of selfmodifications through expansion strategies that favor higher software engineering benchmark performance assuming that this implies more promising subsequent selfmodifications however we identify a mismatch between the agents selfimprovement potential metaproductivity and its coding benchmark performance namely the metaproductivityperformance mismatch inspired by huxleys concept of clade we propose a metric mathrmcmp that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for selfimprovement we show that in our selfimproving coding agent development setting access to the true mathrmcmp is sufficient to simulate how the gdel machine would behave under certain assumptions we introduce the huxleygdel machine hgm which by estimating mathrmcmp and using it as guidance searches the tree of selfmodifications on swebench verified and polyglot hgm outperforms prior selfimproving coding agent development methods while using less wallclock time last but not least hgm demonstrates strong transfer to other coding datasets and large language safety is a fundamental requirement for autonomous systems operating in critical domains control barrier functions cbfs have been used to design safety filters that minimally alter nominal controls for such systems to maintain their safety learning neural cbfs has been proposed as a large language recent studies operationalize selfimprovement through coding agents that edit their own codebases they grow a tree of selfmodifications through expansion strategies that favor higher software engineering benchmark performance assuming that this implies more promising subsequent selfmodifications however we identify a mismatch between the agents selfimprovement potential metaproductivity and its coding benchmark performance namely the metaproductivityperformance mismatch inspired by huxleys concept of clade we propose a metric mathrmcmp that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for selfimprovement we show that in our selfimproving coding agent development setting access to the true mathrmcmp is sufficient to simulate how the gdel machine would behave under certain assumptions we introduce the huxleygdel machine hgm which by estimating mathrmcmp and using it as guidance searches the tree of selfmodifications on swebench verified and polyglot hgm outperforms prior selfimproving coding agent development methods while using less wallclock time last but not least hgm demonstrates strong transfer to other coding datasets and large language safety is a fundamental requirement for autonomous systems operating in critical domains control barrier functions cbfs have been used to design safety filters that minimally alter nominal controls for such systems to maintain their safety learning neural cbfs has been proposed as a large language recent advances in large language this paper presents a first empirical study of agentic ai as autonomous decisionmakers in decentralized governance using more than 3k proposals from major protocols we build an agentic ai voter that interprets proposal contexts retrieves historical deliberation reasoning has become a central paradigm for large language recent studies operationalize selfimprovement through coding agents that edit their own codebases they grow a tree of selfmodifications through expansion strategies that favor higher software engineering benchmark performance assuming that this implies more promising subsequent selfmodifications however we identify a mismatch between the agents selfimprovement potential metaproductivity and its coding benchmark performance namely the metaproductivityperformance mismatch inspired by huxleys concept of clade we propose a metric mathrmcmp that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for selfimprovement we show that in our selfimproving coding agent development setting access to the true mathrmcmp is sufficient to simulate how the gdel machine would behave under certain assumptions we introduce the huxleygdel machine hgm which by estimating mathrmcmp and using it as guidance searches the tree of selfmodifications on swebench verified and polyglot hgm outperforms prior selfimproving coding agent development methods while using less wallclock time last but not least hgm demonstrates strong transfer to other coding datasets and large language safety is a fundamental requirement for autonomous systems operating in critical domains control barrier functions cbfs have been used to design safety filters that minimally alter nominal controls for such systems to maintain their safety learning neural cbfs has been proposed as a large language recent advances in large language reasoning has become a central paradigm for large language modeling the inherent hierarchical structure of 3d objects and 3d scenes is highly desirable as it enables a more holistic understanding of environments for autonomous agents accomplishing this with implicit representations such as neural radiance fields remains an unexplored challenge existing methods that explicitly we propose the multimodal untrimmed video retrieval task along with a new benchmark muvr to advance video retrieval for longvideo platforms muvr aims to retrieve untrimmed videos containing relevant segments using multimodal queries it has the following features 1 practical retrieval paradigm muvr supports videocentric multimodal queries expressing finegrained retrieval needs through long text descriptions video tag prompts and mask prompts it adopts a onetomany retrieval paradigm and focuses on untrimmed videos tailored for longvideo platform applications 2 multilevel visual correspondence to cover common video categories eg news travel dance and precisely define retrieval matching criteria we construct multilevel visual correspondence based on core video content eg news events travel locations dance moves which users are interested in and want to retrieve it covers six levels copy event scene instance action and others 3 comprehensive evaluation criteria we develop 3 versions of muvr ie base filter qa muvrbasefilter evaluates retrieval image stitching synthesizes images captured from multiple perspectives into a single image with a broader field of view the significant variations in object depth often lead to large parallax resulting in ghosting and misalignment in the stitched results to address this we propose a depthconsistencyconstrained seamlessfree image stitching method first to tackle the multiview alignment difficulties caused by parallax a multistage mechanism combined with global depth regularization constraints is developed to enhance the alignment accuracy of the same apparent target across different depth ranges second during the multiview image fusion process an optimal stitching seam is determined through graphbased lowcost computation and a softseam region is diffused to precisely locate transition areas thereby effectively mitigating alignment errors induced by parallax and achieving natural and seamless stitching results furthermore considering the computational overhead in the shift regression process a reparameterization strategy is incorporated to optimize the structural design significantly improving algorithm efficiency while maintaining optimal performance extensive experiments demonstrate the superior performance of the proposed method against the existing methods remote sensing vision tasks require extensive labeled we participated in the synthrad2025 challenge tasks 1 and 2 with a unified pipeline for synthetic ct sct generation from mri and cbct implemented using the konfai framework our uav tracking can be widely applied in scenarios such as disaster rescue environmental monitoring and logistics transportation however existing uav tracking methods predominantly emphasize speed and lack exploration in semantic awareness which hinders the search region from extracting accurate localization information from the template the limitation results in suboptimal performance under typical uav tracking challenges such as camera motion fast motion and low resolution etc to address this issue we propose a dynamic semantic aware correlation modeling tracking framework the core of our framework is a dynamic semantic relevance generator which in combination with the correlation map from the transformer explore semantic relevance the approach enhances the search regions ability to extract important information from the template improving accuracy and robustness under the aforementioned challenges additionally to enhance the tracking speed we design a pruning method for the proposed framework therefore we present multiple in complex orchard environments the phenotypic heterogeneity of different apple leaf diseases characterized by significant variation among lesions poses a challenge to traditional multiscale feature fusion methods these methods only integrate multilayer features extracted by convolutional neural networks cnns and fail to adequately account for the relationships between local and global features therefore this study proposes a multibranch recognition framework named cnntransformerclip ctclip the framework synergistically employs a cnn to extract local lesion detail features and a vision transformer to capture global structural relationships an adaptive feature fusion module affm then dynamically fuses these features achieving optimal coupling of local and global information and effectively addressing the diversity in lesion morphology and distribution additionally to mitigate interference from complex backgrounds and significantly enhance recognition accuracy under fewshot conditions this study proposes a multimodal imagetext learning approach by leveraging pretrained clip weights it achieves deep alignment between visual features and disease semantic descriptions experimental results show that ctclip achieves accuracies of 9738 and 9612 on a publicly available apple disease and a selfbuilt dataset outperforming several baseline methods the proposed ctclip demonstrates strong capabilities in recognizing agricultural diseases significantly enhances identification accuracy under complex environmental conditions provides an innovative and practical solution for automated disease recognition in agricultural applications understanding how cells respond to external stimuli is a central challenge in biomedical research and drug development current computational frameworks for modelling cellular responses remain restricted to twodimensional representations limiting their capacity to capture the complexity of cell morphology under perturbation this dimensional constraint poses a critical bottleneck for the development of accurate virtual cell multimodal large language this paper presents a first empirical study of agentic ai as autonomous decisionmakers in decentralized governance using more than 3k proposals from major protocols we build an agentic ai voter that interprets proposal contexts retrieves historical deliberation autonomous web agents powered by large language with the widespread use of large language we address the challenge of adopting language can large language despite the rapid expansion of large language root cause analysis rca is a crucial aspect of incident management in largescale cloud services while the term root cause analysis or rca has been widely used different studies formulate the task differently this is because the term rca implicitly covers tasks with distinct underlying goals for instance the goal of localizing a faulty service for rapid triage is fundamentally different from identifying a specific functional bug for a definitive fix however previous surveys have largely overlooked these goalbased distinctions conventionally categorizing papers by input combinatorial optimization problems are central to both practical applications and the development of optimization methods while classical and quantum algorithms have been refined over decades machine learningassisted approaches are comparatively recent and have not yet consistently outperformed simple stateoftheart classical methods here we focus on a class of quadratic unconstrained binary optimization qubo problems specifically the challenge of finding minimum energy configurations in threedimensional ising spin glasses we use a global annealing monte carlo algorithm that integrates standard local moves with global moves proposed via machine learning we show that local moves play a crucial role in achieving optimal performance benchmarking against simulated annealing and population annealing we demonstrate that global annealing not only surpasses the performance of simulated annealing but also exhibits greater robustness than population annealing maintaining effectiveness across problem hardness and system size without hyperparameter tuning these results provide to our knowledge the first clear and robust evidence that a machine learningassisted optimization method can exceed the capabilities of classical stateoftheart techniques in a combinatorial optimization setting ai methods are increasingly shaping pharmaceutical drug discovery however their translation to industrial applications remains limited due to their reliance on public datasets lacking scale and diversity of proprietary pharmaceutical ondevice virtual assistants like siri and google assistant are increasingly pivotal yet their capabilities are hamstrung by a reliance on rigid developerdependent apis gui agents offer a powerful apiindependent alternative but their adoption is hindered by the perception of poor performance as even the best with climate change intensifying urban waterlogging poses an increasingly severe threat to global public safety and infrastructure however existing monitoring approaches rely heavily on manual reporting and fail to provide timely and comprehensive assessments in this study we present urban waterlogging assessment uwassess a foundation visionlanguage the rapid evolution of agentic ai marks a new phase in artificial intelligence where large language current approaches to enhancing llm reasoning follows two isolated paradigms monitorgenerate methods like planandsolve wang et al 2023 and selfdiscover zhou et al 2024 excel at strategic planning but lack mechanisms to verify whether selected strategies succeed while generateverify approaches like selfverification weng et al 2022 and selfrefine madaan et al 2023 iteratively refine outputs but commence generation blindly without task assessment this separation creates inefficiencies strategies fail without feedback and refinement occurs without strategic grounding we address this gap by implementing flavells cognitive monitoring reinforcement learning rl has become a predominant technique to align language while recent sound event detection sed systems can identify baleen whale calls in marine audio challenges related to false positive and minorityclass detection persist we propose the boundary proposal network bpn which extends an existing lightweight sed system the bpn is inspired by work in image object detection and aims to reduce the number of false positive detections it achieves this by using intermediate latent representations computed within the backbone classification in this paper we explore semantic clustering properties of deep reinforcement learning drl to improve its interpretability and deepen our understanding of its internal semantic organization in this context semantic clustering refers to the ability of neural networks to cluster inputs based on their semantic similarity in the feature space we propose a drl architecture that incorporates a novel semantic clustering module that combines feature dimensionality reduction with online clustering this module integrates seamlessly into the drl training pipeline addressing the instability of tsne and eliminating the need for extensive manual annotation inherent to prior semantic analysis methods we experimentally validate the effectiveness of the proposed module and demonstrate its ability to reveal semantic clustering properties within drl furthermore we introduce new analytical methods based on these properties to provide insights into the hierarchical structure of policies and semantic organization within the feature space our inference in both brains and machines can be formalized by optimizing a shared objective maximizing the evidence lower bound elbo in machine learning or minimizing variational free energy f in neuroscience elbo f while this equivalence suggests a unifying framework it leaves open how inference is implemented in neural systems here we introduce fond free energy online naturalgradient dynamics a framework that derives neural inference dynamics from three principles 1 natural gradients on f 2 online belief updating and 3 iterative refinement we apply fond to derive ipvae iterative poisson variational autoencoder a recurrent spiking neural network that performs variational inference through membrane potential dynamics replacing amortized encoders with iterative inference updates theoretically ipvae yields several desirable features such as emergent normalization via lateral competition and hardwareefficient integer spike count representations empirically ipvae outperforms both standard vaes and gaussianbased predictive coding in recent years large language formal reasoning and automated theorem proving constitute a challenging subfield of machine learning in which machines are tasked with proving mathematical theorems using formal languages like lean a formal verification system can check whether a formal proof is correct or not almost instantaneously but generating a completely correct formal proof with large language llmbased agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine robotics and programming however maintaining these systems requires substantial effort as they are inevitably prone to bugs and continually evolve to meet changing external requirements therefore automatically resolving agent issues ie bug reports or feature requests is a crucial and challenging task while recent software engineering se agents eg sweagent have shown promise in addressing issues in traditional software systems it remains unclear how effectively they can resolve realworld issues in agent systems which differ significantly from traditional software to fill this gap we first manually analyze 201 realworld agent issues and identify common categories of agent issues we then spend 500 personhours constructing agentissuebench a reproducible benchmark comprising 50 agent issue resolution tasks each with an executable environment and failuretriggering tests we further evaluate stateoftheart se agents on agentissuebench and reveal their limited effectiveness ie with only 067 467 resolution rates these results underscore the unique challenges of maintaining agent systems compared to traditional software highlighting the need for further research to develop advanced se agents for resolving agent issues large language large language large language models llms excel in coderelated tasks like code generation but benchmark evaluations often overlook task characteristics such as difficulty moreover benchmarks are