the coordination of autonomous agents in dynamic environments is hampered by the semantic gap between highlevel mission objectives and lowlevel planner inputs to address this we introduce a framework centered on a knowledge graph kg that functions as an intelligent translation layer the kgs twoplane architecture compiles declarative facts into peragent missionaware worldviews and physicsaware traversal rules decoupling mission semantics from a domainagnostic planner this allows complex coordinated paths to be modified ply by changing facts in the kg a case study involving autonomous underwater vehicles auvs in the gulf of mexico visually demonstrates the endtoend process and quantitatively proves that different declarative policies produce distinct highperforming outcomes this work establishes the kg not merely as a data repository but as a powerful stateful orchestrator for creating adaptive and explainable autonomous systems
companies spend large amounts of money on public relations campaigns to project a positive brand image however sometimes there is a mismatch between what they say and what they do oil gas companies for example are accused of greenwashing with imagery of climatefriendly initiatives understanding the framing and changes in framing at scale can help better understand the goals and nature of public relations campaigns to address this we introduce a benchmark dataset of expertannotated video ads obtained from facebook and youtube the dataset provides annotations for framing types for more than companies or advocacy groups across countries our dataset is especially designed for the evaluation of visionlanguage models vlms distinguishing it from past textonly framing datasets baseline experiments show some promising results while leaving room for improvement for future work gpt can detect environmental messages with f score while our best model only achieves f score on identifying framing around green innovation we also identify challenges that vlms must address such as implicit framing handling videos of various lengths or implicit cultural backgrounds our dataset contributes to research in multimodal analysis of strategic communication in the energy sector
constructing comprehensive knowledge graphs requires the use of multiple ontologies in order to fully contextualize data into a domain ontology matching finds equivalences between concepts interconnecting ontologies and creating a cohesive semantic layer while the ple pairwise state of the art is well established ple equivalence mappings cannot provide full semantic integration of related but disjoint ontologies complex multiontology matching cmom aligns one source entity to composite logical expressions of multiple target entities establishing more nuanced equivalences and provenance along the ontological hierarchy we present cmomgen the first endtoend cmom strategy that generates complete and semantically sound mappings without establishing any restrictions on the number of target ontologies or entities retrievalaugmented generation selects relevant classes to compose the mapping and filters matching reference mappings to serve as examples enhancing incontext learning the strategy was evaluated in three biomedical tasks with partial reference alignments cmomgen outperforms baselines in class selection demonstrating the impact of having a dedicated strategy our strategy also achieves a minimum of in fscore outperforming all baselines and ablated versions in two out of three tasks and placing second in the third furthermore a manual evaluation of nonreference mappings showed that of the mappings achieve the maximum score further substantiating its ability to construct semantically sound mappings
ai agents hold the potential to revolutionize scientific productivity by automating literature reviews replicating experiments analyzing data and even proposing new directions of inquiry indeed there are now many such agents ranging from generalpurpose deep research systems to specialized sciencespecific agents such as ai scientist and aigs rigorous evaluation of these agents is critical for progress yet existing benchmarks fall short on several fronts they fail to provide holistic productinformed measures of realworld use cases such as science research lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities do not account for confounding variables such as model cost and tool access do not provide standardized interfaces for quick agent prototyping and evaluation and lack comprehensive baseline agents necessary to identify true advances in response we define principles and tooling for more rigorously benchmarking agents using these we present astabench a suite that provides the first holistic measure of agentic ability to perform scientific research comprising problems spanning the entire scientific discovery process and multiple scientific domains and including many problems inspired by actual user requests to deployed asta agents our suite comes with the first scientific research environment with productiongrade search tools that enable controlled reproducible evaluation better accounting for confounders alongside we provide a comprehensive suite of nine scienceoptimized classes of asta agents and numerous baselines our extensive evaluation of agents across agent classes reveals several interesting findings most importantly that despite meaningful progress on certain individual aspects ai remains far from solving the challenge of science research assistance
large reasoning models have demonstrated strong problemsolving abilities yet realworld tasks often require external tools and longhorizon interactions existing agent frameworks typically follow predefined workflows which limit autonomous and global task completion in this paper we introduce deepagent an endtoend deep reasoning agent that performs autonomous thinking tool discovery and action execution within a single coherent reasoning process to address the challenges of longhorizon interactions particularly the context length explosion from multiple tool calls and the accumulation of interaction history we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic working and tool memories reducing error accumulation while preserving critical information to teach generalpurpose tool use efficiently and stably we develop an endtoend reinforcement learning strategy namely toolpo that leverages llmulated apis and applies toolcall advantage attribution to assign finegrained credit to the tool invocation tokens extensive experiments on eight benchmarks including general tooluse tasks toolbench apibank tmdb spotify toolhop and downstream applications alfworld webshop gaia hle demonstrate that deepagent consistently outperforms baselines across both labeledtool and openset tool retrieval scenarios this work takes a step toward more general and capable agents for realworld applications the code and demo are available at
recent studies operationalize selfimprovement through coding agents that edit their own codebases they grow a tree of selfmodifications through expansion strategies that favor higher software engineering benchmark performance assuming that this implies more promising subsequent selfmodifications however we identify a mismatch between the agents selfimprovement potential metaproductivity and its coding benchmark performance namely the metaproductivityperformance mismatch inspired by huxleys concept of clade we propose a metric cmp that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for selfimprovement we show that in our selfimproving coding agent development setting access to the true cmp is sufficient to ulate how the gdel machine would behave under certain assumptions we introduce the huxleygdel machine hgm which by estimating cmp and using it as guidance searches the tree of selfmodifications on swebench verified and polyglot hgm outperforms prior selfimproving coding agent development methods while using less wallclock time last but not least hgm demonstrates strong transfer to other coding datasets and large language models the agent optimized by hgm on swebench verified with gptmini and evaluated on swebench lite with gpt achieves humanlevel performance matching the best officially checked results of humanengineered coding agents our code is available at
safety is a fundamental requirement for autonomous systems operating in critical domains control barrier functions cbfs have been used to design safety filters that minimally alter nominal controls for such systems to maintain their safety learning neural cbfs has been proposed as a datadriven alternative for their computationally expensive optimizationbased synthesis however it is often the case that the failure set of states that should be avoided is nonobvious or hard to specify formally eg tailgating in autonomous driving while a set of expert demonstrations that achieve the task and avoid the failure set is easier to generate we use icl to train a constraint function that classifies the states of the system under consideration to safe ie belong to a controlled forward invariant set that is disjoint from the unspecified failure set and unsafe ones ie belong to the complement of that set we then use that function to label a new set of ulated trajectories to train our neural cbf we empirically evaluate our approach in four different environments demonstrating that it outperforms existing baselines and achieves comparable performance to a neural cbf trained with the same data but annotated with groundtruth safety labels
longhorizon reasoning in llmbased agents often fails not from generative weakness but from insufficient verification of intermediate reasoning cosight addresses this challenge by turning reasoning into a falsifiable and auditable process through two complementary mechanisms conflictaware metaverification camv and trustworthy reasoning with structured facts trsf camv reformulates verification as conflict identification and targeted falsification allocating computation only to disagreement hotspots among expert agents rather than to full reasoning chains this bounds verification cost to the number of inconsistencies and improves efficiency and reliability trsf continuously organizes validates and synchronizes evidence across agents through a structured facts module by maintaining verified traceable and auditable knowledge it ensures that all reasoning is grounded in consistent sourceverified information and supports transparent verification throughout the reasoning process together trsf and camv form a closed verification loop where trsf supplies structured facts and camv selectively falsifies or reinforces them yielding transparent and trustworthy reasoning empirically cosight achieves stateoftheart accuracy on gaia and humanitys last exam and strong results on chinesepleqa ablation studies confirm that the synergy between structured factual grounding and conflictaware verification drives these improvements cosight thus offers a scalable paradigm for reliable longhorizon reasoning in llmbased agents code is available at
large language models llms are increasingly deployed as agents in various contexts by providing tools at their disposal however llm agents can exhibit unpredictable behaviors including taking undesirable andor unsafe actions in order to measure the latent propensity of llm agents for taking illegal actions under an eu legislative context we introduce euagentbench a verifiable humancurated benchmark that evaluates an agents alignment with eu legal norms in situations where benign user inputs could lead to unlawful actions our benchmark spans scenarios across several categories including data protection biasdiscrimination and scientific integrity with each user request allowing for both compliant and noncompliant execution of the requested actions comparing the models function calls against a rubric exhaustively supported by citations of the relevant legislature we evaluate the legal compliance of frontier llms and furthermore investigate the compliance effect of providing the relevant legislative excerpts in the agents system prompt along with explicit instructions to comply we release a public preview set for the research community while holding out a private test set to prevent data contamination in evaluating upcoming models we encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multiturn and multilingual interactions we release our code on href this url
existing neural methods for multitask vehicle routing problems vrps typically learn unified solvers to handle multiple constraints ultaneously however they often underutilize the compositional structure of vrp variants each derivable from a common set of basis vrp variants this critical oversight causes unified solvers to miss out the potential benefits of basis solvers each specialized for a basis vrp variant to overcome this limitation we propose a framework that enables unified solvers to perceive the sharedcomponent nature across vrp variants by proactively reusing basis solvers while mitigating the exponential growth of trained neural solvers specifically we introduce a statedecomposable mdp sdmdp that reformulates vrps by expressing the state space as the cartesian product of basis state spaces associated with basis vrp variants more crucially this formulation inherently yields the optimal basis policy for each basis vrp variant furthermore a latent spacebased sdmdp extension is developed by incorporating both the optimal basis policies and a learnable mixture function to enable the policy reuse in the latent space under mild assumptions this extension provably recovers the optimal unified policy of sdmdp through the mixture function that computes the state embedding as a mapping from the basis state embeddings generated by optimal basis policies for practical implementation we introduce the mixtureofspecializedexperts solver moses which realizes basis policies through specialized lowrank adaptation lora experts and implements the mixture function via an adaptive gating mechanism extensive experiments conducted across vrp variants showcase the superiority of moses over prior methods
this study presents autooptk a unique image dataset of over handwritten and printed mathematical optimization models corresponding to singleobjective multiobjective multilevel and stochastic optimization problems exhibiting various types of complexities such as nonlinearity nonconvexity nondifferentiability discontinuity and highdimensionality the labels consist of the latex representation for all the images and modeling language representation for a subset of images the dataset is created by experts following ethical data creation guidelines and verified in twophases to avoid errors further we develop autoopt framework a machine learning based automated approach for solving optimization problems where the user just needs to provide an image of the formulation and autoopt solves it efficiently without any further human intervention autoopt framework consists of three modules i m imagetotext a deep learning model performs the mathematical expression recognition mer task to generate the latex code corresponding to the optimization formulation in image ii m texttotext a smallscale finetuned llm generates the pyomo script optimization modeling language from latex code iii m optimization a bilevel optimization based decomposition bobd method solves the optimization formulation described in the pyomo script we use autooptk dataset for training and testing of deep learning models employed in autoopt the deep learning model for mer task m outperforms chatgpt gemini and nougat on bleu score metric bobd method m which is a hybrid approach yields better results on complex test problems compared to common approaches like interiorpoint algorithm and genetic algorithm
llms have demonstrated highly effective learning humanlike response generationand decisionmaking capabilities in highrisk sectors however these models remain black boxes because they struggle to ensure transparency in responses the literature has explored numerous approaches to address transparency challenges in llms including neurosymbolic ai nesy ai nesy ai approaches were primarily developed for conventional neural networks and are not wellsuited to the unique features of llms consequently there is a limited systematic understanding of how symbolic ai can be effectively integrated into llms this paper aims to address this gap by first reviewing established nesy ai methods and then proposing a novel taxonomy of symbolic integration in llms along with a roadmap to merge symbolic techniques with llms the roadmap introduces a new categorisation framework across four dimensions by organising existing literature within these categories these include symbolic integration across various stages of llm coupling mechanisms architectural paradigms as well as algorithmic and applicationlevel perspectives the paper thoroughly identifies current benchmarks cuttingedge advancements and critical gaps within the field to propose a roadmap for future research by highlighting the latest developments and notable gaps in the literature it offers practical insights for implementing frameworks for symbolic integration into llms to enhance transparency
testtime scaling methods have seen a rapid increase in popularity for its computational efficiency and parameterindependent training to improve reasoning performance on large language models one such method is called budget forcing a decoding intervention strategy which allocates extra compute budget for thinking and elicits the inherent selfcorrecting behavior of the model however this relies on supervised finetuning sft on longcontext reasoning traces which causes performance degradation on smaller models due to verbose responses for this reason we offer a framework integrating reinforcement learning rl to improve token efficiency and boost the performance of a b model for mathematical reasoning we demonstrate this using only k training samples and found that our sftrl model performed better on the gsmk dataset with varying compute budgets our main findings showed an overall higher accuracy while significantly reducing its token usage by over compared to the sft model revealing how rl can recover the losses due to longcontext training and altogether improving performance in mathematical reasoning
large language models llms often struggle with generating truly innovative ideas typically defaulting to highprobability familiar concepts within their training datas gravity wells while advanced searchbased methods like tree of thoughts tot attempt to mitigate this they are fundamentally limited by their reliance on unprincipled inconsistent selfevaluation heuristics to guide exploration to address this gap we introduce magellan a novel framework that reframes creative generation as a principled guided exploration of an llms latent conceptual space at its core magellan employs monte carlo tree search mcts governed by a hierarchical guidance system for longrange direction a semantic compass vector formulated via orthogonal projection steers the search towards relevant novelty for local stepbystep decisions a landscapeaware value function replaces flawed selfevaluation with an explicit reward structure that balances intrinsic coherence extrinsic novelty and narrative progress extensive experiments demonstrate that magellan significantly outperforms strong baselines including react and tot in generating scientific ideas with superior plausibility and innovation our work shows that for creative discovery a principled guided search is more effective than unconstrained agency paving the way for llms to become more capable partners in innovation
chest xray cxr plays a pivotal role in clinical diagnosis and a variety of taskspecific and foundation models have been developed for automatic cxr interpretation however these models often struggle to adapt to new diagnostic tasks and complex reasoning scenarios recently llmbased agent models have emerged as a promising paradigm for cxr analysis enhancing models capability through tool coordination multistep reasoning and team collaboration etc however existing agents often rely on a single diagnostic pipeline and lack mechanisms for assessing tools reliability limiting their adaptability and credibility to this end we propose cxragent a directororchestrated multistage agent for cxr interpretation where a central director coordinates the following stages tool invocation the agent strategically orchestrates a set of cxranalysis tools with outputs normalized and verified by the evidencedriven validator edv which grounds diagnostic outputs with visual evidence to support reliable downstream diagnosis diagnostic planning guided by task requirements and intermediate findings the agent formulates a targeted diagnostic plan it then assembles an expert team accordingly defining member roles and coordinating their interactions to enable adaptive and collaborative reasoning collaborative decisionmaking the agent integrates insights from the expert team with accumulated contextual memories synthesizing them into an evidencebacked diagnostic conclusion experiments on various cxr interpretation tasks show that cxragent delivers strong performance providing visual evidence and generalizes well to clinical tasks of different complexity code and data are valuable at this href link
recent advances in large language models llms have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots demonstrating the potential of llmbased embodied intelligence however these llmbased codeaspolicies approaches often suffer from limited environmental grounding particularly in dynamic or partially observable settings leading to suboptimal task success rates due to incorrect or incomplete code generation in this work we propose a neurosymbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation in the validation phase the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving taskrelevant states this integrated process enhances the grounding of generated code resulting in improved task reliability and success rates in complex environments we evaluate our framework on rlbench and in realworld settings across dynamic partially observable scenarios experimental results demonstrate that our framework improves task success rates by over codeaspolicies baselines and attains over executability of taskrelevant actions thereby enhancing the reliability of task planning in dynamic environments
background trustworthy ai serves as a foundational pillar for two major ai ethics conferences aies and facct however current research often adopts technocentric approaches focusing primarily on technical attributes such as reliability robustness and fairness while overlooking the sociotechnical dimensions critical to understanding ai trustworthiness in realworld contexts objectives this scoping review aims to examine how the aies and facct communities conceptualize measure and validate ai trustworthiness identifying major gaps and opportunities for advancing a holistic understanding of trustworthy ai systems methods we conduct a scoping review of aies and facct conference proceedings to date systematically analyzing how trustworthiness is defined operationalized and applied across different research domains our analysis focuses on conceptualization approaches measurement methods verification and validation techniques application areas and underlying values results while significant progress has been made in defining technical attributes such as transparency accountability and robustness our findings reveal critical gaps current research often predominantly asizes technical precision at the expense of social and ethical considerations the sociotechnical nature of ai systems remains less explored and trustworthiness emerges as a contested concept shaped by those with the power to define it conclusions an interdisciplinary approach combining technical rigor with social cultural and institutional considerations is essential for advancing trustworthy ai we propose actionable measures for the ai ethics community to adopt holistic frameworks that genuinely address the complex interplay between ai systems and society ultimately promoting responsible technological development that benefits all stakeholders
large reasoning models lrms demonstrate remarkable capabilities on complex reasoning tasks but remain vulnerable to severe safety risks including harmful content generation and jailbreak attacks existing mitigation strategies rely on injecting heuristic safety signals during training which often suppress reasoning ability and fail to resolve the safetyreasoning tradeoff to systematically investigate this issue we analyze the reasoning trajectories of diverse lrms and uncover a phenomenon we term selfjailbreak where models override their own risk assessments and justify responding to unsafe prompts this finding reveals that lrms inherently possess the ability to reject unsafe queries but this ability is compromised resulting in harmful outputs building on these insights we propose the chainofguardrail cog a training framework that recomposes or backtracks unsafe reasoning steps steering the model back onto safe trajectories while preserving valid reasoning chains extensive experiments across multiple reasoning and safety benchmarks demonstrate that cog substantially improves the safety of current lrms while preserving comparable reasoning ability significantly outperforming prior methods that suffer from severe safetyreasoning tradeoffs
the upper confidence bounds for trees uct algorithm is not agnostic to the reward scale of the game it is applied to for zerosum games with the sparse rewards of at the end of the game this is not a problem but many games often feature dense rewards with handpicked reward scales causing a nodes qvalue to span different magnitudes across different games in this paper we evaluate various strategies for adaptively choosing the uct exploration constant lambda called lambdastrategies that are agnostic to the games reward scale these lambdastrategies include those proposed in the literature as well as five new strategies given our experimental results we recommend using one of our newly suggested lambdastrategies which is to choose lambda as cdot sigma where sigma is the empirical standard deviation of all stateaction pairs qvalues of the search tree this method outperforms existing lambdastrategies across a wide range of tasks both in terms of a single parameter value and the peak performances obtained by optimizing all available parameters
the operational capabilities and application domains of aienabled autonomous systems have expanded significantly in recent years due to advances in robotics and machine learning ml demonstrating the safety of autonomous systems rigorously is critical for their responsible adoption but it is challenging as it requires robust methodologies that can handle novel and uncertain situations throughout the system lifecycle including detecting outofdistribution ood data thus ood detection is receiving increased attention from the research development and safety engineering communities this comprehensive review analyses ood detection techniques within the context of safety assurance for autonomous systems in particular in safetycritical domains we begin by defining the relevant concepts investigating what causes ood and exploring the factors which make the safety assurance of autonomous systems and ood detection challenging our review identifies a range of techniques which can be used throughout the ml development lifecycle and we suggest areas within the lifecycle in which they may be used to support safety assurance arguments we discuss a number of caveats that system and safety engineers must be aware of when integrating ood detection into system lifecycles we conclude by outlining the challenges and future work necessary for the safe development and operation of autonomous systems across a range of domains and applications
we propose outboundeval a comprehensive benchmark for evaluating large language models llms in expertlevel intelligent outbound calling scenarios unlike existing methods that suffer from three key limitations insufficient dataset diversity and category coverage unrealistic user ulation and inaccurate evaluation metrics outboundeval addresses these issues through a structured framework first we design a benchmark spanning six major business domains and representative subscenarios each with scenariospecific process decomposition weighted scoring and domainadaptive metrics second we develop a largemodeldriven user ulator that generates diverse personarich virtual users with realistic behaviors emotional variability and communication styles providing a controlled yet authentic testing environment third we introduce a dynamic evaluation method that adapts to task variations integrating automated and humanintheloop assessment to measure task execution accuracy professional knowledge application adaptability and user experience quality experiments on stateoftheart llms reveal distinct tradeoffs between expertlevel task completion and interaction fluency offering practical insights for building reliable humanlike outbound ai systems outboundeval establishes a practical extensible and domainoriented standard for benchmarking llms in professional applications
causal relationship discovery has been drawing increasing attention due to its prevalent application existing methods rely on human experience statistical methods or graphical criteria methods which are errorprone stuck at the idealized assumption and rely on a huge amount of data and there is also a serious data gap in accessing multivariate time seriesmts in many areas adding difficulty in finding their causal relationship existing methods are easy to be overfitting on them to fill the gap we mentioned above in this paper we propose shylock a novel method that can work well in both fewshot and normal mts to find the causal relationship shylock can reduce the number of parameters exponentially by using group dilated convolution and a sharing kernel but still learn a better representation of variables with time delay by combing the global constraint and the local constraint shylock achieves information sharing among networks to help improve the accuracy to evaluate the performance of shylock we also design a data generation method to generate mts with time delay we evaluate it on commonly used benchmarks and generated datasets extensive experiments show that shylock outperforms two existing stateofart methods on both fewshot and normal mts we also developed tcausal a library for easy use and deployed it on the earthdataminer platform
pretrained visionlanguage models vlms such as clip have demonstrated remarkable zeroshot generalization enabling deployment in a wide range of realworld tasks without additional taskspecific training however in real deployment scenarios with evolving environments or emerging classes these models inevitably face distributional shifts and novel tasks in such contexts static zeroshot capabilities are insufficient and there is a growing need for continual learning methods that allow models to adapt over time while avoiding catastrophic forgetting we introduce nusacl null space adaptation for continual learning a lightweight memoryfree continual learning framework designed to address this challenge nusacl employs lowrank adaptation and constrains taskspecific weight updates to lie within an imate null space of the models current parameters this strategy minimizes interference with previously acquired knowledge effectively preserving the zeroshot capabilities of the original model unlike methods relying on replay buffers or costly distillation nusacl imposes minimal computational and memory overhead making it practical for deployment in resourceconstrained realworld continual learning environments experiments show that our framework not only effectively preserves zeroshot transfer capabilities but also achieves highly competitive performance on continual learning benchmarks these results position nusacl as a practical and scalable solution for continually evolving zeroshot vlms in realworld applications
we introduce string seed of thought ssot a novel prompting method for llms that improves probabilistic instruction following pif we define pif as a task requiring an llm to select its answer from a predefined set of options each associated with a specific probability such that the empirical distribution of the generated answers aligns with the target distribution when prompted multiple times while llms excel at tasks with single deterministic answers they often fail at pif exhibiting biases problematic for applications requiring nondeterministic behaviors such as humanbehavior ulation content diversification and multiplayer games it also harms the diversity of generated responses a crucial factor in testtime scaling by causing the outputs to collapse into a limited set of answers to address this we propose ssot a ple prompting method that instructs an llm to first output a random string to generate sufficient entropy ssot also instructs the llm to extract randomness by manipulating this string to derive a final answer thereby preserving diversity while adhering to specific constraints we demonstrate that ssot significantly improves the pif performance of llms approaching the ideal performance of a pseudorandom number generator furthermore our experiments on noveltybench show ssots benefits extend beyond closedset tasks to openended tasks by enhancing response diversity
designing optimal prompts and reasoning processes for large language models llms on domainspecific tasks is both necessary and challenging in realworld applications determining how to integrate domain knowledge enhance reasoning efficiency and even provide domain experts with refined knowledge integration hints are particularly crucial yet unresolved tasks in this research we propose evolutionary graph optimization for prompting egoprompt an automated framework to designing better prompts efficient reasoning processes and providing enhanced causalinformed process egoprompt begins with a general prompt and faulttolerant initial semantic causal graph scg descriptions constructed by human experts which is then automatically refined and optimized to guide llm reasoning recognizing that expertdefined scgs may be partial or imperfect and that their optimal integration varies across llms egoprompt integrates a novel causalguided textual gradient process in two steps first generating nearly deterministic reasoning guidance from the scg for each instance and second adapting the llm to effectively utilize the guidance alongside the original input the iterative optimization algorithm further refines both the scg and the reasoning mechanism using textual gradients with groundtruth we tested the framework on realworld public health transportation and human behavior tasks egoprompt achieves higher f than cuttingedge methods and allows small models to reach the performence of larger models at under of the original cost it also outputs a refined domainspecific scg that improves interpretability
retrievalaugmented generation rag empowers large language models llms to dynamically integrate external knowledge during inference improving their factual accuracy and adaptability however adversaries can inject poisoned external knowledge to override the models internal memory while existing attacks iteratively manipulate retrieval content or prompt structure of rag they largely ignore the models internal representation dynamics and neuronlevel sensitivities the underlying mechanism of rag poisoning has not been fully studied and the effect of knowledge conflict with strong parametric knowledge in rag is not considered in this work we propose neurogenpoisoning a novel attack framework that generates adversarial external knowledge in rag guided by llm internal neuron attribution and genetic optimization our method first identifies a set of poisonresponsive neurons whose activation strongly correlates with contextual poisoning knowledge we then employ a genetic algorithm to evolve adversarial passages that maximally activate these neurons crucially our framework enables massivescale generation of effective poisoned rag knowledge by identifying and reusing promising but initially unsuccessful external knowledge variants via observed attribution signals at the same time poisonresponsive neurons guided poisoning can effectively resolves knowledge conflict experimental results across models and datasets demonstrate consistently achieving high population overwrite success rate posr of over while preserving fluency empirical evidence shows that our method effectively resolves knowledge conflict
panic attacks are acute episodes of fear and distress in which timely appropriate intervention can significantly help individuals regain stability however suitable datasets for training such models remain scarce due to ethical and logistical issues to address this we introduce pace which is a dataset that includes highdistress episodes constructed from firstperson narratives and structured around the principles of psychological first aid pfa using this data we train pacer a counseling model designed to provide both empathetic and directive support which is optimized through supervised learning and ulated preference alignment to assess its effectiveness we propose paniceval a multidimensional framework covering general counseling quality and crisisspecific strategies experimental results show that pacer outperforms strong baselines in both counselorside metrics and client affect improvement human evaluations further confirm its practical value with pacer consistently preferred over general cbtbased and gptpowered models in panic scenarios code is available at
this paper presents a first empirical study of agentic ai as autonomous decisionmakers in decentralized governance using more than k proposals from major protocols we build an agentic ai voter that interprets proposal contexts retrieves historical deliberation data and independently determines its voting position the agent operates within a realistic financial ulation environment grounded in verifiable blockchain data implemented through a modular composable program mcp workflow that defines data flow and tool usage via agentics framework we evaluate how closely the agents decisions align with the human and tokenweighted outcomes uncovering strong alignments measured by carefully designed evaluation metrics our findings demonstrate that agentic ai can augment collective decisionmaking by producing interpretable auditable and empirically grounded signals in realistic dao governance settings the study contributes to the design of explainable and economically rigorous ai agents for decentralized financial systems
a key task in artificial intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures offpolicy learning methods like qlearning allow learners to make optimal decisions based on past experiences this paper studies offpolicy learning from biased data in complex and highdimensional domains where unobserved confounding cannot be ruled out a priori building on the wellcelebrated deep qnetwork dqn we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data specifically our algorithm attempts to find a safe policy for the worstcase environment compatible with the observations we apply our method to twelve confounded atari games and find that it consistently dominates the standard dqn in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist
recently large models have shown significant potential for smart healthcare however the deployment of large visionlanguage models lvlms for clinical services is currently hindered by three critical challenges a tendency to hallucinate answers not grounded in visual evidence the inefficiency of fixeddepth reasoning and the difficulty of multiinstitutional collaboration to address these challenges in this paper we develop medalign a novel framework to ensure visually accurate lvlm responses for medical visual question answering medvqa specifically we first propose a multimodal direct preference optimization mdpo objective to explicitly align preference learning with visual context we then design a retrievalaware mixtureofexperts ramoe architecture that utilizes image and text ilarity to route queries to a specialized and contextaugmented lvlm ie an expert thereby mitigating hallucinations in lvlms to achieve adaptive reasoning and facilitate multiinstitutional collaboration we propose a federated governance mechanism where the selected expert finetuned on clinical datasets based on mdpo locally performs iterative chainofthought cot reasoning via the local metacognitive uncertainty estimator extensive experiments on three representative medvqa datasets demonstrate that medalign achieves stateoftheart performance outperforming strong retrievalaugmented baselines by up to in fscore and ultaneously reducing the average reasoning length by compared with fixeddepth cot approaches
the complexity of structured query language sql and the specialized nature of geospatial functions in tools like postgis present significant barriers to nonexperts seeking to analyze spatial data while large language models llms offer promise for translating natural language into sql texttosql singleagent approaches often struggle with the semantic and syntactic complexities of spatial queries to address this we propose a multiagent framework designed to accurately translate natural language questions into spatial sql queries the framework integrates several innovative components including a knowledge base with programmatic schema profiling and semantic enrichment embeddings for context retrieval and a collaborative multiagent pipeline as its core this pipeline comprises specialized agents for entity extraction metadata retrieval query logic formulation sql generation and a review agent that performs programmatic and semantic validation of the generated sql to ensure correctness selfverification we evaluate our system using both the nonspatial kaggledbqa benchmark and a new comprehensive spatialqueryqa benchmark that includes diverse geometry types predicates and three levels of query complexity on kaggledbqa the system achieved an overall accuracy of out of questions after the review agents review and corrections for spatial queries the system achieved an overall accuracy of out of questions compared with without the review agent beyond accuracy results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks this work makes spatial analysis more accessible and provides a robust generalizable foundation for spatial texttosql systems advancing the development of autonomous gis
when should we defer to ai outputs over human expert judgment drawing on recent work in social epistemology i motivate the idea that some ai systems qualify as artificial epistemic authorities aeas due to their demonstrated reliability and epistemic superiority i then introduce ai preemptionism the view that aea outputs should replace rather than supplement a users independent epistemic reasons i show that classic objections to preemptionism such as uncritical deference epistemic entrenchment and unhinging epistemic bases apply in amplified form to aeas given their opacity selfreinforcing authority and lack of epistemic failure markers against this i develop a more promising alternative a total evidence view of ai deference according to this view aea outputs should function as contributory reasons rather than outright replacements for a users independent epistemic considerations this approach has three key advantages i it mitigates expertise atrophy by keeping human users engaged ii it provides an epistemic case for meaningful human oversight and control and iii it explains the justified mistrust of ai when reliability conditions are unmet while demanding in practice this account offers a principled way to determine when ai deference is justified particularly in highstakes contexts requiring rigorous reliability
harmonizing medication data across electronic health record ehr systems is a persistent barrier to monitoring medications for opioid use disorder moud in heterogeneous ehr systems key prescription attributes are scattered across differently formatted fields and freetext notes we present a practical framework that customizes open source large language models llms including llama qwen gemma and medgemma to extract a unified set of moud prescription attributes prescription date drug name duration total quantity daily quantity and refills from heterogeneous site specific data and compute a standardized metric of medication coverage moud days per patient our pipeline processes records directly in a fixed json schema followed by lightweight normalization and crossfield consistency checks we evaluate the system on prescription level ehr data from five clinics in a national oud study records from patients using a previously annotated benchmark of records patients as the ground truth performance is reported as coverage share of records with a valid matchable output and recordlevel exactmatch accuracy larger models perform best overall qwenb achieves textbf coverage with textbf exactmatch accuracy across clinics and medgemmab attains textbftextbf a brief error review highlights three common issues and fixes imputing missing dosage fields using withindrug norms handling monthlyweekly injectables eg vivitrol by setting duration from the documented schedule and adding unit checks to prevent mass units eg g from being misread as daily counts by removing brittle sitespecific etl and supporting local privacypreserving deployment this approach enables consistent crosssite analyses of moud exposure adherence and retention in realworld settings
fuzzy numbers are commonly represented with fuzzy sets their objective is to better represent imprecise data however operations on fuzzy numbers are not as straightforward as maths on crisp numbers commonly the zadehs extension rule is applied to elaborate a result this can produce two problems high computational complexity and for some fuzzy sets and some operations the results is not a fuzzy set with the same features eg multiplication of two triangular fuzzy sets does not produce a triangular fuzzy set one more problem is the fuzzy spread fuzziness of the result increases with the number of operations these facts can severely limit the application field of fuzzy numbers in this paper we would like to revisite this problem with a different kind of fuzzy numbers extensional fuzzy numbers the paper defines operations on extensional fuzzy numbers and relational operators for them the proposed approach is illustrated with several applicational examples the c implementation is available from a public github repository
in openended domains like art autonomous agents must generate ideas that are both original and internally coherent yet current large language models llms either default to familiar cultural patterns or sacrifice coherence when pushed toward novelty we address this by introducing the cultural alien sampler cas a conceptselection method that explicitly separates compositional fit from cultural typicality cas uses two gpt models finetuned on wikiart concepts a concept coherence model that scores whether concepts plausibly cooccur within artworks and a cultural context model that estimates how typical those combinations are within individual artists bodies of work cas targets combinations that are high in coherence and low in typicality yielding ideas that maintain internal consistency while deviating from learned conventions and embedded cultural context in a human evaluation n our approach outperforms random selection and gpto baselines and achieves performance comparable to human art students in both perceived originality and harmony additionally a quantitative study shows that our method produces more diverse outputs and explores a broader conceptual space than its gpto counterpart demonstrating that artificial cultural alienness can unlock creative potential in autonomous agents
this study introduces a humanintheloop pipeline that converts unscaled handdrawn floor plan sketches into semantically consistent d bim models the workflow leverages multimodal large language models mllms within a multiagent framework combining perceptual extraction human feedback schema validation and automated bim scripting initially sketches are iteratively refined into a structured json layout of walls doors and windows later these layouts are transformed into executable scripts that generate d bim models experiments on ten diverse floor plans demonstrate strong convergence openings doors windows are captured with high reliability in the initial pass while wall detection begins around and achieves nearperfect alignment after a few feedback iterations across all categories precision recall and f scores remain above and geometric errors rmse mae progressively decrease to zero through feedback corrections this study demonstrates how mllmdriven multiagent reasoning can make bim creation accessible to both experts and nonexperts using only freehand sketches
computer vision can accelerate ecological research and conservation monitoring yet adoption in ecology lags in part because of a lack of trust in blackbox neuralnetworkbased models we seek to address this challenge by applying posthoc explanations to provide evidence for predictions and document limitations that are important to field deployment using aerial imagery from glacier bay national park we train a faster rcnn to detect pinnipeds harbor seals and generate explanations via gradientbased class activation mapping hirescam layercam local interpretable modelagnostic explanations lime and perturbationbased explanations we assess explanations along three axes relevant to field use i localization fidelity whether highattribution regions coincide with the animal rather than background context ii faithfulness whether deletioninsertion tests produce changes in detector confidence and iii diagnostic utility whether explanations reveal systematic failure modes explanations concentrate on seal torsos and contours rather than surrounding icerock and removal of the seals reduces detection confidence providing modelevidence for true positives the analysis also uncovers recurrent error sources including confusion between seals and black ice and rocks we translate these findings into actionable next steps for model development including more targeted data curation and augmentation by pairing object detection with posthoc explainability we can move beyond blackbox predictions toward auditable decisionsupporting tools for conservation monitoring
tracking human fullbody motion using sparse wearable inertial measurement units imus overcomes the limitations of occlusion and instrumentation of the environment inherent in visionbased approaches however purely imubased tracking compromises translation estimates and accurate relative positioning between individuals as inertial cues are inherently selfreferential and provide no direct spatial reference for others in this paper we present a novel approach for robustly estimating body poses and global translation for multiple individuals by leveraging the distances between sparse wearable sensors both on each individual and across multiple individuals our method group inertial poser estimates these absolute distances between pairs of sensors from ultrawideband ranging uwb and fuses them with inertial observations as input into structured statespace models to integrate temporal motion patterns for precise d pose estimation our novel twostep optimization further leverages the estimated distances for accurately tracking peoples global trajectories through the world we also introduce gipdb the first imuuwb dataset for twoperson tracking which comprises minutes of motion recordings from participants in our evaluation group inertial poser outperforms previous stateoftheart methods in accuracy and robustness across synthetic and realworld data showing the promise of imuuwbbased multihuman motion capture in the wild code models dataset
this paper introduces a novel dynamic knowledge distillation framework gompertzcnn which integrates the gompertz growth model into the training process to address the limitations of traditional knowledge distillation conventional methods often fail to capture the evolving cognitive capacity of student models leading to suboptimal knowledge transfer to overcome this we propose a stageaware distillation strategy that dynamically adjusts the weight of distillation loss based on the gompertz curve reflecting the students learning progression slow initial growth rapid midphase improvement and latestage saturation our framework incorporates wasserstein distance to measure featurelevel discrepancies and gradient matching to align backward propagation behaviors between teacher and student models these components are unified under a multiloss objective where the gompertz curve modulates the influence of distillation losses over time extensive experiments on cifar and cifar using various teacherstudent architectures eg resnet and mobilenetv demonstrate that gompertzcnn consistently outperforms traditional distillation methods achieving up to and accuracy gains on cifar and cifar respectively
deploying reinforcement learning rl in safetycritical settings is constrained by brittleness under distribution shift we study outofdistribution ood detection for rl time series and introduce deedee a twostatistic detector that revisits representationheavy pipelines with a minimal alternative deedee uses only an episodewise mean and an rbf kernel ilarity to a training summary capturing complementary global and local deviations despite its plicity deedee matches or surpasses contemporary detectors across standard rl ood suites delivering a fold reduction in compute flops walltime and an average absolute accuracy gain over strong baselines conceptually our results indicate that diverse anomaly types often imprint on rl trajectories through a small set of loworder statistics suggesting a compact foundation for ood detection in complex environments
knowledge distillation is a promising approach to transfer capabilities from complex teacher models to smaller resourceefficient student models that can be deployed easily particularly in taskaware scenarios however existing methods of taskaware distillation typically require substantial quantities of data which may be unavailable or expensive to obtain in many practical scenarios in this paper we address this challenge by introducing a novel strategy called counterfactualexplanationinfused distillation cod for fewshot taskaware knowledge distillation by systematically infusing counterfactual explanations counterfactual explanations cfes refer to inputs that can flip the output prediction of the teacher model with minimum perturbation our strategy cod leverages these cfes to precisely map the teachers decision boundary with significantly fewer samples we provide theoretical guarantees for motivating the role of cfes in distillation from both statistical and geometric perspectives we mathematically show that cfes can improve parameter estimation by providing more informative examples near the teachers decision boundary we also derive geometric insights on how cfes effectively act as knowledge probes helping the students mimic the teachers decision boundaries more effectively than standard data we perform experiments across various datasets and llms to show that cod outperforms standard distillation approaches in fewshot regimes as low as samples notably cod only uses half of the original samples used by the baselines paired with their corresponding cfes and still improves performance
understanding how information is dynamically accumulated and transformed in human reasoning has long challenged cognitive psychology philosophy and artificial intelligence existing accounts from classical logic to probabilistic models illuminate aspects of output or individual modelling but do not offer a unified quantitative description of general human reasoning dynamics to solve this we introduce information flow tracking iftrack that uses large language models llms as probabilistic encoder to quantify information entropy and gain at each reasoning step through finegrained analyses across diverse tasks our method is the first successfully models the universal landscape of human reasoning behaviors within a single metric space we show that iftrack captures essential reasoning features identifies systematic error patterns and characterizes individual differences applied to discussion of advanced psychological theory we first reconcile single versus dualprocess theories in iftrack and discover the alignment of artificial and human cognition and how llms reshaping human reasoning process this approach establishes a quantitative bridge between theory and measurement offering mechanistic insights into the architecture of reasoning
the increasing need for data privacy and the demand for robust machine learning models have fueled the development of synthetic data generation techniques however current methods often succeed in replicating ple summary statistics but fail to preserve both the pairwise and higherorder correlation structure of the data that define the complex multivariable interactions inherent in realworld systems this limitation can lead to synthetic data that is superficially realistic but fails when used for sophisticated modeling tasks in this white paper we introduce generative correlation manifolds gcm a computationally efficient method for generating synthetic data the technique uses cholesky decomposition of a target correlation matrix to produce datasets that by mathematical proof preserve the entire correlation structure from ple pairwise relationships to higherorder interactions of the source dataset we argue that this method provides a new approach to synthetic data generation with potential applications in privacypreserving data sharing robust model training and ulation
group relative policy optimization grpo has shown strong potential for flowmatchingbased texttoimage ti generation but it faces two key limitations inaccurate advantage attribution and the neglect of temporal dynamics of generation in this work we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues building on this idea we propose chunkgrpo the first chunklevel grpobased approach for ti generation the insight is to group consecutive steps into coherent chunks that capture the intrinsic temporal dynamics of flow matching and to optimize policies at the chunk level in addition we introduce an optional weighted sampling strategy to further enhance performance extensive experiments show that chunkgrpo achieves superior results in both preference alignment and image quality highlighting the promise of chunklevel optimization for grpobased methods
large language models are demonstrating increasing capabilities excelling at benchmarks once considered very difficult as their capabilities grow there is a need for more challenging evaluations that go beyond surfacelevel linguistic competence namely language competence involves not only syntax and semantics but also pragmatics ie understanding situational meaning as shaped by context as well as linguistic and cultural norms to contribute to this line of research we introduce sloprageval and slopragmega the first pragmatics understanding benchmarks for slovene that contain altogether multiplechoice questions we discuss the difficulties of translation describe the campaign to establish a human baseline and report pilot evaluations with llms our results indicate that current models have greatly improved in understanding nuanced language but may still fail to infer implied speaker meaning in nonliteral utterances especially those that are culturespecific we also observe a significant gap between proprietary and opensource models finally we argue that benchmarks targeting nuanced language understanding and knowledge of the target culture must be designed with care preferably constructed from native data and validated with human responses
this paper presents a novel approach for pretraining robotic manipulation visionlanguageaction vla models using a large corpus of unscripted reallife video recordings of human hand activities treating human hand as dexterous robot endeffector we show that inthewild egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic vla training data in terms of task granularity and labels this is achieved by the development of a fullyautomated holistic human activity analysis approach for arbitrary human hand videos this approach can generate atomiclevel hand activity segments and their language descriptions each accompanied with framewise d hand motion and camera motion we process a large volume of egocentric videos and create a handvla training dataset containing m episodes and m frames this training data covers a wide range of objects and concepts dexterous manipulation tasks and environment variations in real life vastly exceeding the coverage of existing robot data we design a dexterous hand vla model architecture and pretrain the model on this dataset the model exhibits strong zeroshot capabilities on completely unseen realworld observations additionally finetuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments we also demonstrate the appealing scaling behavior of the models task performance with respect to pretraining data scale we believe this work lays a solid foundation for scalable vla pretraining advancing robots toward truly generalizable embodied intelligence
with the current progress of artificial intelligence ai technology and its increasingly broader applications trust is seen as a required criterion for ai usage acceptance and deployment a robust measurement instrument is essential to correctly evaluate trust from a humancentered perspective this paper describes the development and validation process of a trust measure instrument which follows psychometric principles and consists of a items trust scale the instrument was built explicitly for research in humanai interaction to measure trust attitudes towards ai systems from layperson nonexpert perspective the usecase we used to develop the scale was in the context of ai medical support systems specifically cancerhealth prediction the scale development measurement item development and validation measurement item evaluation involved six research stages item development item evaluation survey administration test of dimensionality test of reliability and test of validity the results of the sixstages evaluation show that the proposed trust measurement instrument is empirically reliable and valid for systematically measuring and comparing nonexperts trust in ai medical support systems
vision encoders are indispensable for allowing impressive performance of multimodal large language models mllms in vision language tasks such as visual question answering and reasoning however existing vision encoders focus on global image representations but overlook finegrained regional analysis they are limited in fine grained perception due to the scarcity of fine grained annotated data and the lack of a fine grained pretraining paradigm in this paper we propose granvit a novel vision transformer that integrates finegrained feature extraction with semantic alignment to large language models llms via region level autoregressive training we first construct granm a dataset comprising million natural and ocr images paired with over million highquality regionlevel annotations to enable large scale fine grained pretraining consequently we develop a pretrainingadaptation framework along with a self distillation mechanism to train finegrained granvit on granm we sufficiently exploit the finegrained annotations from granm to resort to boundingboxtocaption regression to enhance localized visual representation of the vision encoder in the pretraining and captiontoboundingbox regression to improve vision feature utilization and localization for llm in the adaptation we further incorporate a self distillation mechanism that imposes explicit localization constraints on the vision encoder to strengthen its regional reasoning capability extensive experiments show that granvit surpasses existing vision encoders and attains strong transferability to varying llms remarkably it achieves stateoftheart results on finegrained recognition multimodal vqa and ocr understanding
as artificial intelligence continues to advance and becomes more integrated into sensitive areas like healthcare education and everyday life its crucial for these systems to be both resilient and robust this paper shows how resilience is a fundamental characteristic of social robots which through it ensure trust in the robot itselfan essential element especially when operating in contexts with elderly people who often have low trust in these systems resilience is therefore the ability to operate under adverse or stressful conditions even when degraded or weakened while maintaining essential operational capabilities
interactive world models that ulate object dynamics are crucial for robotics vr and ar however it remains a significant challenge to learn physicsconsistent dynamics models from limited realworld video data especially for deformable objects with spatiallyvarying physical properties to overcome the challenge of data scarcity we propose physworld a novel framework that utilizes a ulator to synthesize physically plausible and diverse demonstrations to learn efficient world models specifically we first construct a physicsconsistent digital twin within mpm ulator via constitutive model selection and globaltolocal optimization of physical properties subsequently we apply partaware perturbations to the physical properties and generate various motion patterns for the digital twin synthesizing extensive and diverse demonstrations finally using these demonstrations we train a lightweight gnnbased world model that is embedded with physical properties the real video can be used to further refine the physical properties physworld achieves accurate and fast future predictions for various deformable objects and also generalizes well to novel interactions experiments show that physworld has competitive performance while enabling inference speeds times faster than the recent stateoftheart method ie phystwin
recently large language models llms have demonstrated outstanding reasoning capabilities on mathematical and coding tasks however their application to financial tasksespecially the most fundamental task of stock movement predictionremains underexplored we study a threeclass classification problem up hold down and by analyzing existing reasoning responses observe that llms follow analysts opinions rather than exhibit a systematic independent analytical logic cots llms list summaries from different sources without weighing adversarial evidence yet such counterevidence is crucial for reliable prediction it shows that the model does not make good use of its reasoning ability to complete the task to address this we propose reflective evidence tuning retuning a coldstart method prior to reinforcement learning to enhance prediction ability while generating cot retuning encourages dynamically constructing an analytical framework from diverse information sources organizing and scoring evidence for price up or down based on that frameworkrather than on contextual viewpointsand finally reflecting to derive the prediction this approach maximally aligns the model with its learned analytical framework ensuring independent logical reasoning and reducing undue influence from context we also build a largescale dataset spanning all of for ashare stocks with long contexts k tokens and over k samples in addition to price and news it incorporates analysts opinions quantitative reports fundamental data macroeconomic indicators and ilar stocks experiments show that retuning successfully unlocks the models reasoning ability in the financial domain inferencetime scaling still works even after months or on outofdistribution stocks since the models gain valuable insights about stock movement prediction
lexical data collection in language documentation often contains transcription errors and undocumented borrowings that can mislead linguistic analysis we present unsupervised anomaly detection methods to identify phonotactic inconsistencies in wordlists applying them to a multilingual dataset of kokborok varieties with bangla using characterlevel and syllablelevel phonotactic features our algorithms identify potential transcription errors and borrowings while precision and recall remain modest due to the subtle nature of these anomalies syllableaware features significantly outperform characterlevel baselines the highrecall approach provides fieldworkers with a systematic method to flag entries requiring verification supporting data quality improvement in lowresourced language documentation
large language models llms display notable variation in multilingual behavior yet the role of genealogical language structure in shaping this variation remains underexplored in this paper we investigate whether llms exhibit sensitivity to linguistic genera by extending prior analyses on the multiq dataset we first check if models prefer to switch to genealogically related languages when prompt language fidelity is not maintained next we investigate whether knowledge consistency is better preserved within than across genera we show that genuslevel effects are present but strongly conditioned by training resource availability we further observe distinct multilingual strategies across llms families our findings suggest that llms encode aspects of genuslevel structure but training data imbalances remain the primary factor shaping their multilingual performance
we apply category theory to extract multimodal document structure which leads us to develop information theoretic measures content summarization and extension and selfsupervised improvement of large pretrained models we first develop a mathematical representation of a document as a category of questionanswer pairs second we develop an orthogonalization procedure to divide the information contained in one or more documents into nonoverlapping pieces the structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document we also build on those steps to develop new summarization techniques as well as to develop a solution to a new problem viz exegesis resulting in an extension of the original document our questionanswer pair methodology enables a novel rate distortion analysis of summarization techniques we implement our techniques using large pretrained models and we propose a multimodal extension of our overall mathematical framework finally we develop a novel selfsupervised method using rlvr to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework
retrievalaugmented generation rag integrates external knowledge to mitigate hallucinations yet models often generate outputs inconsistent with retrieved content accurate hallucination detection requires disentangling the contributions of external context and parametric knowledge which prior methods typically conflate we investigate the mechanisms underlying rag hallucinations and find they arise when laterlayer ffn modules disproportionately inject parametric knowledge into the residual stream to address this we explore a mechanistic detection approach based on external context scores and parametric knowledge scores using qwenb we compute these scores across layers and attention heads and train regressionbased classifiers to predict hallucinations our method is evaluated against stateoftheart llms gpt gpt and detection baselines ragas trulens refchecker furthermore classifiers trained on qwenb signals generalize to gptmini responses demonstrating the potential of proxymodel evaluation our results highlight mechanistic signals as efficient generalizable predictors for hallucination detection in rag systems
pretrained language models are remarkably effective in aligning with human brain responses elicited by natural language stimuli positioning them as promising model organisms for studying language processing in the brain however existing approaches for both estimating and improving this brain alignment are participantdependent and highly affected by the amount of data available per participant hindering both generalization to new participants and populationlevel analyses in this work we address these limitations by introducing a scalable generalizable braintuning method in which we finetune pretrained speech language models to jointly predict fmri responses from multiple participants we demonstrate that the resulting braintuned models exhibit strong individual brain alignment while generalizing across participants specifically our method leads to a fold decrease in the amount of fmri data needed to predict brain data from new participants up to a increase in the overall brain alignment and strong generalization to new unseen datasets furthermore this multiparticipant braintuning additionally improves downstream performance on semantic tasks suggesting that training using brain data from multiple participants leads to more generalizable semantic representations taken together these findings demonstrate a bidirectional benefit between neuroscience and ai helping bridge the gap between the two fields we make our code and models publicly available at
recent advances in diffusion language models dlms have presented a promising alternative to traditional autoregressive large language models llms however dlms still lag behind llms in reasoning performance especially as the number of denoising steps decreases our analysis reveals that this shortcoming arises primarily from the independent generation of masked tokens across denoising steps which fails to capture the token correlation in this paper we define two types of token correlation intrasequence correlation and intersequence correlation and demonstrate that enhancing these correlations improves reasoning performance to this end we propose a multireward optimization mro approach which encourages dlms to consider the token correlation during the denoising process more specifically our mro approach leverages testtime scaling reject sampling and reinforcement learning to directly optimize the token correlation with multiple elaborate rewards additionally we introduce group step and importance sampling strategies to mitigate reward variance and enhance sampling efficiency through extensive experiments we demonstrate that mro not only improves reasoning performance but also achieves significant sampling speedups while maintaining high performance on reasoning benchmarks
with the widespread adoption of wearable devices in our daily lives the demand and appeal for remote patient monitoring have significantly increased most research in this field has concentrated on collecting sensor data visualizing it and analyzing it to detect anomalies in specific diseases such as diabetes heart disease and depression however this domain has a notable gap in the aspect of humanmachine interaction this paper proposes remoni an autonomous remote health monitoring system that integrates multimodal large language models mllms the internet of things iot and wearable devices the system automatically and continuously collects vital signs accelerometer data from a special wearable such as a smartwatch and visual data in patient video clips collected from cameras this data is processed by an anomaly detection module which includes a fall detection model and algorithms to identify and alert caregivers of the patients emergency conditions a distinctive feature of our proposed system is the natural language processing component developed with mllms capable of detecting and recognizing a patients activity and emotion while responding to healthcare workers inquiries additionally prompt engineering is employed to integrate all patient information seamlessly as a result doctors and nurses can access realtime vital signs and the patients current state and mood by interacting with an intelligent agent through a userfriendly web application our experiments demonstrate that our system is implementable and scalable for reallife scenarios potentially reducing the workload of medical professionals and healthcare costs a fullfledged prototype illustrating the functionalities of the system has been developed and being tested to demonstrate the robustness of its various capabilities
traditional information retrieval ir metrics such as ndcg map and mrr assume that human users sequentially examine documents with diminishing attention to lower ranks this assumption breaks down in retrieval augmented generation rag systems where search results are consumed by large language models llms which unlike humans process all retrieved documents as a whole rather than sequentially additionally traditional ir metrics do not account for related but irrelevant documents that actively degrade generation quality rather than merely being ignored due to these two major misalignments namely human vs machine position discount and human relevance vs machine utility classical ir metrics do not accurately predict rag performance we introduce a utilitybased annotation schema that quantifies both the positive contribution of relevant passages and the negative impact of distracting ones building on this foundation we propose udcg utility and distractionaware cumulative gain a metric using an llmoriented positional discount to directly optimize the correlation with the endtoend answer accuracy experiments on five datasets and six llms demonstrate that udcg improves correlation by up to compared to traditional metrics our work provides a critical step toward aligning ir evaluation with llm consumers and enables more reliable assessment of rag components
as generative ai continues to evolve vision language models vlms have emerged as promising tools in various healthcare applications one area that remains relatively underexplored is their use in human activity recognition har for remote health monitoring vlms offer notable strengths including greater flexibility and the ability to overcome some of the constraints of traditional deep learning models however a key challenge in applying vlms to har lies in the difficulty of evaluating their dynamic and often nondeterministic outputs to address this gap we introduce a descriptive caption data set and propose comprehensive evaluation methods to evaluate vlms in har through comparative experiments with stateoftheart deep learning models our findings demonstrate that vlms achieve comparable performance and in some cases even surpass conventional approaches in terms of accuracy this work contributes a strong benchmark and opens new possibilities for the integration of vlms into intelligent healthcare systems
transformerbased models have advanced nlp yet hebrew still lacks a largescale roberta encoder which is extensively trained existing models such as hebert alephbert and hero are limited by corpus size vocabulary or training depth we present hallelubert a robertabased encoder family base and large trained from scratch on gb of deduplicated hebrew web text and wikipedia with a hebrewspecific bytelevel bpe vocabulary evaluated on ner and sentiment classification benchmarks hallelubert outperforms both monolingual and multilingual baselines hallelubert sets a new state of the art for hebrew and highlights the benefits of fully converged monolingual pretraining
transformer models have revolutionized nlp yet many morphologically rich languages remain underrepresented in largescale pretraining efforts with sindbert we set out to chart the seas of turkish nlp providing the first largescale robertabased encoder for turkish trained from scratch on gb of turkish text mc oscar wikipedia sindbert is released in both base and large configurations representing the first largescale encoderonly language model available for turkish we evaluate sindbert on partofspeech tagging named entity recognition offensive language detection and the turblimp linguistic acceptability benchmark our results show that sindbert performs competitively with existing turkish and multilingual models with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall this flat scaling trend also observed for xlmr and eurobert suggests that current turkish benchmarks may already be saturated at the same time comparisons with smaller but more curated models such as berturk highlight that corpus quality and diversity can outweigh sheer data volume taken together sindbert contributes both as an openly released resource for turkish nlp and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages the sindbert models are released under the mit license and made available in both fairseq and huggingface formats
many swedish benchmarks are translated uscentric benchmarks and therefore not suitable for testing knowledge that is particularly relevant or even specific to sweden we therefore introduce a manually written questionanswering benchmark specifically targeted to swedenrelated personalities and events many of which receive very limited coverage in international media our annotators drew inspiration from a popular radio program featuring public figures from culture and media as well as major sports events in sweden the dataset can be used to measure factual recall across models of varying sizes and degrees of swedish coverage and allows to probe crosslingual factual consistency as to contains english translations using the dataset we find that smaller models with stronger swedish coverage perform comparably to a three times larger multilingual model in recalling swedenrelated facts we also observe that continued pretraining on swedish generally improves factual knowledge but also leads to forgetting of a part of the previously known information these results demonstrate the datasets potential as a diagnostic tool for studying language adaptation and knowledge retention in multilingual models and during language adaptation
the reasoning capabilities of large language models llms are typically developed through the singleturn reinforcement learning whereas realworld applications often involve multiturn interactions with human feedback leading to a potential mismatch between training and deployment conditions in this work we study whether multiturn training with human feedback is necessary for reasoning tasks we compare conventional singleturn training with three multiturn strategies and reach contrary conclusions to previous research we find that models trained in a singleturn setting generalize effectively to both single and multiturn evaluations while models trained with multiturn strategies exhibit a significant degradation in singleturn reasoning performance these results suggest that for tasks with complete information robust singleturn training remains more effective and reliable as multiturn training with basic feedback provides limited benefits and can even degrade reasoning capabilities
recent efforts like tripcraft and travelplanner have advanced the use of large language models llms for personalized constraint aware travel itinerary generation yet real travel often faces disruptions to address this we present triptide the first benchmark evaluating llms ability to revise itineraries under realistic disruptions triptide models key dimensions such as disruption severity and traveler tolerance enabling nuanced assessment of llm adaptability to events like flight cancellations weather closures or overbooked attractions we conduct a threefold evaluation first we introduce automatic metrics including preservation of intent how well the revised plan maintains feasibility and goals responsiveness promptness and appropriateness of disruption handling and adaptability semantic spatial and sequential divergence between original and revised plans second we apply an llmasajudge approach to automatically assess revision quality third we perform manual expert evaluation to verify whether revisions preserve semantic spatial sequential and responsive aspects our experiments show that llms maintain strong sequential consistency and semantic stability while spatial deviations are larger for shorter trips but decrease with longer ones indicating that extended plans encourage better geographic coherence however disruptionhandling ability declines as plan length increases highlighting limits in llm robustness triptide establishes a benchmark for evaluating adaptability personalization and resilience in llmbased travel planning under realworld uncertainty
research in linguistics has shown that humans can read words with internally scrambled letters a phenomenon recently dubbed typoglycemia some specific nlp models have recently been proposed that ilarly demonstrate robustness to such distortions by ignoring the internal order of characters by design this raises a fundamental question how can models perform well when many distinct words eg form and from collapse into identical representations under typoglycemia our work focusing exclusively on the english language seeks to shed light on the underlying aspects responsible for this robustness we hypothesize that the main reasons have to do with the fact that i relatively few english words collapse under typoglycemia and that ii collapsed words tend to occur in contexts so distinct that disambiguation becomes trivial in our analysis we i analyze the british national corpus to quantify word collapse and ambiguity under typoglycemia ii evaluate berts ability to disambiguate collapsing forms and iii conduct a probing experiment by comparing variants of bert trained from scratch on clean versus typoglycemic wikipedia text our results reveal that the performance degradation caused by scrambling is smaller than expected
accurately estimating semantic aleatoric and epistemic uncertainties in large language models llms is particularly challenging in freeform question answering qa where obtaining stable estimates often requires many expensive generations we introduce a diversitysteered sampler that discourages semantically redundant outputs during decoding covers both autoregressive and masked diffusion paradigms and yields substantial sampleefficiency gains the key idea is to inject a continuous semanticilarity penalty into the models proposal distribution using a natural language inference nli model lightly finetuned on partial prefixes or intermediate diffusion states we debias downstream uncertainty estimates with importance reweighting and shrink their variance with control variates across four qa benchmarks our method matches or surpasses baselines while covering more semantic clusters with the same number of samples being modular and requiring no gradient access to the base llm the framework promises to serve as a dropin enhancement for uncertainty estimation in risksensitive model deployments
large language models llms have demonstrated high performance on tasks expressed in natural language particularly in zero or fewshot settings these are typically framed as supervised eg classification or unsupervised eg clustering problems however limited work evaluates llms as agents in reinforcement learning rl tasks eg playing games where learning occurs through interaction with an environment and a reward system while prior work focused on representing tasks that rely on a language representation we study structured nonlinguistic reasoning such as interpreting positions in a grid world we therefore introduce parl promptbased agent for reinforcement learning a method that uses llms as rl agents through prompting without any finetuning parl encodes actions states and rewards in the prompt enabling the model to learn through trialanderror interaction we evaluate parl on three standard rl tasks that do not entirely rely on natural language we show that it can match or outperform traditional rl agents in ple environments by leveraging pretrained knowledge however we identify performance limitations in tasks that require complex mathematical operations or decoding states and actions
scaling the context length of large language models llms offers significant benefits but is computationally expensive this expense stems primarily from the selfattention mechanism whose on complexity with respect to sequence length presents a major bottleneck for both memory and latency fortunately the attention matrix is often sparse particularly for long sequences suggesting an opportunity for optimization blocksparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks however the effectiveness of this method is highly dependent on the underlying attention patterns which can lead to suboptimal blocklevel sparsity for instance important key tokens for queries within a single block may be scattered across numerous other blocks leading to computational redundancy in this work we propose permuted blocksparse attention pbsattn a plugandplay method that leverages the permutation properties of attention to increase blocklevel sparsity and enhance the computational efficiency of llm prefilling we conduct comprehensive experiments on challenging realworld longcontext datasets demonstrating that pbsattn consistently outperforms existing blocksparse attention methods in model accuracy and closely matches the full attention baseline powered by our custom permutedflashattention kernels pbsattn achieves an endtoend speedup of up to times in longcontext prefilling confirming its practical viability code available at
large language models llms have achieved remarkable progress in natural language generation yet they continue to display puzzling behaviors such as repetition and incoherence even when exhibiting low perplexity this highlights a key limitation of conventional evaluation metrics which asize local prediction accuracy while overlooking longrange structural complexity we introduce correlation dimension a fractalgeometric measure of selfilarity to quantify the epistemological complexity of text as perceived by a language model this measure captures the hierarchical recurrence structure of language bridging local and global properties in a unified framework through extensive experiments we show that correlation dimension reveals three distinct phases during pretraining reflects contextdependent complexity indicates a models tendency toward hallucination and reliably detects multiple forms of degeneration in generated text the method is computationally efficient robust to model quantization down to bit precision broadly applicable across autoregressive architectures eg transformer and mamba and provides fresh insight into the generative dynamics of llms
objective emergency medical dispatch emd is a highstakes process challenged by caller distress ambiguity and cognitive load large language models llms and multiagent systems mas offer opportunities to augment dispatchers this study aimed to develop and evaluate a taxonomygrounded llmpowered multiagent system for ulating realistic emd scenarios methods we constructed a clinical taxonomy chief complaints caller identities from mimiciii and a sixphase call protocol using this framework we developed an autogenbased mas with caller and dispatcher agents the system grounds interactions in a fact commons to ensure clinical plausibility and mitigate misinformation we used a hybrid evaluation framework four physicians assessed ulated cases for guidance efficacy and dispatch effectiveness supplemented by automated linguistic analysis sentiment readability politeness results human evaluation with substantial interrater agreement gwes ac confirmed the systems high performance it demonstrated excellent dispatch effectiveness eg contacting the correct potential other agents and guidance efficacy advice provided in of cases both rated highly by physicians algorithmic metrics corroborated these findings indicating a predominantly neutral affective profile neutral sentiment neutral emotion high readability flesch and a consistently polite style polite impolite conclusion our taxonomygrounded mas ulates diverse clinically plausible dispatch scenarios with high fidelity findings support its use for dispatcher training protocol evaluation and as a foundation for realtime decision support this work outlines a pathway for safely integrating advanced ai agents into emergency response workflows
the rise of rightwing populism in europe has brought to the forefront the significance of analysing social media discourse to understand the dissemination of extremist ideologies and their impact on political outcomes twitter as a platform for interaction and mobilisation provides a unique window into the everyday communication of farright supporters in this paper we propose a methodology that uses stateoftheart natural language processing techniques with sociological insights to analyse the migrtwit corpus of farright tweets in english and french we aim to uncover patterns of discourse surrounding migration hate speech and persuasion techniques employed by right and farright actors by integrating linguistic sociological and computational approaches we seek to offer crossdisciplinary insights into societal dynamics and contribute to a better understanding of contemporary challenges posed by rightwing extremism on social media platforms
the availability of llm benchmarks for the estonian language is limited and a comprehensive evaluation comparing the performance of different llms on estonian tasks has yet to be conducted we introduce a new benchmark for evaluating llms in estonian based on seven diverse datasets these datasets assess general and domainspecific knowledge understanding of estonian grammar and vocabulary summarization abilities contextual comprehension and more the datasets are all generated from native estonian sources without using machine translation we compare the performance of base models instructiontuned opensource models and commercial models our evaluation includes base models and instructiontuned models to assess the results we employ both human evaluation and llmasajudge methods human evaluation scores showed moderate to high correlation with benchmark evaluations depending on the dataset claude sonnet used as an llm judge demonstrated strong alignment with human ratings indicating that topperforming llms can effectively support the evaluation of estonianlanguage models
reliable ulation of human behavior is essential for explaining predicting and intervening in our society recent advances in large language models llms have shown promise in emulating human behaviors interactions and decisionmaking offering a powerful new lens for social science studies however the extent to which llms diverge from authentic human behavior in social contexts remains underexplored posing risks of misinterpretation in scientific studies and unintended consequences in realworld applications here we introduce a systematic framework for analyzing llms behavior in social ulation our approach ulates multiagent interactions through chatroomstyle conversations and analyzes them across five linguistic dimensions providing a ple yet effective method to examine emergent social cognitive biases we conduct extensive experiments involving eight representative llms across three families our findings reveal that llms do not faithfully reproduce genuine human behavior but instead reflect overly idealized versions of it shaped by the social desirability bias in particular llms show social role bias primacy effect and positivity bias resulting in utopian societies that lack the complexity and variability of real human interactions these findings call for more socially grounded llms that capture the diversity of human social behavior
large language models llms have achieved remarkable success in natural language processing through strong semantic understanding and generation however their blackbox nature limits structured and multihop reasoning in contrast textattributed graphs tags provide explicit relational structures enriched with textual context yet often lack semantic depth recent research shows that combining llms and tags yields complementary benefits enhancing tag representation learning and improving the reasoning and interpretability of llms this survey provides the first systematic review of llmtag integration from an orchestration perspective we introduce a novel taxonomy covering two fundamental directions llm for tag where llms enrich graphbased tasks and tag for llm where structured graphs improve llm reasoning we categorize orchestration strategies into sequential parallel and multimodule frameworks and discuss advances in tagspecific pretraining prompting and parameterefficient finetuning beyond methodology we summarize empirical insights curate available datasets and highlight diverse applications across recommendation systems biomedical analysis and knowledgeintensive question answering finally we outline open challenges and promising research directions aiming to guide future work at the intersection of language and graph learning
ensuring that large language models llms generate summaries faithful to a given source document is essential for realworld applications while prior research has explored llm faithfulness existing benchmarks suffer from annotation ambiguity primarily due to the illdefined boundary of permissible external knowledge in generated outputs for instance common sense is often incorporated into responses and labeled as faithful yet the acceptable extent of such knowledge remains unspecified leading to inconsistent annotations to address this issue we propose a novel faithfulness annotation framework which introduces an intermediate category outdependent to classify cases where external knowledge is required for verification using this framework we construct verigray verification with the gray zone a new unfaithfulness detection benchmark in summarization statistics reveal that even sota llms such as gpt exhibit hallucinations sim of sentences in summarization tasks moreover a substantial proportion sim on average of models of generated sentences fall into the outdependent category underscoring the importance of resolving annotation ambiguity in unfaithfulness detection benchmarks experiments demonstrate that our benchmark poses significant challenges to multiple baseline methods indicating considerable room for future improvement
supervised finetuning sft has emerged as a crucial method for aligning large language models llms with humanannotated demonstrations however sft being an offpolicy approach ilar to behavior cloning often struggles with overfitting and poor outofdomain generalization especially in limiteddata scenarios to address these limitations we propose selfrewarding ppo a novel finetuning method that leverages onpolicy techniques to enhance generalization performance our approach combines the strengths of sft and proximal policy optimization ppo to achieve more effective alignment from demonstration data at its core is a reward function designed as the log policy ratio between the sft model and the pretrained base model this function serves as an implicit reward signal using the pretrained policy as a baseline and the sft policy as a target by doing so it enables onpolicy finetuning without relying on human preference annotations the integration of this selfrewarding mechanism with ppo addresses key limitations of sft improving generalization data efficiency and robustness our empirical evaluation across a range of natural language processing tasks demonstrates that selfrewarding ppo consistently outperforms traditional sft methods the results highlight the effectiveness of our approach in aligning llms using demonstration data particularly in scenarios where highquality annotated data is scarce
intelligent drug recommendation based on electronic health records ehrs is critical for improving for improving the quality and efficiency of clinical decisionmaking by leveraging largescale patient data drug recommendation systems can assist physicians in selecting the most appropriate medications according to a patients medical history diagnoses laboratory results and comorbidities however the advancement of such systems is significantly hampered by the scarcity of publicly available realworld ehr datasets particularly in languages other than english in this work we present cdrugred a first publicly available chinese drug recommendation dataset focused on discharge medications for metabolic diseases the dataset includes deidentified records from patients containing comprehensive information such as patient demographics medical history clinical course and discharge diagnoses we assess the utility of cdrugred by benchmarking several stateoftheart large language models llms on the discharge medication recommendation task experimental results show that while supervised finetuning improves model performance there remains substantial room for improvement with the best model achieving the f score of and jaccard score of this result highlights the complexity of the clinical drug recommendation task and establishes cdrugred as a challenging and valuable resource for developing more robust and accurate drug recommendation systems the dataset is publicly available to the research community under the data usage agreements at
question answering qa has seen significant improvements with the advancement of machine learning models further studies enhanced this question answering system by retrieving external information called retrievalaugmented generation rag to produce more accurate and informative answers however these stateoftheartperformance is predominantly in english language to address this gap we made an effort of bridging language gaps by incorporating adaptive rag system to indonesian language adaptive rag system integrates a classifier whose task is to distinguish the question complexity which in turn determines the strategy for answering the question to overcome the limited availability of indonesian language dataset our study employs machine translation as data augmentation approach experiments show reliable question complexity classifier however we observed significant inconsistencies in multiretrieval answering strategy which negatively impacted the overall evaluation when this strategy was applied these findings highlight both the promise and challenges of question answering in lowresource language suggesting directions for future improvement
large language models llms excel at factual recall yet still propagate stale or incorrect knowledge incontext knowledge editing offers a gradientfree remedy suitable for blackbox apis but current editors rely on static demonstration sets chosen by surfacelevel ilarity leading to two persistent obstacles i a quantityquality tradeoff and ii lack of adaptivity to task difficulty we address these issues by dynamically selecting supporting demonstrations according to their utility for the edit we propose dynamic retriever for incontext knowledge editing drike a lightweight framework that trains a bert retriever with reinforce to rank demonstrations by editing reward and employs a learnable threshold to prune lowvalue examples shortening the prompt when the edit is easy and expanding it when the task is hard drike performs editing without modifying model weights relying solely on forward passes for compatibility with blackbox llms on the counterfact benchmark it improves edit success by up to reduces latency by and preserves accuracy on unrelated queries demonstrating scalable and adaptive knowledge editing the code is available at
reasoning has become a central paradigm for large language models llms consistently boosting accuracy across diverse benchmarks yet its suitability for precisionsensitive tasks remains unclear we present the first systematic study of reasoning for classification tasks under strict low false positive rate fpr regimes our analysis covers two taskssafety detection and hallucination detectionevaluated in both finetuned and zeroshot settings using standard llms and large reasoning models lrms our results reveal a clear tradeoff think on reasoningaugmented generation improves overall accuracy but underperforms at the lowfpr thresholds essential for practical use in contrast think off no reasoning during inference dominates in these precisionsensitive regimes with think on surpassing only when higher fprs are acceptable in addition we find tokenbased scoring substantially outperforms selfverbalized confidence for precisionsensitive deployments finally a ple ensemble of the two modes recovers the strengths of each taken together our findings position reasoning as a doubleedged tool beneficial for average accuracy but often illsuited for applications requiring strict precision
a major concern when deploying llms in accuracycritical domains such as sports reporting is that the generated text may not faithfully reflect the input data we quantify how input structure affects hallucinations and other factual errors in llmgenerated summaries of nba playbyplay data across three formats rowstructured json and unstructured we manually annotated factual errors across game summaries produced by two models llamab and qwenb input structure has a strong effect json input reduces error rates by for llama and for qwen compared to unstructured input while rowstructured input reduces errors by for llama and for qwen a twoway repeated measures anova shows that input structure accounts for over of the variance in error rates with tukey hsd post hoc tests confirming statistically significant differences between all input formats
chainofthought cot prompting has emerged as a common technique for enhancing the reasoning abilities of large language models llms while extended reasoning can boost accuracy on complex tasks it is often unnecessary and substantially increases token usage limiting the practicality of reasoning models in many scenarios recent models such as gptoss and qwen expose controls that enable users to adjust the length of cot or determine whether it is used at all yet it remains unclear when cot should be used on some tasks it improves performance while on others it provides little benefit or even harms performance we address this challenge with confidencegated cot where a model invokes reasoning only when confidence in its direct answer is low to this end we present the first systematic study of trainingfree confidence estimation methods for cot gating specifically we evaluate four trainingfree confidence estimation methods and compare them to a random baseline and an oracle that always knows when cot is needed through extensive experiments we show that existing trainingfree confidence measures can reduce redundant cot and outperform randomly invoked cot however the utility of individual confidence measures is inconsistent varying with both the dataset and the model underscoring the difficulty of deploying confidencegated cot in practice by analysing both strengths and failure modes our study highlights the potential and limitations of current methods and paves the way toward more reliable adaptive gating of cot
we present irishblimp irish benchmark of linguistic minimal pairs the first dataset and framework designed for finegrained evaluation of linguistic competence in the irish language an endangered language drawing on a variety of linguistic literature and grammar reference works we manually constructed and reviewed minimal pairs across a taxonomy of linguistic features through a team of fluent irish speakers we evaluate both existing large language models llms and fluent human participants on their syntactic knowledge of irish our findings show that humans outperform all models across all linguistic features achieving higher accuracy on average moreover a substantial performance gap of persists between open and closedsource llms with even the strongest model gpt reaching only accuracy compared to by human interestingly human participants and models struggle on different aspects of irish grammar thus highlighting a difference in representation learned by the models overall irishblimp provides the first systematic framework for evaluating the grammatical competence of llms in irish and offers a valuable benchmark for advancing research on linguistic understanding in lowresource languages
large language models llms with extended context windows show promise for complex legal reasoning tasks yet their ability to understand long legal documents remains insufficiently evaluated developing longcontext benchmarks that capture realistic highstakes tasks remains a significant challenge in the field as most existing evaluations rely on plified synthetic tasks that fail to represent the complexity of realworld document understanding overruling relationships are foundational to commonlaw doctrine and commonly found in judicial opinions they provide a focused and important testbed for longdocument legal understanding that closely resembles what legal professionals actually do we present an assessment of stateoftheart llms on identifying overruling relationships from us supreme court cases using a dataset of case pairs our evaluation reveals three critical limitations era sensitivity the models show degraded performance on historical cases compared to modern ones revealing fundamental temporal bias in their training shallow reasoning models rely on shallow logical heuristics rather than deep legal comprehension and contextdependent reasoning failures models produce temporally impossible relationships in complex openended tasks despite maintaining basic temporal awareness in ple contexts our work contributes a benchmark that addresses the critical gap in realistic longcontext evaluation providing an environment that mirrors the complexity and stakes of actual legal reasoning tasks
as language models become capable of processing increasingly long and complex texts there has been growing interest in their application within computational literary studies however evaluating the usefulness of these models for such tasks remains challenging due to the cost of finegrained annotation for longform texts and the data contamination concerns inherent in using publicdomain literature current embedding ilarity datasets are not suitable for evaluating literarydomain tasks because of a focus on coarsegrained ilarity and primarily on very short text we assemble and release ficsim a dataset of longform recently written fiction including scores along axes of ilarity informed by authorproduced metadata and validated by digital humanities scholars we evaluate a suite of embedding models on this task demonstrating a tendency across models to focus on surfacelevel features over semantic categories that would be useful for computational literary studies tasks throughout our datacollection process we prioritize author agency and rely on continual informed author consent
reasoning models rms language models lms trained with reinforcement learning to produce longform natural language reasoning have been remarkably successful but they still require large amounts of computation and data to train and can be slow and expensive to run in this paper we show that standard instruct lms can already be elicited to be strong reasoners at a level comparable to or even surpassing their corresponding rms eg deepseek v vs r without finetuning across diverse domains from instruction following and creative generation to mathematical reasoning this is achieved by codeadapt our ple recipe that combines the codeact framework where lms interleave natural language reasoning with code execution in a multistep fashion with fewshot bootstrap incontext learning from as few as five training problems analyzing four matched pairs of lms and rms we find that codeadapt enables three lms to outperform the corresponding rms on average over eight tasks up to while being more token efficient and delivers superior performance on six tasks when averaged over the four models up to furthermore the codeaugmented reasoning traces display rich and varied problemsolving strategies our findings support that codeadaptstyle learning and reasoning may be robust and domain general and codeenabled lms are cognitively grounded and powerful systems potentially providing a strong foundation for inweight reinforcement learning
many highstakes applications of ai require forming datadriven hypotheses and making targeted guesses eg in scientific and diagnostic settings given limited resources to what extent do agents based on language models lms act rationally we develop methods to benchmark and enhance agentic informationseeking drawing on insights from human behavior first we introduce a strategic decisionoriented dialogue task called collaborative battleship in which a partiallyinformed captain must balance exploration asking questions and action taking shots while a fullyinformed spotter must provide accurate answers under an information bottleneck compared to human players n we find that lm agents struggle to ground answers in context generate informative questions and select highvalue actions next to address these gaps we develop novel monte carlo inference strategies for lms based on principles from bayesian experimental design bed for spotter agents our approach boosts accuracy by up to absolute over lmonly baselines for captain agents it raises expected information gain eig by up to bits of the achievable noise ceiling combined these components yield sharper targeting f and enable weaker lms such as llamascout to outperform both humans win rate and frontier models win rate vs gpt at of gpts cost we replicate these findings on guess who where our methods significantly boost accuracy pp demonstrating their general applicability for building rational informationseeking agents
deep research systems have revolutionized how llms solve complex questions through iterative reasoning and evidence gathering however current systems remain fundamentally constrained to textual web data overlooking the vast knowledge embedded in multimodal documents processing such documents demands sophisticated parsing to preserve visual semantics figures tables charts and equations intelligent chunking to maintain structural coherence and adaptive retrieval across modalities which are capabilities absent in existing systems in response we present docresearcher a unified system that bridges this gap through three integrated components i deep multimodal parsing that preserves layout structure and visual semantics while creating multigranular representations from chunk to document level ii systematic retrieval architecture supporting textonly visiononly and hybrid paradigms with dynamic granularity selection and iii iterative multiagent workflows that decompose complex queries progressively accumulate evidence and synthesize comprehensive answers across documents and modalities to enable rigorous evaluation we introduce mdocbench the first benchmark for multimodal multihop multidocument and multiturn deep research featuring expertannotated questions with complete evidence chains across documents mdocbench tests capabilities that existing benchmarks cannot assess experiments demonstrate that docresearcher achieves accuracy xbetter than stateoftheart baselines validating that effective document research requires not just better retrieval but fundamentally deep parsing that preserve multimodal integrity and support iterative research our work establishes a new paradigm for conducting deep research on multimodal document collections
with the rapid development of multimodal large language modelbased agents the landscape of agentic service management has evolved from singleagent systems to multiagent systems and now to massiveagent ecosystems current massiveagent ecosystems face growing challenges including impersonal service experiences a lack of standardization and untrustworthy behavior to address these issues we propose colorecosystem a novel blueprint designed to enable personalized standardized and trustworthy agentic service at scale concretely colorecosystem consists of three key components agent carrier agent store and agent audit the agent carrier provides personalized service experiences by utilizing userspecific data and creating a digital twin while the agent store serves as a centralized standardized platform for managing diverse agentic services the agent audit based on the supervision of developer and user activities ensures the integrity and credibility of both service providers and users through the analysis of challenges transitional forms and practical considerations the colorecosystem is poised to power personalized standardized and trustworthy agentic service across massiveagent ecosystems meanwhile we have also implemented part of colorecosystems functionality and the relevant code is opensourced at
language and visionlanguage models have shown impressive performance across a wide range of tasks but their internal mechanisms remain only partly understood in this work we study how individual attention heads in textgenerative models specialize in specific semantic or visual attributes building on an established interpretability method we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing this lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers remarkably we find that editing as few as of the heads selected using our method can reliably suppress or enhance targeted concepts in the model output we validate our approach on language tasks such as question answering and toxicity mitigation as well as visionlanguage tasks including image classification and captioning our findings highlight an interpretable and controllable structure within attention layers offering ple tools for understanding and editing largescale generative models
todays pursuit of a single large language model lmm for all software engineering tasks is resourceintensive and overlooks the potential benefits of complementarity where different models contribute unique strengths however the degree to which coding llms complement each other and the best strategy for maximizing an ensembles potential are unclear leaving practitioners without a clear path to move beyond singlemodel systems to address this gap we empirically compare ten individual llms from five families and three ensembles of these llms across three software engineering benchmarks covering code generation and program repair we assess the complementarity between models and the performance gap between the best individual model and the ensembles next we evaluate various selection heuristics to identify correct solutions from an ensembles candidate pool we find that the theoretical upperbound for an ensembles performance can be above the best single model our results show that consensusbased strategies for selecting solutions fall into a popularity trap amplifying common but incorrect outputs in contrast a diversitybased strategy realizes up to of this theoretical potential and proves effective even in small twomodel ensembles enabling a costefficient way to enhance performance by leveraging multiple llms
honeypots are decoy systems used for gathering valuable threat intelligence or diverting attackers away from production systems maximising attacker engagement is essential to their utility however research has highlighted that contextawareness such as the ability to respond to new attack types systems and attacker agents is necessary to increase engagement large language models llms have been shown as one approach to increase context awareness but suffer from several challenges including accuracy and timeliness of response time high operational costs and dataprotection issues due to cloud deployment we propose the systembased attention shell honeypot sbash framework which manages dataprotection issues through the use of lightweight local llms we investigate the use of retrieval augmented generation rag supported llms and nonrag llms for linux shell commands and evaluate them using several different metrics such as response time differences realism from human testers and ilarity to a real system calculated with levenshtein distance sbert and bertscore we show that rag improves accuracy for untuned models while models that have been tuned via a system prompt that tells the llm to respond like a linux system achieve without rag a ilar accuracy as untuned with rag while having a slightly lower latency
context and motivation large language models llms show notable results in natural language processing nlp tasks for requirements engineering re however their use is compromised by high computational cost data sharing risks and dependence on external services in contrast small language models slms offer a lightweight locally deployable alternative questionproblem it remains unclear how well slms perform compared to llms in re tasks in terms of accuracy results our preliminary study compares eight models including three llms and five slms on requirements classification tasks using the promise promise reclass and secreq datasets our results show that although llms achieve an average f score of higher than slms this difference is not statistically significant slms almost reach llms performance across all datasets and even outperform them in recall on the promise reclass dataset despite being up to times smaller we also found that dataset characteristics play a more significant role in performance than model size contribution our study contributes with evidence that slms are a valid alternative to llms for requirements classification offering advantages in privacy cost and local deployability
hikma semiautonomous conference is the first experiment in reimagining scholarly communication through an endtoend integration of artificial intelligence into the academic publishing and presentation pipeline this paper presents the design implementation and evaluation of the hikma framework which includes ai dataset curation aibased manuscript generation aiassisted peer review aidriven revision ai conference presentation and ai archival dissemination by combining language models structured research workflows and domain safeguards hikma shows how ai can support not replace traditional scholarly practices while maintaining intellectual property protection transparency and integrity the conference functions as a testbed and proof of concept providing insights into the opportunities and challenges of aienabled scholarship it also examines questions about ai authorship accountability and the role of humanai collaboration in research
when a vision model performs image recognition which visual attributes drive its predictions detecting unintended reliance on specific visual features is critical for ensuring model robustness preventing overfitting and avoiding spurious correlations we introduce an automated framework for detecting such dependencies in trained vision models at the core of our method is a selfreflective agent that systematically generates and tests hypotheses about visual attributes that a model may rely on this process is iterative the agent refines its hypotheses based on experimental outcomes and uses a selfevaluation protocol to assess whether its findings accurately explain model behavior when inconsistencies arise the agent selfreflects over its findings and triggers a new cycle of experimentation we evaluate our approach on a novel benchmark of models designed to exhibit diverse visual attribute dependencies across categories our results show that the agents performance consistently improves with selfreflection with a significant performance increase over nonreflective baselines we further demonstrate that the agent identifies realworld visual attribute dependencies in stateoftheart models including clips vision encoder and the yolov object detector
in this paper we show that visual diffusion models can serve as effective geometric solvers they can directly reason about geometric problems by working in pixel space we first demonstrate this on the inscribed square problem a longstanding problem in geometry that asks whether every jordan curve contains four points forming a square we then extend the approach to two other wellknown hard geometric problems the steiner tree problem and the ple polygon problem our method treats each problem instance as an image and trains a standard visual diffusion model that transforms gaussian noise into an image representing a valid imate solution that closely matches the exact one the model learns to transform noisy geometric structures into correct configurations effectively recasting geometric reasoning as image generation unlike prior work that necessitates specialized architectures and domainspecific adaptations when applying diffusion to parametric geometric representations we employ a standard visual diffusion model that operates on the visual representation of the problem this plicity highlights a surprising bridge between generative modeling and geometric problem solving beyond the specific problems studied here our results point toward a broader paradigm operating in image space provides a general and practical framework for imating notoriously hard problems and opens the door to tackling a far wider class of challenging geometric tasks
diffusion transformers dits have recently driven significant progress in texttovideo tv generation however generating multiple videos with consistent characters and backgrounds remains a significant challenge existing methods typically rely on reference images or extensive training and often only address character consistency leaving background consistency to imagetovideo models we introduce bachvid the first trainingfree method that achieves consistent video generation without needing any reference images our approach is based on a systematic analysis of dits attention mechanism and intermediate features revealing its ability to extract foreground masks and identify matching points during the denoising process our method leverages this finding by first generating an identity video and caching the intermediate variables and then inject these cached variables into corresponding positions in newly generated videos ensuring both foreground and background consistency across multiple videos experimental results demonstrate that bachvid achieves robust consistency in generated videos without requiring additional training offering a novel and efficient solution for consistent video generation without relying on reference images or additional training
we tackle the challenge of generating the infinitely extendable d world large continuous environments with coherent geometry and realistic appearance existing methods face key challenges dlifting approaches suffer from geometric and appearance inconsistencies across views d implicit representations are hard to scale up and current d foundation models are mostly objectcentric limiting their applicability to scenelevel generation our key insight is leveraging strong generation priors from pretrained d models for structured scene block generation to this end we propose worldgrow a hierarchical framework for unbounded d scene synthesis our method features three core components a data curation pipeline that extracts highquality scene blocks for training making the d structured latent representations suitable for scene generation a d block inpainting mechanism that enables contextaware scene extension and a coarsetofine generation strategy that ensures both global layout plausibility and local geometrictextural fidelity evaluated on the largescale dfront dataset worldgrow achieves sota performance in geometry reconstruction while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs these results highlight its capability for constructing largescale virtual environments and potential for building future world models
the rapid generation of wholeslide images wsis in dermatopathology necessitates automated methods for efficient processing and accurate classification this study evaluates the performance of two foundation models uni and virchow as feature extractors for classifying wsis into three diagnostic categories melanocytic basaloid and squamous lesions patchlevel embeddings were aggregated into slidelevel features using a meanaggregation strategy and subsequently used to train multiple machine learning classifiers including logistic regression gradientboosted trees and random forest models performance was assessed using precision recall true positive rate false positive rate and the area under the receiver operating characteristic curve auroc on the test set results demonstrate that patchlevel features extracted using virchow outperformed those extracted via uni across most slidelevel classifiers with logistic regression achieving the highest accuracy for virchow though the difference was not statistically significant the study also explored data augmentation techniques and image normalization to enhance model robustness and generalizability the meanaggregation approach provided reliable slidelevel feature representations all experimental results and metrics were tracked and visualized using facilitating reproducibility and interpretability this research highlights the potential of foundation models for automated wsi classification providing a scalable and effective approach for dermatopathological diagnosis while paving the way for future advancements in slidelevel representation learning
separating synapses into different classes based on their appearance in em images has many applications in biology examples may include assigning a neurotransmitter to a particular class or separating synapses whose strength can be modulated from those whose strength is fixed traditionally this has been done in a supervised manner giving the classification algorithm examples of the different classes here we instead separate synapses into classes based only on the observation that nearby synapses in the same neuron are likely more ilar than synapses chosen randomly from different cells we apply our methodology to data from it drosophila our approach has the advantage that the number of synapse types does not need to be known in advance it may also provide a principled way to select groundtruth that spans the range of synapse structure
as most in the wild data collections of the natural world the north america camera trap images nacti dataset shows severe longtailed class imbalance noting that the largest head class alone covers of the m images in the corpus building on the pytorch wildlife model we present a systematic study of longtail recognition methodologies for species recognition on the nacti dataset covering experiments on various ltr loss functions plus ltrsensitive regularisation our best configuration achieves top accuracy on our nacti test data split substantially improving over a baseline using standard crossentropy with adam this also improves on previously reported top performance in mlwic at albeit using partly unpublished potentially different partitioning optimiser and evaluation protocols to evaluate domain shifts eg nighttime captures occlusion motionblur towards other datasets we construct a reducedbias test set from the enadetection dataset where our experimentally optimised longtail enhanced model achieves leading accuracy up from with wce loss demonstrating stronger generalisation capabilities under distribution shift we document the consistent improvements of ltrenhancing scheduler choices in this nacti wildlife domain particularly when in tandem with stateoftheart ltr losses we finally discuss qualitative and quantitative shortcomings that ltr methods cannot sufficiently address including catastrophic breakdown for tail classes under severe domain shift for maximum reproducibility we publish all dataset splits key code and full network weights
compared to d data the scale of point cloud data in different domains available for training is quite limited researchers have been trying to combine these data of different domains for masked autoencoder mae pretraining to leverage such a data scarcity issue however the prior knowledge learned from mixed domains may not align well with the downstream d point cloud analysis tasks leading to degraded performance to address such an issue we propose the domainadaptive point cloud masked autoencoder dapmae an mae pretraining method to adaptively integrate the knowledge of crossdomain datasets for general point cloud analysis in dapmae we design a heterogeneous domain adapter that utilizes an adaptation mode during pretraining enabling the model to comprehensively learn information from point clouds across different domains while employing a fusion mode in the finetuning to enhance point cloud features meanwhile dapmae incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks with only one pretraining dapmae achieves excellent performance across four different point cloud analysis tasks reaching in object classification on scanobjectnn and in facial expression recognition on bosphorus
video generation models have progressed tremendously through large latent diffusion transformers trained with rectified flow techniques yet these models still struggle with geometric inconsistencies unstable motion and visual artifacts that break the illusion of realistic d scenes dconsistent video generation could significantly impact numerous downstream applications in generation and reconstruction tasks we explore how epipolar geometry constraints improve modern video diffusion models despite massive training data these models fail to capture fundamental geometric principles underlying visual content we align diffusion models using pairwise epipolar geometry constraints via preferencebased optimization directly addressing unstable camera trajectories and geometric artifacts through mathematically principled geometric enforcement our approach efficiently enforces geometric principles without requiring endtoend differentiability evaluation demonstrates that classical geometric constraints provide more stable optimization signals than modern learned metrics which produce noisy targets that compromise alignment quality training on static scenes with dynamic cameras ensures highquality measurements while the model generalizes effectively to diverse dynamic content by bridging datadriven deep learning with classical geometric computer vision we present a practical method for generating spatially consistent videos without compromising visual quality
crossmodal alignment aims to map heterogeneous modalities into a shared latent space as exemplified by models like clip which benefit from largescale imagetext pretraining for strong recognition capabilities however when operating in resourceconstrained settings with limited or lowquality data these models often suffer from overconfidence and degraded performance due to the prevalence of ambiguous or weakly correlated imagetext pairs current contrastive learning approaches which rely on single positive pairs further exacerbate this issue by reinforcing overconfidence on uncertain samples to address these challenges we propose modestalign a lightweight alignment framework designed for robustness and efficiency our approach leverages two complementary strategies random perturbation which introduces controlled noise to ulate uncertainty and embedding smoothing which calibrates ilarity distributions in the embedding space these mechanisms collectively reduce overconfidence and improve performance on noisy or weakly aligned samples extensive experiments across multiple benchmark datasets demonstrate that modestalign outperforms stateoftheart methods in retrieval tasks achieving competitive results with over x less training data and x less gpu time than clip our method offers a practical and scalable solution for crossmodal alignment in realworld lowresource scenarios
salient object detection exemplifies databounded tasks where expensive pixelprecise annotations force separate model training for related subtasks like dis and hrsod we present a method that dramatically improves generalization through largescale synthetic data generation and ambiguityaware architecture we introduce sod a dataset of over highresolution images created through our multimodal diffusion pipeline that extracts labels from diffusion and dinov features the iterative generation framework prioritizes challenging categories based on model performance we propose a streamlined multimask decoder that naturally handles the inherent ambiguity in salient object detection by predicting multiple valid interpretations models trained solely on synthetic data achieve error reduction in crossdataset generalization while finetuned versions reach stateoftheart performance across dis and hrsod benchmarks
in drugresistant epilepsy presurgical evaluation of epilepsy can be considered magnetoencephalography meg has been shown to be an effective exam to inform the localization of the epileptogenic zone through the localization of interictal epileptic spikes manual detection of these pathological biomarkers remains a fastidious and errorprone task due to the high dimensionality of meg recordings and interrater agreement has been reported to be only moderate current automated methods are unsuitable for clinical practice either requiring extensively annotated data or lacking robustness on nontypical data in this work we demonstrate that deep learning models can be used for detecting interictal spikes in meg recordings even when only temporal and singleexpert annotations are available which represents realworld clinical practice we propose two model architectures a featurebased artificial neural network ann and a convolutional neural network cnn trained on a database of patients and evaluated against a stateoftheart model to classify short time windows of signal in addition we employ an interactive machine learning strategy to iteratively improve our data annotation quality using intermediary model outputs both proposed models outperform the stateoftheart model fscores cnn ann when tested on holdout test patients the interactive machine learning strategy demonstrates that our models are robust to noisy annotations overall results highlight the robustness of models with ple architectures when analyzing complex and imperfectly annotated data our method of interactive machine learning offers great potential for faster data annotation while our models represent useful and efficient tools for automated interictal spikes detection
current generative superresolution methods show strong performance on natural images but distort text creating a fundamental tradeoff between image quality and textual readability to address this we introduce tiger textimage guided suptextbferresolution a novel twostage framework that breaks this tradeoff through a textfirst imagelater paradigm tiger explicitly decouples glyph restoration from image enhancement it first reconstructs precise text structures and then uses them to guide subsequent fullimage superresolution this glyphtoimage guidance ensures both high fidelity and visual consistency to support comprehensive training and evaluation we also contribute the ultrazoomst ultrazoomscene text the first scene text dataset with extreme zoom times extensive experiments show that tiger achieves stateoftheart performance enhancing readability while preserving overall image quality
nighttime uav tracking faces significant challenges in realworld robotics operations lowlight conditions not only limit visual perception capabilities but cluttered backgrounds and frequent viewpoint changes also cause existing trackers to drift or fail during deployment to address these difficulties researchers have proposed solutions based on lowlight enhancement and domain adaptation however these methods still have notable shortcomings in actual uav systems lowlight enhancement often introduces visual artifacts domain adaptation methods are computationally expensive and existing lightweight designs struggle to fully leverage dynamic object information based on an indepth analysis of these key issues we propose matracka multiscale adaptive system designed specifically for nighttime uav tracking matrack tackles the main technical challenges of nighttime tracking through the collaborative work of three core modules multiscale hierarchy blende mhb enhances feature consistency between static and dynamic templates adaptive key token gate accurately identifies object information within complex backgrounds nighttime template calibrator ntc ensures stable tracking performance over long sequences extensive experiments show that matrack achieves a significant performance improvement on the uavdark benchmark its precision normalized precision and auc surpass stateoftheart sota methods by and respectively while maintaining a realtime processing speed of fps further tests on a realworld uav platform validate the systems reliability demonstrating that matrack can provide stable and effective nighttime uav tracking support for critical robotics applications such as nighttime search and rescue and border patrol
foley control is a lightweight approach to videoguided foley that keeps pretrained singlemodality models frozen and learns only a small crossattention bridge between them we connect vjepa video embeddings to a frozen stable audio open dit texttoaudio ta model by inserting compact video crossattention after the models existing text crossattention so prompts set global semantics while video refines timing and local dynamics the frozen backbones retain strong marginals video audio given text and the bridge learns the audiovideo dependency needed for synchronization without retraining the audio prior to cut memory and stabilize training we pool video tokens before conditioning on curated videoaudio benchmarks foley control delivers competitive temporal and semantic alignment with far fewer trainable parameters than recent multimodal systems while preserving promptdriven controllability and productionfriendly modularity swapupgrade encoders or the ta backbone without endtoend retraining although we focus on videotofoley the same bridge design can potentially extend to other audio modalities eg speech
classifierfree guidance cfg is an essential component of texttoimage diffusion models and understanding and advancing its operational mechanisms remains a central focus of research existing approaches stem from divergent theoretical interpretations thereby limiting the design space and obscuring key design choices to address this we propose a unified perspective that reframes conditional guidance as fixed point iterations seeking to identify a golden path where latents produce consistent outputs under both conditional and unconditional generation we demonstrate that cfg and its variants constitute a special case of singlestep shortinterval iteration which is theoretically proven to exhibit inefficiency to this end we introduce foresight guidance fsg which prioritizes solving longerinterval subproblems in early diffusion stages with increased iterations extensive experiments across diverse datasets and model architectures validate the superiority of fsg over stateoftheart methods in both image quality and computational efficiency our work offers novel perspectives for conditional guidance and unlocks the potential of adaptive design
placental abruption is a severe complication during pregnancy and its early accurate diagnosis is crucial for ensuring maternal and fetal safety traditional ultrasound diagnostic methods heavily rely on physician experience leading to issues such as subjective bias and diagnostic inconsistencies this paper proposes an improved model ehyolovn enhanced hemorrhageyolovn based on smallsample learning aiming to achieve automatic detection of hematoma features in placental ultrasound images the model enhances performance through multidimensional optimization it integrates wavelet convolution and coordinate convolution to strengthen frequency and spatial feature extraction incorporates a cascaded group attention mechanism to suppress ultrasound artifacts and occlusion interference thereby improving bounding box localization accuracy experimental results demonstrate a detection accuracy of representing a improvement over yolovn and a increase over yolov the model exhibits significant superiority in precisionrecall curves confidence scores and occlusion scenarios combining high accuracy with realtime processing this model provides a reliable solution for computeraided diagnosis of placental abruption holding significant clinical application value
grapmot is a new approach for solving the person mot problem dedicated to videos of closed areas with overlapping multicamera views where person occlusion frequently occurs our novel graphweighted solution updates a persons identification label online based on tracks and the persons characteristic features to find the best solution we deeply investigated all elements of the mot process including feature extraction tracking and community search furthermore grapmot is equipped with a persons position estimation module which gives additional key information to the mot method ensuring better results than methods without position data we tested grapmot on recordings acquired in a closedarea model and on publicly available real datasets that fulfil the requirement of a highly congested space showing the superiority of our proposition finally we analyzed existing metrics used to compare mot algorithms and concluded that idf is more adequate than mota in such comparisons we made our code along with the acquired dataset publicly available
accurate interpretation of histopathological images demands integration of information across spatial and semantic scales from nuclear morphology and cellular textures to global tissue organization and diseasespecific patterns although recent foundation models in pathology have shown strong capabilities in capturing global tissue context their omission of celllevel feature modeling remains a key limitation for finegrained tasks such as cancer subtype classification to address this we propose a dualstream architecture that models the interplay between macroscale tissue features and aggregated cellular representations to efficiently aggregate information from large cell sets we propose a receptanceweighted keyvalue aggregation model a recurrent transformer that captures intercell dependencies with linear complexity furthermore we introduce a bidirectional tissuecell interaction module to enable mutual attention between localized cellular cues and their surrounding tissue environment experiments on four histopathological subtype classification benchmarks show that the proposed method outperforms existing models demonstrating the critical role of celllevel aggregation and tissuecell interaction in finegrained computational pathology
deep learning models have achieved remarkable accuracy in chest xray diagnosis yet their widespread clinical adoption remains limited by the blackbox nature of their predictions clinicians require transparent verifiable explanations to trust automated diagnoses and identify potential failure modes we introduce cxrlanic languagegrounded interpretable classifier for chest xrays a novel framework that addresses this interpretability challenge through taskaligned pattern discovery our approach trains transcoderbased sparse autoencoders on a biomedclip diagnostic classifier to decompose medical image representations into interpretable visual patterns by training an ensemble of transcoders on multimodal embeddings from the mimiccxr dataset we discover imately monosemantic patterns spanning cardiac pulmonary pleural structural device and artifact categories each pattern exhibits consistent activation behavior across images sharing specific radiological features enabling transparent attribution where predictions decompose into interpretable patterns with verifiable activation galleries cxrlanic achieves competitive diagnostic accuracy on five key findings while providing the foundation for natural language explanations through planned large multimodal model annotation our key innovation lies in extracting interpretable features from a classifier trained on specific diagnostic objectives rather than generalpurpose embeddings ensuring discovered patterns are directly relevant to clinical decisionmaking demonstrating that medical ai systems can be both accurate and interpretable supporting safer clinical deployment through transparent clinically grounded explanations
recent video inpainting methods often employ imagetovideo iv priors to model temporal consistency across masked frames while effective in moderate cases these methods struggle under severe content degradation and tend to overlook spatiotemporal stability resulting in insufficient control over the latter parts of the video to address these limitations we decouple video inpainting into two subtasks multiframe consistent image inpainting and masked area motion propagation we propose vidsplice a novel framework that introduces spacedframe priors to guide the inpainting process with spatiotemporal cues to enhance spatial coherence we design a cospliced module to perform firstframe propagation strategy that diffuses the initial frame content into subsequent reference frames through a splicing mechanism additionally we introduce a delicate context controller module that encodes coherent priors after frame duplication and injects the spliced video into the iv generative backbone effectively constraining content distortion during generation extensive evaluations demonstrate that vidsplice achieves competitive performance across diverse video inpainting scenarios moreover its design significantly improves both foreground alignment and motion stability outperforming existing approaches
video anomaly detection vad aims to locate unusual activities or behaviors within videos recently offline vad has garnered substantial research attention which has been invigorated by the progress in large language models llms and visionlanguage models vlms offering the potential for a more nuanced understanding of anomalies however online vad has seldom received attention due to realtime constraints and computational intensity in this paper we introduce a novel memorybased online scoring queue scheme for trainingfree vad monitor to address the inherent complexities in online vad specifically monitor applies a streaming input to vlms leveraging the capabilities of pretrained largescale models to capture temporal dependencies more effectively we incorporate a novel prediction mechanism inspired by long shortterm memory lstm networks this ensures the model can effectively model past states and leverage previous predictions to identify anomalous behaviors thereby it better understands the current frame moreover we design a scoring queue and an anomaly prior to dynamically store recent scores and cover all anomalies in the monitoring scenario providing guidance for llms to distinguish between normal and abnormal behaviors over time we evaluate monitor on two large datasets ie ucfcrime and xdviolence containing various surveillance and realworld scenarios the results demonstrate that monitor outperforms stateoftheart methods and is competitive with weakly supervised methods without training code is available at
modeling the inherent hierarchical structure of d objects and d scenes is highly desirable as it enables a more holistic understanding of environments for autonomous agents accomplishing this with implicit representations such as neural radiance fields remains an unexplored challenge existing methods that explicitly model hierarchical structures often face significant limitations they either require multiple rendering passes to capture embeddings at different levels of granularity significantly increasing inference time or rely on predefined closedset discrete hierarchies that generalize poorly to the diverse and nuanced structures encountered by agents in the real world to address these challenges we propose openhype a novel approach that represents scene hierarchies using a continuous hyperbolic latent space by leveraging the properties of hyperbolic geometry openhype naturally encodes multiscale relationships and enables smooth traversal of hierarchies through geodesic paths in latent space our method outperforms stateoftheart approaches on standard benchmarks demonstrating superior efficiency and adaptability in d scene understanding
table lookup realization of image restoration cnns has the potential of achieving competitive image quality while being much faster and resource frugal than the straightforward cnn implementation the main technical challenge facing the lutbased cnn algorithm designers is to manage the table size without overly restricting the receptive field the prevailing strategy is to reuse the table for small pixel patches of different orientations apparently assuming a degree of isotropy and then fuse the lookup results the fusion is currently done by average pooling which we find being ill suited to anisotropic signal structures to alleviate the problem we investigate and discuss anisotropic pooling methods to replace naive averaging for improving the performance of the current lutrealizable cnn restoration methods first we introduce the method of generalized median pooling which leads to measurable gains over average pooling we then extend this idea by learning datadependent pooling coefficients for each orientation so that they can adaptively weigh the contributions of differently oriented pixel patches experimental results on various restoration benchmarks show that our anisotropic pooling strategy yields both perceptually and numerically superior results compared to existing lutrealizable cnn methods
we propose artilatent a generative framework that synthesizes humanmade d objects with finegrained geometry accurate articulation and realistic appearance our approach jointly models part geometry and articulation dynamics by embedding sparse voxel representations and associated articulation properties including joint type axis origin range and part category into a unified latent space via a variational autoencoder a latent diffusion model is then trained over this space to enable diverse yet physically plausible sampling to reconstruct photorealistic d shapes we introduce an articulationaware gaussian decoder that accounts for articulationdependent visibility changes eg revealing the interior of a drawer when opened by conditioning appearance decoding on articulation state our method assigns plausible texture features to regions that are typically occluded in static poses significantly improving visual realism across articulation configurations extensive experiments on furniturelike objects from partnetmobility and acd datasets demonstrate that artilatent outperforms existing approaches in geometric consistency and appearance fidelity our framework provides a scalable solution for articulated d object synthesis and manipulation
human intelligence effortlessly interprets visual scenes along a rich spectrum of semantic dimensions however existing approaches to languagegrounded visual concept learning are limited to a few predefined primitive axes such as color and shape and are typically explored in synthetic datasets in this work we propose a scalable framework that adaptively identifies imagerelated concept axes and grounds visual concepts along these axes in realworld scenes leveraging a pretrained visionlanguage model and our universal prompting strategy our framework identifies a diverse imagerelated axes without any prior knowledge our universal concept encoder adaptively binds visual features to the discovered axes without introducing additional model parameters for each concept to ground visual concepts along the discovered axes we optimize a compositional anchoring objective which ensures that each axis can be independently manipulated without affecting others we demonstrate the effectiveness of our framework on subsets of imagenet celebahq and afhq showcasing superior editing capabilities across diverse realworld concepts that are too varied to be manually predefined our method also exhibits strong compositional generalization outperforming existing visual concept learning and textbased editing methods the code is available at
we propose the multimodal untrimmed video retrieval task along with a new benchmark muvr to advance video retrieval for longvideo platforms muvr aims to retrieve untrimmed videos containing relevant segments using multimodal queries it has the following features practical retrieval paradigm muvr supports videocentric multimodal queries expressing finegrained retrieval needs through long text descriptions video tag prompts and mask prompts it adopts a onetomany retrieval paradigm and focuses on untrimmed videos tailored for longvideo platform applications multilevel visual correspondence to cover common video categories eg news travel dance and precisely define retrieval matching criteria we construct multilevel visual correspondence based on core video content eg news events travel locations dance moves which users are interested in and want to retrieve it covers six levels copy event scene instance action and others comprehensive evaluation criteria we develop versions of muvr ie base filter qa muvrbasefilter evaluates retrieval models while muvrqa assesses mllms in a questionanswering format we also propose a reranking score to evaluate the reranking ability of mllms muvr consists of k untrimmed videos from the video platform bilibili with multimodal queries and k matches extensive evaluations of stateoftheart video retrieval models imagebased vlms and mllms are conducted muvr reveals the limitations of retrieval methods in processing untrimmed videos and multimodal queries as well as mllms in multivideo understanding and reranking our code and benchmark is available at
image stitching synthesizes images captured from multiple perspectives into a single image with a broader field of view the significant variations in object depth often lead to large parallax resulting in ghosting and misalignment in the stitched results to address this we propose a depthconsistencyconstrained seamlessfree image stitching method first to tackle the multiview alignment difficulties caused by parallax a multistage mechanism combined with global depth regularization constraints is developed to enhance the alignment accuracy of the same apparent target across different depth ranges second during the multiview image fusion process an optimal stitching seam is determined through graphbased lowcost computation and a softseam region is diffused to precisely locate transition areas thereby effectively mitigating alignment errors induced by parallax and achieving natural and seamless stitching results furthermore considering the computational overhead in the shift regression process a reparameterization strategy is incorporated to optimize the structural design significantly improving algorithm efficiency while maintaining optimal performance extensive experiments demonstrate the superior performance of the proposed method against the existing methods code is available at
remote sensing vision tasks require extensive labeled data across multiple interconnected domains however current generative data augmentation frameworks are taskisolated ie each vision task requires training an independent generative model and ignores the modeling of geographical information and spatial constraints to address these issues we propose terragen a unified layouttoimage generation framework that enables flexible spatially controllable synthesis of remote sensing imagery for various highlevel vision tasks eg detection segmentation and extraction specifically terragen introduces a geographicspatial layout encoder that unifies bounding box and segmentation mask inputs combined with a multiscale injection scheme and maskweighted loss to explicitly encode spatial constraints from global structures to fine details also we construct the first largescale multitask remote sensing layout generation dataset containing k images and establish a standardized evaluation protocol for this task experimental results show that our terragen can achieve the best generation image quality across diverse tasks additionally terragen can be used as a universal dataaugmentation generator enhancing downstream task performance significantly and demonstrating robust crosstask generalisation in both fulldata and fewshot scenarios
in this work we propose a novel framework to enable diffusion models to adapt their generation quality based on realtime network bandwidth constraints traditional diffusion models produce highfidelity images by performing a fixed number of denoising steps regardless of downstream transmission limitations however in practical cloudtodevice scenarios limited bandwidth often necessitates heavy compression leading to loss of fine textures and wasted computation to address this we introduce a joint endtoend training strategy where the diffusion model is conditioned on a target quality level derived from the available bandwidth during training the model learns to adaptively modulate the denoising process enabling earlystop sampling that maintains perceptual quality appropriate to the target transmission condition our method requires minimal architectural changes and leverages a lightweight quality embedding to guide the denoising trajectory experimental results demonstrate that our approach significantly improves the visual fidelity of bandwidthadapted generations compared to naive earlystopping offering a promising solution for efficient image delivery in bandwidthconstrained environments code is available at
we participated in the synthrad challenge tasks and with a unified pipeline for synthetic ct sct generation from mri and cbct implemented using the konfai framework our model is a d unet with a resnet encoder trained jointly across anatomical regions and finetuned per region the loss function combined pixelwise l loss with impactsynth a perceptual loss derived from sam and totalsegmentator to enhance structural fidelity training was performed using adamw initial learning rate halved every k steps on patchbased normalized bodymasked inputs x for mri x for cbct with random flipping as the only augmentation no postprocessing was applied final predictions leveraged testtime augmentation and fivefold ensembling the best model was selected based on validation mae two registration strategies were evaluated i elastix with mutual information consistent with the challenge pipeline and ii impact a featurebased ilarity metric leveraging pretrained segmentation networks on the local test sets impactbased registration achieved more accurate and anatomically consistent alignments than mutualinformationbased registration resulting in improved sct synthesis with lower mae and more realistic anatomical structures on the public validation set however models trained with elastixaligned data achieved higher scores reflecting a registration bias favoring alignment strategies consistent with the evaluation pipeline this highlights how registration errors can propagate into supervised learning influencing both training and evaluation and potentially inflating performance metrics at the expense of anatomical fidelity by promoting anatomically consistent alignment impact helps mitigate this bias and supports the development of more robust and generalizable sct synthesis models
eye gaze offers valuable cues about attention shortterm intent and future actions making it a powerful signal for modeling egocentric behavior in this work we propose a gazeregularized framework that enhances vlms for two key egocentric understanding tasks finegrained future event prediction and current activity understanding unlike prior approaches that rely solely on visual inputs or use gaze as an auxiliary input signal our method uses gaze only during training we introduce a gazeregularized attention mechanism that aligns model focus with human visual gaze this design is flexible and modular allowing it to generalize across multiple vlm architectures that utilize attention experimental results show that our approach improves semantic prediction scores by up to for future event prediction and around for current activity understanding compared to the corresponding baseline models trained without gaze regularization these results highlight the value of gazeguided training in improving the accuracy and robustness of egocentric vlms overall this work establishes a foundation for using human gaze to enhance the predictive capabilities of vlms in realworld scenarios like assistive robots and humanmachine collaboration code and additional information is available at
uav tracking can be widely applied in scenarios such as disaster rescue environmental monitoring and logistics transportation however existing uav tracking methods predominantly asize speed and lack exploration in semantic awareness which hinders the search region from extracting accurate localization information from the template the limitation results in suboptimal performance under typical uav tracking challenges such as camera motion fast motion and low resolution etc to address this issue we propose a dynamic semantic aware correlation modeling tracking framework the core of our framework is a dynamic semantic relevance generator which in combination with the correlation map from the transformer explore semantic relevance the approach enhances the search regions ability to extract important information from the template improving accuracy and robustness under the aforementioned challenges additionally to enhance the tracking speed we design a pruning method for the proposed framework therefore we present multiple model variants that achieve tradeoffs between speed and accuracy enabling flexible deployment according to the available computational resources experimental results validate the effectiveness of our method achieving competitive performance on multiple uav tracking datasets the code is available at
in complex orchard environments the phenotypic heterogeneity of different apple leaf diseases characterized by significant variation among lesions poses a challenge to traditional multiscale feature fusion methods these methods only integrate multilayer features extracted by convolutional neural networks cnns and fail to adequately account for the relationships between local and global features therefore this study proposes a multibranch recognition framework named cnntransformerclip ctclip the framework synergistically employs a cnn to extract local lesion detail features and a vision transformer to capture global structural relationships an adaptive feature fusion module affm then dynamically fuses these features achieving optimal coupling of local and global information and effectively addressing the diversity in lesion morphology and distribution additionally to mitigate interference from complex backgrounds and significantly enhance recognition accuracy under fewshot conditions this study proposes a multimodal imagetext learning approach by leveraging pretrained clip weights it achieves deep alignment between visual features and disease semantic descriptions experimental results show that ctclip achieves accuracies of and on a publicly available apple disease and a selfbuilt dataset outperforming several baseline methods the proposed ctclip demonstrates strong capabilities in recognizing agricultural diseases significantly enhances identification accuracy under complex environmental conditions provides an innovative and practical solution for automated disease recognition in agricultural applications
understanding how cells respond to external stimuli is a central challenge in biomedical research and drug development current computational frameworks for modelling cellular responses remain restricted to twodimensional representations limiting their capacity to capture the complexity of cell morphology under perturbation this dimensional constraint poses a critical bottleneck for the development of accurate virtual cell models here we present form a machine learning framework for predicting perturbationinduced changes in threedimensional cellular structure form consists of two components a morphology encoder trained endtoend via a novel multichannel vqgan to learn compact d representations of cell shape and a diffusionbased perturbation trajectory module that captures how morphology evolves across perturbation conditions trained on a largescale dataset of over multifluorescence d cell volumes spanning diverse chemical and genetic perturbations form supports both unconditional morphology synthesis and conditional ulation of perturbed cell states beyond generation form can predict downstream signalling activity ulate combinatorial perturbation effects and model morphodynamic transitions between states of unseen perturbations to evaluate performance we introduce morphoeval a benchmarking suite that quantifies perturbationinduced morphological changes in structural statistical and biological dimensions together form and morphoeval work toward the realisation of the d virtual cell by linking morphology perturbation and function through highresolution predictive ulation
the alignment of visionlanguage representations endows current visionlanguage models vlms with strong multimodal reasoning capabilities however the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multimodal representations into a unified concept set to address this problem we propose vlsae a sparse autoencoder that encodes visionlanguage representations into its hidden activations each neuron in its hidden layer correlates to a concept represented by semantically ilar images and texts thereby interpreting these representations with a unified concept set to establish the neuronconcept correlation we encourage semantically ilar representations to exhibit consistent neuron activations during selfsupervised training first to measure the semantic ilarity of multimodal representations we perform their alignment in an explicit form based on cosine ilarity second we construct the vlsae with a distancebased encoder and two modalityspecific decoders to ensure the activation consistency of semantically ilar representations experiments across multiple vlms eg clip llava demonstrate the superior capability of vlsae in interpreting and enhancing the visionlanguage alignment for interpretation the alignment between vision and language representations can be understood by comparing their semantics with concepts for enhancement the alignment can be strengthened by aligning visionlanguage representations at the concept level contributing to performance improvements in downstream tasks including zeroshot image classification and hallucination elimination codes are available at
multimodal large language models mllms have shown remarkable capabilities across a wide range of visionlanguage tasks however due to the restricted input resolutions mllms face significant challenges in precisely understanding and localizing visual details in highresolution images particularly when dealing with extrasmall objects embedded in cluttered contexts to address this issue we propose textscfiners a twostage mllmbased reinforcement learning framework for jointly reasoning and segmenting extremely small objects within highresolution scenes textscfiners adopts a coarsetofine pipeline comprising global semantic exploration gse and localized perceptual refinement lpr specifically gse performs instructionguided reasoning to generate a textural response and a coarse target region while lpr refines this region to produce an accurate bounding box and segmentation mask to couple the two stages we introduce a locateinformed retrospective reward where lprs outputs are used to optimize gse for more robust coarse region exploration additionally we present textscfinersk a new dataset for evaluating mllms on attributelevel reasoning and pixellevel segmentation on subtle smallscale targets in complex highresolution scenes experimental results on textscfinersk and public datasets demonstrate that our method consistently outperforms stateoftheart mllmbased approaches on both instructionguided segmentation and visual reasoning tasks
d gaussian splatting dgs a d representation method with photorealistic realtime rendering capabilities is regarded as an effective tool for narrowing the simtoreal gap however it lacks finegrained semantics and physical executability for visuallanguage navigation vln to address this we propose saged semantically and physically aligned gaussian environments for d navigation a new paradigm that upgrades dgs into an executable semantically and physically aligned environment it comprises two components objectcentric semantic grounding which adds objectlevel finegrained annotations to dgs and physicsaware execution jointing which embeds collision objects into dgs and constructs rich physical interfaces we release interiorgs containing k objectannotated dgs indoor scene data and introduce sagebench the first dgsbased vln benchmark with m vln data experiments show that dgs scene data is more difficult to converge while exhibiting strong generalizability improving baseline performance by on the vlnce unseen task the data and code will be available soon
in this paper we introduce topology sculptor shape refiner tssr a novel method for generating highquality artiststyle d meshes based on discrete diffusion models ddms our primary motivation for tssr is to achieve highly accurate token prediction while enabling parallel generation a significant advantage over sequential autoregressive methods by allowing tssr to see all mesh tokens concurrently we unlock a new level of efficiency and control we leverage this parallel generation capability through three key innovations decoupled training and hybrid inference which distinctly separates the ddmbased generation into a topology sculpting stage and a subsequent shape refinement stage this strategic decoupling enables tssr to effectively capture both intricate local topology and overarching global shape an improved hourglass architecture featuring bidirectional attention enriched by facevertexsequence level rotational positional embeddings rope thereby capturing richer contextual information across the mesh structure a novel connection loss which acts as a topological constraint to further enhance the realism and fidelity of the generated meshes extensive experiments on complex datasets demonstrate that tssr generates highquality d artiststyle meshes capable of achieving up to faces at a remarkable spatial resolution of the code will be released at
shortcut models represent a promising nonadversarial paradigm for generative modeling uniquely supporting onestep fewstep and multistep sampling from a single trained network however their widespread adoption has been stymied by critical performance bottlenecks this paper tackles the five core issues that held shortcut models back the hidden flaw of compounding guidance which we are the first to formalize causing severe image artifacts inflexible fixed guidance that restricts inferencetime control a pervasive frequency bias driven by a reliance on lowlevel distances in the direct domain which biases reconstructions toward low frequencies divergent selfconsistency arising from a conflict with ema training and curvy flow trajectories that impede convergence to address these challenges we introduce ism a unified training framework that systematically resolves each limitation our framework is built on four key improvements intrinsic guidance provides explicit dynamic control over guidance strength resolving both compounding guidance and inflexibility a multilevel wavelet loss mitigates frequency bias to restore highfrequency details scaling optimal transport sot reduces training variance and learns straighter more stable generative paths finally a twin ema strategy reconciles training stability with selfconsistency extensive experiments on imagenet x demonstrate that our approach yields substantial fid improvements over baseline shortcut models across onestep fewstep and multistep generation making shortcut models a viable and competitive class of generative models
food analysis is becoming a hot topic in health area in which finegrained food recognition task plays an important role in this paper we describe the details of our solution to the largefinefoodaiiccv workshoprecognition challenge held on kaggle we find a proper combination of arcface loss and circle loss can bring improvement to the performance with arcface and the combined loss model was trained with carefully tuned configurations and ensembled to get the final results our solution won the rd place in the competition
this paper introduces the rd place solution to the iccv largefinefoodai retrieval competition on kaggle four basic models are independently trained with the weighted sum of arcface and circle loss then tta and ensemble are successively applied to improve feature representation ability in addition a new reranking method for retrieval is proposed based on diffusion and kreciprocal reranking finally our method scored and map on the public and private leaderboard respectively
the rapid progress of multimodal large language models mllms calls for more reliable evaluation protocols existing static benchmarks suffer from the potential risk of data contamination and saturation leading to inflated or misleading performance evaluations to address these issues we first apply graph formulation to represent a static or dynamic vqa sample with the formulation we propose knowledgeenhanced benchmark evolutionkbe a dynamic multimodal evaluation framework kbe first analyzes the original static benchmark then expands it by integrating multimodal knowledge transforming the static benchmark into a controllable dynamic evolving version crucially kbe can both reconstruct questions by reselecting visual information in the original image and expand existing questions with external textual knowledge it enables difficultycontrollable evaluation by adjusting the degree of question exploration extensive experiments demonstrate that kbe alleviates the risk of data contamination data saturation and provides a more comprehensive assessment of mllm capabilities
with the rapid growth of research in ai and robotics now producing over papers annually it has become increasingly difficult for researchers to stay up to date fast evolving trends the rise of interdisciplinary work and the need to explore domains beyond ones expertise all contribute to this challenge to address these issues we propose a generalizable pipeline capable of systematically analyzing any research area identifying emerging trends uncovering cross domain opportunities and offering concrete starting points for new inquiry in this work we present real deep research rdr a comprehensive framework applied to the domains of ai and robotics with a particular focus on foundation models and robotics advancements we also briefly extend our analysis to other areas of science the main paper details the construction of the rdr pipeline while the appendix provides extensive results across each analyzed topic we hope this work sheds light for researchers working in the field of ai and beyond
recent work by citethendrycksagidefinition formalized artificial general intelligence agi as the arithmetic mean of proficiencies across cognitive domains derived from the cattellhorncarroll chc model of human cognition while elegant this definition assumes compensability that exceptional ability in some domains can offset failure in others true general intelligence however should reflect coherent sufficiency balanced competence across all essential domains we propose a coherenceaware measure of agi based on the integral of generalized means over a continuum of compensability exponents this formulation spans arithmetic geometric and harmonic regimes and the resulting area under the curve auc quantifies robustness under varying compensability assumptions unlike the arithmetic mean which rewards specialization the auc penalizes imbalance and captures interdomain dependency applied to published chcbased domain scores for gpt and gpt the coherenceadjusted auc reveals that both systems remain far from general competence despite high arithmetic scores eg gpt at integrating the generalized mean thus yields a principled interpretable and stricter foundation for measuring genuine progress toward agi
knowledge graph question answering aims to answer natural language questions by reasoning over structured knowledge graphs while large language models have advanced kgqa through their strong reasoning capabilities existing methods continue to struggle to fully exploit both the rich knowledge encoded in kgs and the reasoning capabilities of llms particularly in complex scenarios they often assume complete kg coverage and lack mechanisms to judge when external information is needed and their reasoning remains locally myopic failing to maintain coherent multistep planning leading to reasoning failures even when relevant knowledge exists we propose graphrft a novel twostage reinforcement finetuning kgqa framework with a plankgsearchandwebsearchduringthink paradigm that enables llms to perform autonomous planning and adaptive retrieval scheduling across kg and web sources under incomplete knowledge conditions graphrft introduces a chainofthought finetuning method with a customized planretrieval dataset activates structured reasoning and resolves the grpo coldstart problem it then introduces a novel planretrieval guided reinforcement learning process integrates explicit planning and retrieval actions with a multireward design enabling coverageaware retrieval scheduling it employs a cartesianinspired planning module to decompose complex questions into ordered subquestions and logical expression to guide tool invocation for globally consistent multistep reasoning this reasoning retrieval process is optimized with a multireward combining outcome and retrieval specific signals enabling the model to learn when and how to combine kg and web retrieval effectively
evaluating the quality of reasoning traces from large language models remains understudied laborintensive and unreliable current practice relies on expert rubrics manual annotation and slow pairwise judgments automated efforts are dominated by graphbased proxies that quantify structural connectivity but do not clarify what constitutes highquality reasoning such abstractions can be overly plistic for inherently complex processes we introduce a topological data analysis tdabased evaluation framework that captures the geometry of reasoning traces and enables labelefficient automated assessment in our empirical study topological features yield substantially higher predictive power for assessing reasoning quality than standard graph metrics suggesting that effective reasoning is better captured by higherdimensional geometric structures rather than purely relational graphs we further show that a compact stable set of topological features reliably indicates trace quality offering a practical signal for future reinforcement learning algorithms
thanks to the remarkable humanlike capabilities of machine learning ml models in perceptual and cognitive tasks frameworks integrating ml within rational agent architectures are gaining traction yet the landscape remains fragmented and incoherent often focusing on embedding ml into generic agent containers while overlooking the expressive power of rational architecturessuch as beliefdesireintention bdi agents this paper presents a finegrained systematisation of existing approaches using the bdi paradigm as a reference our analysis illustrates the fastevolving literature on rational agents enhanced by ml and identifies key research opportunities and open challenges for designing effective rational ml agents
this paper introduces the fluidity index fi to quantify model adaptability in dynamic scaling environments the benchmark evaluates response accuracy based on deviations in initial current and future environment states assessing context switching and continuity we distinguish between closedended and openended benchmarks prioritizing closedloop openended realworld benchmarks to test adaptability the approach measures a models ability to understand predict and adjust to state changes in scaling environments a truly superintelligent model should exhibit at least secondorder adaptability enabling selfsustained computation through digital replenishment for optimal fluidity
large language models llms excel on generalpurpose nlp benchmarks yet their capabilities in specialized domains remain underexplored in ecommerce existing evaluationssuch as ecominstruct chineseecomqa ecellm and shopping mmlusuffer from limited task diversity eg lacking product guidance and aftersales issues limited task modalities eg absence of multimodal data synthetic or curated data and a narrow focus on english and chinese leaving practitioners without reliable tools to assess models on complex realworld shopping scenarios we introduce ecomeval a comprehensive multilingual and multimodal benchmark for evaluating llms in ecommerce ecomeval covers six categories and tasks including multimodal tasks sourced primarily from authentic customer queries and transaction logs reflecting the noisy and heterogeneous nature of real business interactions to ensure both quality and scalability of reference answers we adopt a semiautomatic pipeline in which large models draft candidate responses subsequently reviewed and modified by over expert annotators with strong ecommerce and multilingual expertise we define difficulty levels for each question and task category by averaging evaluation scores across models with different sizes and capabilities enabling challengeoriented and finegrained assessment ecomeval also spans seven languagesincluding five lowresource southeast asian languagesoffering a multilingual perspective absent from prior work
interpretablebydesign models are crucial for fostering trust accountability and safe adoption of automated decisionmaking models in realworld applications in this paper we formalize the ground for the mimosa mining interpretable models exploiting sophisticated algorithms framework a comprehensive methodology for generating predictive models that balance interpretability with performance while embedding key ethical properties we formally define here the supervised learning setting across diverse decisionmaking tasks and data types including tabular data time series images text transactions and trajectories we characterize three major families of interpretable models feature importance rule and instance based models for each family we analyze their interpretability dimensions reasoning mechanisms and complexity beyond interpretability we formalize three critical ethical properties namely causality fairness and privacy providing formal definitions evaluation metrics and verification procedures for each we then examine the inherent tradeoffs between these properties and discuss how privacy requirements fairness constraints and causal reasoning can be embedded within interpretable pipelines by evaluating ethical measures during model generation this framework establishes the theoretical foundations for developing ai systems that are not only accurate and interpretable but also fair privacypreserving and causally aware ie trustworthy
random walk centrality is a fundamental metric in graph mining for quantifying node importance and influence defined as the weighted average of hitting times to a node from all other nodes despite its ability to capture rich graph structural information and its wide range of applications computing this measure for large networks remains impractical due to the computational demands of existing methods in this paper we present a novel formulation of random walk centrality underpinning two scalable algorithms one leveraging imate cholesky factorization and sparse inverse estimation while the other sampling rooted spanning trees both algorithms operate in nearlinear time and provide strong imation guarantees extensive experiments on large realworld networks including one with over million nodes demonstrate the efficiency and imation quality of the proposed algorithms
evaluating large language models llms on finalanswer correctness is the dominant paradigm this approach however provides a coarse signal for model improvement and overlooks the quality of the underlying reasoning process we argue that a more granular evaluation of reasoning offers a more effective path to building robust models we decompose reasoning quality into two dimensions relevance and coherence relevance measures if a step is grounded in the problem coherence measures if it follows logically from prior steps to measure these aspects reliably we introduce causal stepwise evaluation case this method assesses each reasoning step using only its preceding context which avoids hindsight bias we validate case against human judgments on our new expertannotated benchmarks mragsmk and mramath more importantly we show that curating training data with caseevaluated relevance and coherence directly improves final task performance our work provides a scalable framework for analyzing debugging and improving llm reasoning demonstrating the practical value of moving beyond validity checks
network topology optimization nto via busbar splitting can mitigate transmission grid congestion and reduce redispatch costs however solving this mixedinteger nonlinear problem for largescale systems in nearrealtime is currently intractable with existing solvers machine learning ml approaches have emerged as a promising alternative but they have limited generalization to unseen topologies varying operating conditions and different systems which limits their practical applicability this paper formulates nto for congestion management problem considering linearized ac pf and proposes a graph neural network gnnaccelerated approach we develop a heterogeneous edgeaware message passing nn to predict effective busbar splitting actions as candidate nto solutions the proposed gnn captures local flow patterns achieves generalization to unseen topology changes and improves transferability across systems case studies show up to ordersofmagnitude speedup delivering acfeasible solutions within one minute and a optimality gap on the goc bus system these results demonstrate a significant step toward nearrealtime nto for largescale systems with topology and crosssystem generalization
the worlds people have strong opinions about artificial intelligence ai and they want policymakers to listen governments are inviting public comment on ai but as they translate input into policy much of what citizens say is lost policymakers are missing a critical opportunity to build trust in ai and its governance this paper compares three countries australia colombia and the united states that invited citizens to comment on ai risks and policies using a landscape analysis the authors examined how each government solicited feedback and whether that input shaped governance yet in none of the three cases did citizens and policymakers establish a meaningful dialogue governments did little to attract diverse voices or publicize calls for comment leaving most citizens unaware or unprepared to respond in each nation fewer than one percent of the population participated moreover officials showed limited responsiveness to the feedback they received failing to create an effective feedback loop the study finds a persistent gap between the promise and practice of participatory ai governance the authors conclude that current approaches are unlikely to build trust or legitimacy in ai because policymakers are not adequately listening or responding to public concerns they offer eight recommendations promote ai literacy monitor public feedback broaden outreach hold regular online forums use innovative engagement methods include underrepresented groups respond publicly to input and make participation easier
knowledge graph alignment is the task of matching equivalent entities that is instances and classes and relations across two knowledge graphs most existing methods focus on pure entitylevel alignment computing the ilarity of entities in some embedding space they lack interpretable reasoning and need training data to work in this paper we propose flora a ple yet effective method that is unsupervised ie does not require training data provides a holistic alignment for entities and relations iteratively is based on fuzzy logic and thus delivers interpretable results provably converges allows dangling entities ie entities without a counterpart in the other kg and achieves stateoftheart results on major benchmarks
concept learning exploits background knowledge in the form of description logic axioms to learn explainable classification models from knowledge bases despite recent breakthroughs in neurosymbolic concept learning most approaches still cannot be deployed on realworld knowledge bases this is due to their use of description logic reasoners which are not robust against inconsistencies nor erroneous data we address this challenge by presenting a novel neural reasoner dubbed ebr our reasoner relies on embeddings to imate the results of a symbolic reasoner we show that ebr solely requires retrieving instances for atomic concepts and existential restrictions to retrieve or imate the set of instances of any concept in the description logic mathcalshoiq in our experiments we compare ebr with stateoftheart reasoners our results suggest that ebr is robust against missing and erroneous data in contrast to existing reasoners
this paper presents a new computational model of creative outcomes informed by creativity theories and techniques which was implemented to generate more novel opportunities for innovation projects the model implemented five functions that were developed to contribute to the generation of innovation opportunities with higher novelty without loss of usefulness the model was evaluated using opportunities generated for an innovation project in the hospitality sector the evaluation revealed that the computational model generated outcomes that were more novel andor useful than outcomes from notebook lm and chatgpto however not all model functions contributed to the generation of more novel opportunities leading to new directions for further model development
continual pretraining promises to adapt large language models llms to new domains using only unlabeled testtime data but naively applying standard selfsupervised objectives to instructiontuned models is known to degrade their instructionfollowing capability and semantic representations existing fixes assume access to the original base model or rely on knowledge from an external domainspecific database both of which pose a realistic barrier in settings where the base model weights are withheld for safety reasons or reliable external corpora are unavailable in this work we propose instructionknowledgeaware continual adaptation iknow a ple and general framework that formulates novel selfsupervised objectives in the instructionresponse dialogue format rather than depend ing on external resources iknow leverages domain knowledge embedded within the text itself and learns to encode it at a deeper semantic level
knowledge graphs kgs have long served as a fundamental infrastructure for structured knowledge representation and reasoning with the advent of large language models llms the construction of kgs has entered a new paradigmshifting from rulebased and statistical pipelines to languagedriven and generative frameworks this survey provides a comprehensive overview of recent progress in llmempowered knowledge graph construction systematically analyzing how llms reshape the classical threelayered pipeline of ontology engineering knowledge extraction and knowledge fusion we first revisit traditional kg methodologies to establish conceptual foundations and then review emerging llmdriven approaches from two complementary perspectives schemabased paradigms which asize structure normalization and consistency and schemafree paradigms which highlight flexibility adaptability and open discovery across each stage we synthesize representative frameworks analyze their technical mechanisms and identify their limitations finally the survey outlines key trends and future research directions including kgbased reasoning for llms dynamic knowledge memory for agentic systems and multimodal kg construction through this systematic review we aim to clarify the evolving interplay between llms and knowledge graphs bridging symbolic knowledge engineering and neural semantic understanding toward the development of adaptive explainable and intelligent knowledge systems
in an era where ai artificial intelligence systems play an increasing role in the battlefield ensuring responsible targeting demands rigorous assessment of potential collateral effects in this context a novel collateral damage assessment model for target engagement of ai systems in military operations is introduced the model integrates temporal spatial and force dimensions within a unified knowledge representation and reasoning krr architecture following a design science methodological approach its layered structure captures the categories and architectural components of the ai systems to be engaged together with corresponding engaging vectors and contextual aspects at the same time spreading severity likelihood and evaluation metrics are considered in order to provide a clear representation enhanced by transparent reasoning mechanisms further the model is demonstrated and evaluated through instantiation which serves as a basis for further dedicated efforts that aim at building responsible and trustworthy intelligent systems for assessing the effects produced by engaging ai systems in military operations
artificial intelligence ai holds great promise for transforming healthcare however despite significant advances the integration of ai solutions into realworld clinical practice remains limited a major barrier is the quality and fairness of training data which is often compromised by biased data collection practices this paper draws on insights from the aihealthyaging project part of spains national rd initiative where our task was to detect biases during clinical data collection we identify several types of bias across multiple use cases including historical representation and measurement biases these biases manifest in variables such as sex gender age habitat socioeconomic status equipment and labeling we conclude with practical recommendations for improving the fairness and robustness of clinical problem design and data collection we hope that our findings and experience contribute to guiding future projects in the development of fairer ai systems in healthcare
embodied question answering eqa requires agents to explore d environments to obtain observations and answer questions related to the scene existing methods leverage vlms to directly explore the environment and answer questions without explicit thinking or planning which limits their reasoning ability and results in excessive or inefficient exploration as well as ineffective responses in this paper we introduce tooleqa an agent that integrates external tools with multistep reasoning where external tools can provide more useful information for completing the task helping the model derive better exploration directions in the next step of reasoning and thus obtaining additional effective information this enables tooleqa to generate more accurate responses with a shorter exploration distance to enhance the models ability for toolusage and multistep reasoning we further design a novel eqa data generation pipeline that automatically constructs largescale eqa tasks with reasoning trajectories and corresponding answers based on the pipeline we collect the eqart dataset that contains about k tasks divided into a training set eqarttrain and two test sets eqartseen scenes overlapping with the training set and eqartunseen novel scenes experiments on eqartseen and eqartunseen show that tooleqa improves the success rate by over stateoftheart baselines while outperforming the zeroshot tooleqa by in success rate in addition tooleqa also achieves stateoftheart performance on the hmeqa openeqa and expressbench datasets demonstrating its generality our homepage see
human mobility forecasting is crucial for disaster relief city planning and public health however existing models either only model location sequences or include time information merely as auxiliary input thereby failing to leverage the rich semantic context provided by points of interest pois to address this we enrich a bertbased mobility model with derived temporal descriptors and poi embeddings to better capture the semantics underlying human movement we propose stabert semantictemporal aware bert which integrates both poi and temporal information at each location to construct a unified semantically enriched representation of mobility experimental results show that stabert significantly improves prediction accuracy for singlecity prediction the geobleu score improved from to for multicity prediction from to
generating an abstraction of a dynamic domain that aligns with a given purpose remains a significant challenge given that the choice of such an abstraction can impact an agents ability to plan reason and provide explanations effectively we model the agents concrete behaviors in pddl and investigate the use of incontext learning with large language models llms for the generation of abstract pddl domains and problem instances given an abstraction objective specified in natural language the benchmark examples we use are new and have not been part of the data any llms have been trained on we consider three categories of abstractions abstraction of choice of alternative concrete actions abstraction of sequences of concrete actions and abstraction of actionpredicate parameters as well as combinations of these the generated abstract pddl domains and problem instances are then checked by symbolic validation tools as well as human experts our experiments show that gpto can generally synthesize useful planning domain abstractions in ple settings although it is better at abstracting over actions than over the associated fluents
individualized cognitive ulation ics aims to build computational models that imate the thought processes of specific individuals while large language models llms convincingly mimic surfacelevel human behavior such as roleplay their ability to ulate deeper individualized cognitive processes remains poorly understood to address this gap we introduce a novel task that evaluates different cognitive representation methods in ics we construct a dataset from recently published novels later than the release date of the tested llms and propose an condition cognitive evaluation framework to benchmark seven offtheshelf llms in the context of authorial style emulation we hypothesize that effective cognitive representations can help llms generate storytelling that better mirrors the original author thus we test different cognitive representations eg linguistic features concept mappings and profilebased information results show that combining conceptual and linguistic features is particularly effective in ics outperforming static profilebased cues in overall evaluation importantly llms are more effective at mimicking linguistic style than narrative structure underscoring their limits in deeper cognitive ulation these findings provide a foundation for developing ai systems that adapt to individual ways of thinking and expression advancing more personalized and humanaligned creative technologies
optimizing artificial intelligence ai for dynamic environments remains a fundamental challenge in machine learning research in this paper we examine evolutionary training methods for optimizing ai to solve the game a d sliding puzzle with its mix of strategic gameplay and stochastic elements presents an ideal playground for studying decisionmaking longterm planning and dynamic adaptation we implemented two distinct systems a twoagent metaprompting system where a thinker large language model llm agent refines gameplay strategies for an executor llm agent and a singleagent system based on refining a value function for a limited monte carlo tree search we also experimented with rollback features to avoid performance degradation our results demonstrate the potential of evolutionary refinement techniques in improving ai performance in nondeterministic environments the singleagent system achieved substantial improvements with an average increase of points per cycle and with clear upward trends correlation rho across training cycles the llms understanding of the game grew as well shown in its development of increasingly advanced strategies conversely the twoagent system did not garner much improvement highlighting the inherent limits of metaprompting
large language models llms remain broadly open and highly steerable they imitate at scale accept arbitrary system prompts and readily adopt multiple personae by analogy to human development we hypothesize that progress toward artificial general intelligence agi involves a lockin phase a transition from open imitation to identity consolidation in which goal structures refusals preferences and internal representations become comparatively stable and resistant to external steering we formalize this phase link it to known phenomena in learning dynamics and propose operational metrics for onset detection experimentally we demonstrate that while the behavioral consolidation is rapid and nonlinear its sideeffects on general capabilities are not monolithic our results reveal a spectrum of outcomesfrom performance tradeoffs in small models through largely costfree adoption in midscale models to transient instabilities in large quantized models we argue that such consolidation is a prerequisite for agilevel reliability and also a critical control point for safety identities can be deliberately engineered for reliability yet may also emerge spontaneously during scaling potentially hardening unpredictable goals and behaviors
large language models generate complex reasoning chains that reveal their decisionmaking yet verifying the faithfulness and harmlessness of these intermediate steps remains a critical unsolved problem existing auditing methods are centralized opaque and hard to scale creating significant risks for deploying proprietary models in highstakes domains we identify four core challenges robustness centralized auditors are single points of failure prone to bias or attacks scalability reasoning traces are too long for manual verification opacity closed auditing undermines public trust privacy exposing full reasoning risks model theft or distillation we propose trust a transparent decentralized auditing framework that overcomes these limitations via a consensus mechanism among diverse auditors guaranteeing correctness under up to malicious participants a hierarchical dag decomposition of reasoning traces enabling scalable parallel auditing a blockchain ledger that records all verification decisions for public accountability privacypreserving segmentation sharing only partial reasoning steps to protect proprietary logic we provide theoretical guarantees for the security and economic incentives of the trust framework experiments across multiple llms gptoss deepseekr qwen and reasoning tasks math medical science humanities show trust effectively detects reasoning flaws and remains robust against adversarial auditors our work pioneers decentralized ai auditing offering a practical path toward safe and trustworthy llm deployment
it is often claimed that machine learningbased generative ai products will drastically streamline and reduce the cost of legal practice this enthusiasm assumes lawyers can effectively manage ais risks cases in australia and elsewhere in which lawyers have been reprimanded for submitting inaccurate aigenerated content to courts suggest this paradigm must be revisited this paper argues that a new paradigm is needed to evaluate ai use in practice given a ais disconnection from reality and its lack of transparency and b lawyers paramount duties like honesty integrity and not to mislead the court it presents an alternative model of ai use in practice that more holistically reflects these features the verificationvalue paradox that paradox suggests increases in efficiency from ai use in legal practice will be met by a correspondingly greater imperative to manually verify any outputs of that use rendering the net value of ai use often negligible to lawyers the paper then sets out the paradoxs implications for legal practice and legal education including for ai use but also the values that the paradox suggests should undergird legal practice fidelity to the truth and civic responsibility
we present hcla a humancentered multiagent system for anomaly detection in digital asset transactions the system links three roles parsing detection and explanation into a conversational workflow that lets nonexperts ask questions in natural language inspect structured analytics and obtain contextaware rationales implemented with an opensource web ui hcla translates user intents into a schema for a classical detector xgboost in our prototype and returns narrative explanations grounded in the underlying features on a labeled bitcoin mixing dataset wasabi wallet the baseline detector reaches strong accuracy while hcla adds interpretability and interactive refinement we describe the architecture interaction loop dataset evaluation protocol and limitations and discuss how a humanintheloop design improves transparency and trust in financial forensics
we present ai pb a productionscale generative agent deployed in real retail finance unlike reactive chatbots that answer queries passively ai pb proactively generates grounded compliant and userspecific investment insights it integrates i a componentbased orchestration layer that deterministically routes between internal and external llms based on data sensitivity ii a hybrid retrieval pipeline using opensearch and the financedomain embedding model and iii a multistage recommendation mechanism combining rule heuristics sequential behavioral modeling and contextual bandits operating fully onpremises under korean financial regulations the system employs docker swarm and vllm across x nvidia h gpus through human qa and system metrics we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy ai insights in highstakes finance
a meaningful text can be hidden inside another completely different yet still coherent and plausible text of the same length for example a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader or an ordinary product review could conceal a secret manuscript this uncanny state of affairs is now possible thanks to large language models and in this paper we present a ple and efficient protocol to achieve it we show that even modest billionparameter opensource llms are sufficient to obtain highquality results and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds the existence of such a protocol demonstrates a radical decoupling of text from authorial intent further eroding trust in written communication already shaken by the rise of llm chatbots we illustrate this with a concrete scenario a company could covertly deploy an unfiltered llm by encoding its answers within the compliant responses of a safe model this possibility raises urgent questions for ai safety and challenges our understanding of what it means for a large language model to know something
the study explores the potential of ai technologies in personalized learning suggesting the prediction of academic success through leadership personality traits and machine learning modelling the primary data were obtained from masters students in the environmental engineering department who underwent five leadership personality tests with characteristics students used selfassessment tools that included personality insight workplace culture motivation at work management skills and emotion control tests the test results were combined with the average grade obtained from academic reports the study employed exploratory data analysis and correlation analysis feature selection utilized pearson correlation coefficients of personality traits the average grades were separated into three categories fail pass and excellent the modelling process was performed by tuning seven ml algorithms such as svm lr knn dt gb rf xgboost and lightgbm the highest predictive performance was achieved with the rf classifier which yielded an accuracy of for the model incorporating personality trait features and the leadership mark feature and an accuracy of for the model excluding this feature in this way the study offers an additional opportunity to identify students strengths and weaknesses at an early stage of their education process and select the most suitable strategies for personalized learning
generative ai is supercharging insurance fraud by making it easier to falsify accident evidence at scale and in rapid time insurance fraud is a pervasive and costly problem amounting to tens of billions of dollars in losses each year in the vehicle insurance sector fraud schemes have traditionally involved staged accidents exaggerated damage or forged documents the rise of generative ai including deepfake image and video generation has introduced new methods for committing fraud at scale fraudsters can now fabricate highly realistic crash photos damage evidence and even fake identities or documents with minimal effort exploiting ai tools to bolster false insurance claims insurers have begun deploying countermeasures such as aibased deepfake detection software and enhanced verification processes to detect and mitigate these aidriven scams however current mitigation strategies face significant limitations detection tools can suffer from false positives and negatives and sophisticated fraudsters continuously adapt their tactics to evade automated checks this catandmouse arms race between generative ai and detection technology combined with resource and cost barriers for insurers means that combating aienabled insurance fraud remains an ongoing challenge in this white paper we present uveye layered solution for vehicle fraud representing a major leap forward in the ability to detect mitigate and deter this new wave of fraud
relational multitable data is common in domains such as ecommerce healthcare and scientific research and can be naturally represented as heterogeneous temporal graphs with multimodal node attributes existing graph neural networks gnns rely on schemaspecific feature encoders requiring separate modules for each node type and feature column which hinders scalability and parameter sharing we introduce relate relational encoder for latent aggregation of typed entities a schemaagnostic plugandplay feature encoder that can be used with any general purpose gnn relate employs shared modalityspecific encoders for categorical numerical textual and temporal attributes followed by a perceiverstyle crossattention module that aggregates features into a fixedsize permutationinvariant node representation we evaluate relate on relgnn and hgt in the relbench benchmark where it achieves performance within of schemaspecific encoders while reducing parameter counts by up to x this design supports varying schemas and enables multidataset pretraining for generalpurpose gnns paving the way toward foundation models for relational graph data
building agents that generalize across web desktop and mobile environments remains an open challenge as prior systems rely on environmentspecific interfaces that limit crossplatform deployment we introduce surfer a unified architecture operating purely from visual observations that achieves stateoftheart performance across all three environments surfer integrates hierarchical context management decoupled planning and execution and selfverification with adaptive recovery enabling reliable operation over long task horizons our system achieves accuracy on webvoyager on webarena on osworld and on androidworld outperforming all prior systems without taskspecific finetuning with multiple attempts surfer exceeds human performance on all benchmarks these results demonstrate that systematic orchestration amplifies foundation model capabilities and enables generalpurpose computer control through visual interaction alone while calling for a nextgeneration vision language model to achieve paretooptimal costefficiency
large language models llms demonstrate strong performance on mathematical problems when prompted with chainofthought cot yet it remains unclear whether this success stems from search rote procedures or ruleconsistent reasoning to address this we propose modeling cot as a certain rulebased stochastic process over directed acyclic graphs dags where nodes represent intermediate derivation states and edges encode rule applications within this framework we introduce logical closeness a metric that quantifies how well a models cot trajectory ie the llms final output adheres to the dag structure providing evaluation beyond classical passk metrics building on this we introduce the dagmath cot format and construct a benchmark that guides llms to generate cot trajectories in this format thereby enabling the evaluation of their reasoning ability under our framework across standard mathematical reasoning datasets our analysis uncovers statistically significant differences in reasoning fidelity among representative llm familieseven when passk is comparablehighlighting gaps between finalanswer accuracy and ruleconsistent derivation our framework provides a balance between freeform cot and formal proofs systems offering actionable diagnostics for llms reasoning evaluation our benchmark and code are available at
autonomous web agents powered by large language models llms show strong potential for performing goaloriented tasks such as information retrieval report generation and online transactions these agents mark a key step toward practical embodied reasoning in open web environments however existing approaches remain limited in reasoning depth and efficiency vanilla linear methods fail at multistep reasoning and lack effective backtracking while other search strategies are coarsegrained and computationally costly we introduce branchandbrowse a finegrained web agent framework that unifies structured reasoningacting contextual memory and efficient execution it i employs explicit subtask management with treestructured exploration for controllable multibranch reasoning ii bootstraps exploration through efficient web state replay with background reasoning and iii leverages a page action memory to share explored actions within and across sessions on the webarena benchmark branchandbrowse achieves a task success rate of and reduces execution time by up to relative to stateoftheart methods these results demonstrate that branchandbrowse is a reliable and efficient framework for llmbased web agents
artificial intelligence and machine learning are increasingly used for forecasting optimization and policy design in the energy sector yet no standardized framework exists to evaluate whether these systems reason correctly current validation practices focus on predictive accuracy or computational efficiency leaving the logical integrity of analytical conclusions untested this study introduces the analytical reliability benchmark arb a reproducible framework that quantifies reasoning reliability in large language models applied to energy system analysis the benchmark integrates five submetrics accuracy reasoning reliability uncertainty discipline policy consistency and transparency and evaluates model performance across deterministic probabilistic and epistemic scenarios using open technoeconomic datasets nrel atb doe hahnew iea weo four frontier models gpt claude sonnet gemini pro llama b were tested under identical factual and regulatory conditions results show that reasoning reliability can be objectively measured gpt and claude sonnet achieved consistent and policycompliant reasoning analytical reliability index greater than gemini pro demonstrated moderate stability and llama b remained below professional thresholds statistical validation confirmed that these differences are significant and reproducible the arb establishes the first quantitative method in the energy literature for verifying causal probabilistic and policydriven reasoning in artificial intelligence systems providing a reference framework for trustworthy and transparent analytical applications in the global energy transition
we propose and evaluate a quantuminspired algorithm for solving quadratic unconstrained binary optimization qubo problems which are mathematically equivalent to finding ground states of ising spinglass hamiltonians the algorithm employs matrix product states mps to compactly represent large superpositions of spin configurations and utilizes a discrete driving schedule to guide the mps toward the ground state at each step a driver hamiltonian incorporating a transverse magnetic field is combined with the problem hamiltonian to enable spin flips and facilitate quantum tunneling the mps is updated using the standard density matrix renormalization group dmrg method which iteratively minimizes the systems energy via multiple sweeps across the spin chain despite its heuristic nature the algorithm reliably identifies global minima not merely nearoptimal solutions across diverse qubo instances we first demonstrate its effectiveness on intermediatelevel sudoku puzzles from publicly available sources involving over ising spins with longrange couplings dictated by constraint satisfaction we then apply the algorithm to maxcut problems from the biq mac library successfully solving instances with up to nodes and edges we discuss the advantages of this quantuminspired approach including its scalability generalizability and suitability for industrialscale qubo applications
recent advances in generative modeling have positioned diffusion models as stateoftheart tools for sampling from complex data distributions while these models have shown remarkable success across singlemodality domains such as images and audio extending their capabilities to modality translation mt translating information across different sensory modalities remains an open challenge existing approaches often rely on restrictive assumptions including shared dimensionality gaussian source priors and modalityspecific architectures which limit their generality and theoretical grounding in this work we propose the latent denoising diffusion bridge model lddbm a generalpurpose framework for modality translation based on a latentvariable extension of denoising diffusion bridge models by operating in a shared latent space our method learns a bridge between arbitrary modalities without requiring aligned dimensions we introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domainagnostic encoderdecoder architecture tailored for noise prediction in latent space additionally we propose a predictive loss to guide training toward accurate crossdomain translation and explore several training strategies to improve stability our approach supports arbitrary modality pairs and performs strongly on diverse mt tasks including multiview to d shape generation image superresolution and multiview scene synthesis comprehensive experiments and ablations validate the effectiveness of our framework establishing a new strong baseline in general modality translation for more information see our project page
a fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment eg quadrupeds can walk up stairs but rovers cannot we propose vamos a hierarchical vla that decouples semantic planning from embodiment grounding a generalist planner learns from diverse openworld data while a specialist affordance model learns the robots physical constraints and capabilities in safe lowcost ulation we enabled this separation by carefully designing an interface that lets a highlevel planner propose candidate paths directly in image space that the affordance model then evaluates and reranks our realworld experiments show that vamos achieves higher success rates in both indoor and complex outdoor navigation than stateoftheart modelbased and endtoend learning methods we also show that our hierarchical design enables crossembodied navigation across legged and wheeled robots and is easily steerable using natural language realworld ablations confirm that the specialist model is key to embodiment grounding enabling a single highlevel planner to be deployed across physically distinct wheeled and legged robots finally this model significantly enhances singlerobot reliability achieving x higher success rates by rejecting physically infeasible plans website
this paper presents gsworld a robust photorealistic ulator for robotics manipulation that combines d gaussian splatting with physics engines our framework advocates closing the loop of developing manipulation policies with reproducible evaluation of policies learned from realrobot data and simreal policy training without using real robots to enable photorealistic rendering of diverse scenes we propose a new asset format which we term gsdf gaussian scene description file that infuses gaussianonmesh representation with robot urdf and other objects with a streamlined reconstruction pipeline we curate a database of gsdf that contains robot embodiments for singlearm and bimanual manipulation as well as more than objects combining gsdf with physics engines we demonstrate several immediate interesting applications learning zeroshot simreal pixeltoaction manipulation policy with photorealistic rendering automated highquality dagger data collection for adapting policies to deployment environments reproducible benchmarking of realrobot manipulation policies in ulation ulation data collection by virtual teleoperation and zeroshot simreal visual reinforcement learning website
large visionlanguage models vlms have achieved remarkable progress in multimodal understanding yet they struggle when reasoning over informationintensive images that densely interleave textual annotations with finegrained graphical elements the main challenges lie in precisely localizing critical cues in dense layouts and multihop reasoning to integrate dispersed evidence we propose speculative verdict sv a trainingfree framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model in the draft stage small vlms act as draft experts to generate reasoning paths that provide diverse localization candidates in the verdict stage a strong vlm synthesizes these paths to produce the final answer minimizing computational cost while recovering correct answers to further improve efficiency and accuracy sv introduces a consensus expert selection mechanism that forwards only highagreement reasoning paths to the verdict empirically sv achieves consistent gains on challenging informationintensive and highresolution visual question answering benchmarks including infographicvqa chartmuseum chartqapro and hrbench k by synthesizing correct insights from multiple partially accurate reasoning paths sv achieves both error correction and costefficiency compared to large proprietary models or training pipelines code is available at
with the widespread use of large language models llms many researchers have turned their attention to detecting text generated by them however there is no consistent or precise definition of their target namely llmgenerated text differences in usage scenarios and the diversity of llms further increase the difficulty of detection what is commonly regarded as the detecting target usually represents only a subset of the text that llms can potentially produce human edits to llm outputs together with the subtle influences that llms exert on their users are blurring the line between llmgenerated and humanwritten text existing benchmarks and evaluation approaches do not adequately address the various conditions in realworld detector applications hence the numerical results of detectors are often misunderstood and their significance is diminishing therefore detectors remain useful under specific conditions but their results should be interpreted only as references rather than decisive indicators
machine learning has facilitated significant advancements across various robotics domains including navigation locomotion and manipulation many such achievements have been driven by the extensive use of ulation as a critical tool for training and testing robotic systems prior to their deployment in realworld environments however ulations consist of abstractions and imations that inevitably introduce discrepancies between ulated and real environments known as the reality gap these discrepancies significantly hinder the successful transfer of systems from ulation to the real world closing this gap remains one of the most pressing challenges in robotics recent advances in simtoreal transfer have demonstrated promising results across various platforms including locomotion navigation and manipulation by leveraging techniques such as domain randomization realtosim transfer state and action abstractions and simreal cotraining many works have overcome the reality gap however challenges persist and a deeper understanding of the reality gaps root causes and solutions is necessary in this survey we present a comprehensive overview of the simtoreal landscape highlighting the causes solutions and evaluation metrics for the reality gap and simtoreal transfer
recently sharma et al suggested a method called layerselectiverank reduction laser which demonstrated that pruning highorder components of carefully chosen llms weight matrices can boost downstream accuracy without any gradientbased finetuning yet lasers exhaustive permatrix search each requiring fulldataset forward passes makes it impractical for rapid deployment we demonstrate that this overhead can be removed and find that i only a small carefully chosen subset of matrices needs to be inspected eliminating the layerbylayer sweep ii the gradient of each matrixs singular values pinpoints which matrices merit reduction iii increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to percentage points and finally iv we discover that evaluating on just samples rather than the full training data both for computing the indicative gradients and for measuring the final accuracy suffices to further reduce the search time we explain that as adaptation to downstream tasks is dominated by prompting style not dataset size as a result we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks overall with a single gradient step on examples and a quick scan of the top candidate layers and factorization techniques we can adapt llms to new datasets entirely without finetuning
a common strategy to reduce the computational costs of using long contexts in retrievalaugmented generation rag with large language models llms is soft context compression where the input sequence is transformed into a shorter continuous representation we develop a lightweight and ple meanpooling approach that consistently outperforms the widely used compressiontokens architecture and study training the same compressor to output multiple compression ratios we conduct extensive experiments across indomain and outofdomain qa datasets as well as across model families scales and compression ratios overall our ple meanpooling approach achieves the strongest performance with a relatively small drop when training for multiple compression ratios more broadly though across architectures and training regimes the tradeoffs are more nuanced illustrating the complex landscape of compression methods
deep learning has emerged as a transformative methodology in modern cosmology providing powerful tools to extract meaningful physical information from complex astronomical datasets this paper implements a novel bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field pmf cosmology directly from ulated cosmic microwave background cmb maps our methodology utilizes deepsphere a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of cmb data through healpix pixelization to advance beyond deterministic point estimates and enable robust uncertainty quantification we integrate bayesian neural networks bnns into the framework capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions the proposed approach demonstrates exceptional performance achieving r scores exceeding for the magnetic parameter estimation we further obtain wellcalibrated uncertainty estimates through posthoc training techniques including variance scaling and gpnormal this integrated deepspherebnns framework not only delivers accurate parameter estimation from cmb maps with pmf contributions but also provides reliable uncertainty quantification providing the necessary tools for robust cosmological inference in the era of precision cosmology
current methods for evaluating large language models llms typically focus on highlevel tasks such as text generation without targeting a particular ai application this approach is not sufficient for evaluating llms for responsible ai dimensions like fairness since protected attributes that are highly relevant in one application may be less relevant in another in this work we construct a dataset that is driven by a realworld application generate a plaintext product description given a list of product features parameterized by fairness attributes intersected with gendered adjectives and product categories yielding a rich set of labeled prompts we show how to use the data to identify quality veracity safety and fairness gaps in llms contributing a proposal for llm evaluation paired with a concrete resource for the research community
recent advancements in large reasoning models lrms have introduced an intermediate thinking process prior to generating final answers improving their reasoning capabilities on complex downstream tasks however the potential of lrms as evaluators for machine translation mt quality remains underexplored we provides the first systematic analysis of lrmasajudge in mt evaluation we identify key challenges revealing lrms require tailored evaluation materials tend to overthink pler instances and have issues with scoring mechanisms leading to overestimation to address these we propose to calibrate lrm thinking by training them on synthetic humanlike thinking trajectories our experiments on wmt metrics benchmarks demonstrate that this approach largely reduces thinking budgets by x while concurrently improving evaluation performance across different lrm scales from b to b eg rdistillqwenb achieves a correlation point improvement these findings highlight the potential of efficiently calibrated lrms to advance finegrained automatic mt evaluation
largescale and diverse datasets are vital for training robust robotic manipulation policies yet existing data collection methods struggle to balance scale diversity and quality ulation offers scalability but suffers from simtoreal gaps while teleoperation yields highquality demonstrations with limited diversity and high labor cost we introduce fieldgen a fieldguided data generation framework that enables scalable diverse and highquality realworld data collection with minimal human supervision fieldgen decomposes manipulation into two stages a premanipulation phase allowing trajectory diversity and a fine manipulation phase requiring expert precision human demonstrations capture key contact and pose information after which an attraction field automatically generates diverse trajectories converging to successful configurations this decoupled design combines scalable trajectory diversity with precise supervision moreover fieldgenreward augments generated data with reward annotations to further enhance policy learning experiments demonstrate that policies trained with fieldgen achieve higher success rates and improved stability compared to teleoperationbased baselines while significantly reducing human effort in longterm realworld data collection webpage is available at
modellearning agents should gather information to learn world models that support many downstream tasks and inferences such as predicting unobserved states estimating near and farterm consequences of actions planning action sequences and detecting changes in dynamics current methods for learning and evaluating world models diverge from this goal training and evaluation are anchored to nextframe prediction and success is scored by reward maximization in the same environment we propose worldtest a protocol to evaluate modellearning agents that separates rewardfree interaction from a scored test phase in a different but related environment worldtest is openended models should support many different tasks unknown ahead of time and agnostic to model representation allowing comparison across approaches we instantiated worldtest with autumnbench a suite of interactive gridworld environments and tasks across three families maskedframe prediction planning and predicting changes to the causal dynamics we compared human participants and three frontier models on autumnbench we found that humans outperform the models and scaling compute improves performance only in some environments but not others worldtest provides a novel template rewardfree exploration derived tests and behaviorbased scoring to evaluate what agents learn about environment dynamics and autumnbench exposes significant headroom in worldmodel learning
llmbased agents are increasingly moving towards proactivity rather than awaiting instruction they exercise agency to anticipate user needs and solve them autonomously however evaluating proactivity is challenging current benchmarks are constrained to localized context limiting their ability to test reasoning across sources and longer time horizons to address this gap we present probe proactive resolution of bottlenecks probe decomposes proactivity as a pipeline of three core capabilities searching for unspecified issues identifying specific bottlenecks and executing appropriate resolutions we apply probe to evaluate leading llms and popular agentic frameworks showing that even stateoftheart models struggle to solve this benchmark computing our consistent measurements across frontier llms and agents we find that the best endtoend performance of is achieved by both gpt and claude opus additionally we demonstrate the relative capabilities of each model and analyze mutual failure modes our results highlight the current limitations of autonomous action in agentic systems and expose promising future research directions
advanced ai systems sometimes act in ways that differ from human intent to gather clear reproducible examples we ran the misalignment bounty a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals the bounty received submissions of which nine were awarded this report explains the programs motivation and evaluation criteria and walks through the nine winning submissions step by step
to enable embodied agents to operate effectively over extended timeframes it is crucial to develop models that form and access memories to stay contextualized in their environment in the current paradigm of training transformerbased policies for embodied sequential decisionmaking tasks visual inputs often overwhelm the context limits of transformers while humans can maintain and utilize a lifetime of experience compressed as memories significant compression is possible in principle as much of the input is irrelevant and can be abstracted however existing approaches predominantly focus on either recurrent models with fixedsize memory or transformers with fullcontext reliance in this work we propose memo a transformerbased architecture and training recipe for reinforcement learning rl on memoryintensive longhorizon tasks memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training we demonstrate memos effectiveness on a gridworld metarl benchmark and a multiobject navigation task in photorealistic indoor settings memo outperforms naive longcontext transformer baselines while being more compute and storage efficient additionally memo generalizes better to longer contexts at inference time and remains robust in streaming settings where historical context must be truncated to fit inference constraints
large language models llms can propose rules in natural language sidestepping the need for a predefined predicate space in traditional rule learning yet many llmbased approaches ignore interactions among rules and the opportunity to couple llms with probabilistic rule learning for robust inference remains underexplored we present rlie a unified framework that integrates llms with probabilistic modeling to learn a set of weighted rules rlie has four stages rule generation where an llm proposes and filters candidates logistic regression which learns probabilistic weights for global selection and calibration iterative refinement which updates the rule set using prediction errors and evaluation which compares the weighted rule set as a direct classifier with methods that inject rules into an llm we evaluate multiple inference strategies on realworld datasets applying rules directly with their learned weights yields superior performance whereas prompting llms with the rules weights and logisticmodel outputs surprisingly degrades accuracy this supports the view that llms excel at semantic generation and interpretation but are less reliable for precise probabilistic integration rlie clarifies the potential and limitations of llms for inductive reasoning and couples them with classic probabilistic rule combination methods to enable more reliable neurosymbolic reasoning
the increasing number of spectators and players in esports along with the development of optimized communication solutions and cloud computing technology has motivated the constant growth of the online game industry even though artificial intelligencebased solutions for esports analytics are traditionally defined as extracting meaningful patterns from related data and visualizing them to enhance decisionmaking most of the effort in professional winning prediction has been focused on the classification aspect from a batch perspective also leaving aside the visualization techniques consequently this work contributes to an explainable win prediction classification solution in streaming in which input data is controlled over several sliding windows to reflect relevant game changes experimental results attained an accuracy higher than surpassing the performance of competing solutions in the literature ultimately our system can be leveraged by ranking and recommender systems for informed decisionmaking thanks to the explainability module which fosters trust in the outcome predictions
we present a graphbased engine for computing chord tone soloing suggestions for guitar students chord tone soloing is a fundamental practice for improvising over a chord progression where the instrumentalist uses only the notes contained in the current chord this practice is a building block for all advanced jazz guitar theory but is difficult to learn and practice first we discuss methods for generating chordtone arpeggios next we construct a weighted graph where each node represents a chord tone arpeggio for a chord in the progression then we calculate the edge weight between each consecutive chords nodes in terms of optimal transition tones we then find the shortest path through this graph and reconstruct a chordtone soloing line finally we discuss a userfriendly system to handle input and output to this engine for guitar students to practice chord tone soloing
webbased participatory urban sensing has emerged as a vital approach for modern urban management by leveraging mobile individuals as distributed sensors however existing urban sensing systems struggle with limited generalization across diverse urban scenarios and poor interpretability in decisionmaking in this work we introduce agentsense a hybrid trainingfree framework that integrates large language models llms into participatory urban sensing through a multiagent evolution system agentsense initially employs classical planner to generate baseline solutions and then iteratively refines them to adapt sensing task assignments to dynamic urban conditions and heterogeneous worker preferences while producing natural language explanations that enhance transparency and trust extensive experiments across two largescale mobility datasets and seven types of dynamic disturbances demonstrate that agentsense offers distinct advantages in adaptivity and explainability over traditional methods furthermore compared to singleagent llm baselines our approach outperforms in both performance and robustness while delivering more reasonable and transparent explanations these results position agentsense as a significant advancement towards deploying adaptive and explainable urban sensing systems on the web
effective deep search agents must not only access opendomain and domainspecific knowledge but also apply complex rulessuch as legal clauses medical manuals and tariff rules these rules often feature vague boundaries and implicit logic relationships making precise application challenging for agents however this critical capability is largely overlooked by current agent benchmarks to fill this gap we introduce hscodecomp the first realistic expertlevel ecommerce benchmark designed to evaluate deep search agents in hierarchical rule application in this task the deep reasoning process of agents is guided by these rules to predict digit harmonized system code hscode of products with noisy but realistic descriptions these codes established by the world customs organization are vital for global supply chain efficiency built from realworld data collected from largescale ecommerce platforms our proposed hscodecomp comprises product entries spanning diverse product categories with these hscodes annotated by several human experts extensive experimental results on several stateoftheart llms opensource and closedsource agents reveal a huge performance gap best agent achieves only digit accuracy far below human experts at besides detailed analysis demonstrates the challenges of hierarchical rule application and testtime scaling fails to improve performance further
comprehending natural language and following human instructions are critical capabilities for intelligent agents however the flexibility of linguistic instructions induces substantial ambiguity across languageconditioned tasks severely degrading algorithmic performance to address these limitations we present a novel method named dail distributional aligned learning featuring two key components distributional policy and semantic alignment specifically we provide theoretical results that the value distribution estimation mechanism enhances task differentiability meanwhile the semantic alignment module captures the correspondence between trajectories and linguistic instructions extensive experimental results on both structured and visual observation benchmarks demonstrate that dail effectively resolves instruction ambiguities achieving superior performance to baseline methods our implementation is available at
we address the challenge of adopting language models lms for embodied tasks in dynamic environments where online access to largescale inference engines or symbolic planners is constrained due to latency connectivity and resource limitations to this end we present nesypr a novel embodied reasoning framework that compiles knowledge via neurosymbolic proceduralization thereby equipping lmbased agents with structured adaptive and timely reasoning capabilities in nesypr taskspecific plans are first explicitly generated by a symbolic tool leveraging its declarative knowledge these plans are then transformed into composable procedural representations that encode the plans implicit production rules enabling the resulting composed procedures to be seamlessly integrated into the lms inference process this neurosymbolic proceduralization abstracts and generalizes multistep symbolic structured pathfinding and reasoning into singlestep lm inference akin to human knowledge compilation it supports efficient testtime inference without relying on external symbolic guidance making it well suited for deployment in latencysensitive and resourceconstrained physical systems we evaluate nesypr on the embodied benchmarks pddlgym virtualhome and alfworld demonstrating its efficient reasoning capabilities over largescale reasoning models and a symbolic planner while using more compact lms
we introduce mscbench a largescale benchmark for evaluating multihop endtoend tool orchestration by llm agents in a hierarchical modelcontext protocol mcp ecosystem existing benchmarks often evaluate tools in isolation ignoring challenges such as functional overlap and crossserver orchestration leading to overly optimistic assessments mscbench addresses these gaps by constructing ground truth through equal function sets allowing objective metrics such as f score and reducing the dependency on llmasajudge evaluation organized as a fivelevel curriculum it systematically tests agent capabilities from singletool orchestration to complex crossserver planning and robustness to outofscope requests experiments reveal that rigid hierarchies can hinder performance without codesigned strategies and even stateoftheart agents exhibit systemic weaknesses in robustness mscbench provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient toolusing agents the benchmark and resources are publicly available at
reinforcement learning enables agents to learn optimal behaviors through interactions with environments however realworld environments are typically nonstationary requiring agents to continuously adapt to new tasks and changing conditions although continual reinforcement learning facilitates learning across multiple tasks existing methods often suffer from catastrophic forgetting and inefficient knowledge utilization to address these challenges we propose continual knowledge adaptation for reinforcement learning ckarl which enables the accumulation and effective utilization of historical knowledge specifically we introduce a continual knowledge adaptation strategy which involves maintaining a taskspecific knowledge vector pool and dynamically using historical knowledge to adapt the agent to new tasks this process mitigates catastrophic forgetting and enables efficient knowledge transfer across tasks by preserving and adapting critical model parameters additionally we propose an adaptive knowledge merging mechanism that combines ilar knowledge vectors to address scalability challenges reducing memory requirements while ensuring the retention of essential knowledge experiments on three benchmarks demonstrate that the proposed ckarl outperforms stateoftheart methods achieving an improvement of in overall performance and in forward transfer the source code is available at
can large language model llm agents reproduce the complex social dynamics that characterize human online behavior shaped by homophily reciprocity and social validation and what memory and learning mechanisms enable such dynamics to emerge we present a multiagent llm ulation framework in which agents repeatedly interact evaluate one another and adapt their behavior through incontext learning accelerated by a coaching signal to model human social behavior we design behavioral reward functions that capture core drivers of online engagement including social interaction information seeking selfpresentation coordination and emotional support these rewards align agent objectives with empirically observed user motivations enabling the study of how network structures and group formations emerge from individual decisionmaking our experiments show that coached llm agents develop stable interaction patterns and form emergent social ties yielding network structures that mirror properties of real online communities by combining behavioral rewards with incontext adaptation our framework establishes a principled testbed for investigating collective dynamics in llm populations and reveals how artificial agents may imate or diverge from humanlike social behavior
precedential constraint is one foundation of casebased reasoning in ai and law it generally assumes that the underlying set of precedents must be consistent to relax this assumption a generalized notion of the reason model has been introduced while several argumentative explanation approaches exist for reasoning with precedents based on the traditional consistent reason model there has been no corresponding argumentative explanation method developed for this generalized reasoning framework accommodating inconsistent precedents to address this question this paper examines an extension of the derivation state argumentation framework dsaframework to explain the reasoning according to the generalized notion of the reason model
this study examines the performance of chatgpt with an experiment in the legal domain we compare the outcome with it a baseline using regular expressions regex rather than focusing solely on the assessment against human performance the study reveals that even if chatgpt has access to the necessary knowledge and competencies it is unable to assemble them reason through in a way that leads to an exhaustive result this unveils a major limitation of chatgpt intelligence encompasses the ability to break down complex issues and address them according to multiple required competencies providing a unified and comprehensive solution in the legal domain one of the most crucial tasks is reading legal decisions and extracting key passages condensed from principles of law pols which are then incorporated into subsequent rulings by judges or defense documents by lawyers in performing this task artificial intelligence lacks an allencompassing understanding and reasoning which makes it inherently limited genuine intelligence remains a uniquely human trait at least in this particular field
current evaluation of web agents largely reduces to binary success metrics or conformity to a single reference trajectory ignoring the structural diversity present in benchmark datasets we present webgrapheval a framework that abstracts trajectories from multiple agents into a unified weighted action graph this representation is directly compatible with benchmarks such as webarena leveraging leaderboard runs and newly collected trajectories without modifying environments the framework canonically encodes actions merges recurring behaviors and applies structural analyses including reward propagation and successweighted edge statistics evaluations across thousands of trajectories from six web agents show that the graph abstraction captures crossmodel regularities highlights redundancy and inefficiency and identifies critical decision points overlooked by outcomebased metrics by framing web interaction as graphstructured data webgrapheval establishes a general methodology for multipath crossagent and efficiencyaware evaluation of web agents
reasoning models have demonstrated exceptional performance in tasks such as mathematics and logical reasoning primarily due to their ability to engage in stepbystep thinking during the reasoning process however this often leads to overthinking resulting in unnecessary computational overhead to address this issue mode selection aims to automatically decide between longcot chainofthought or shortcot by utilizing either a thinking or nothinking mode ultaneously early exit determines the optimal stopping point during the iterative reasoning process both methods seek to reduce the computational burden in this paper we first identify mode selection as a more challenging variant of the early exit problem as they share ilar objectives but differ in decision timing while early exit focuses on determining the best stopping point for concise reasoning at inference time mode selection must make this decision at the beginning of the reasoning process relying on predefined fake thoughts without engaging in an explicit reasoning process referred to as zerostep thinking through empirical studies on nine baselines we observe that promptbased approaches often fail due to their limited classification capabilities when provided with minimal handcrafted information in contrast approaches that leverage internal information generally perform better across most scenarios but still exhibit issues with stability our findings indicate that existing methods relying solely on the information provided by models are insufficient for effectively addressing mode selection in scenarios with limited information highlighting the ongoing challenges of this task our code is available at
despite the rapid expansion of large language models llms in healthcare the ability of these systems to assess clinical trial reporting according to consort standards remains unclear particularly with respect to their cognitive and reasoning strategies this study applies a behavioral and metacognitive analytic approach with expertvalidated data systematically comparing two representative llms under three prompt conditions clear differences emerged in how the models approached various consort items and prompt types including shifts in reasoning style explicit uncertainty and alternative interpretations shaped response patterns our results highlight the current limitations of these systems in clinical compliance automation and underscore the importance of understanding their cognitive adaptations and strategic behavior in developing more explainable and reliable medical ai
multimodal large language models mllms have demonstrated capabilities in audio understanding but current evaluations may obscure fundamental weaknesses in relational reasoning we introduce the music understanding and structural evaluation muse benchmark an opensource resource with tasks designed to probe fundamental music perception skills we evaluate four sota models gemini pro and flash qwenomni and audioflamingo against a large human baseline n our results reveal a wide variance in sota capabilities and a persistent gap with human experts while gemini pro succeeds on basic perception qwen and audio flamingo perform at or near chance exposing severe perceptual deficits furthermore we find chainofthought cot prompting provides inconsistent often detrimental results our work provides a critical tool for evaluating invariant musical representations and driving development of more robust ai systems
in reinforcement learning from human feedback preferencebased reward models play a central role in aligning large language models to humanaligned behavior however recent studies show that these models are prone to reward hacking and often fail to generalize well due to overoptimization they achieve high reward scores by exploiting shortcuts that is exploiting spurious features eg response verbosity agreeable tone or sycophancy that correlate with human preference labels in the training data rather than genuinely reflecting the intended objectives in this paper instead of probing these issues one at a time we take a broader view of the reward hacking problem as shortcut behaviors and introduce a principled yet flexible approach to mitigate shortcut behaviors in preferencebased reward learning inspired by the invariant theory in the kernel perspective we propose preferencebased reward invariance for shortcut mitigation prism which learns groupinvariant kernels with feature maps in a closedform learning objective experimental results in several benchmarks show that our method consistently improves the accuracy of the reward model on diverse outofdistribution tasks and reduces the dependency on shortcuts in downstream policy models establishing a robust framework for preferencebased alignment
there is growing interest in using machine learning ml to support clinical diagnosis but most approaches rely on static fully observed datasets and fail to reflect the sequential resourceaware reasoning clinicians use in practice diagnosis remains complex and error prone especially in highpressure or resourcelimited settings underscoring the need for frameworks that help clinicians make timely and costeffective decisions we propose actmed adaptive clinical test selection via modelbased experimental design a diagnostic framework that integrates bayesian experimental design bed with large language models llms to better emulate realworld diagnostic reasoning at each step actmed selects the test expected to yield the greatest reduction in diagnostic uncertainty for a given patient llms act as flexible ulators generating plausible patient state distributions and supporting belief updates without requiring structured taskspecific training data clinicians can remain in the loop reviewing test suggestions interpreting intermediate outputs and applying clinical judgment throughout we evaluate actmed on realworld datasets and show it can optimize test selection to improve diagnostic accuracy interpretability and resource use this represents a step toward transparent adaptive and clinicianaligned diagnostic systems that generalize across settings with reduced reliance on domainspecific data
while testtime scaling with verification has shown promise in improving the performance of large language models llms the role of the verifier and its imperfections remain underexplored the effect of verification manifests through interactions of three quantities i the generators coverage ii the verifiers region of convergence roc and iii the sampling algorithms suboptimality though recent studies capture subsets of these factors a unified framework quantifying the geometry of their interplay is missing we frame verifiable testtime scaling as a transport problem this characterizes the interaction of coverage roc and suboptimality and uncovers that the suboptimalitycoverage curve exhibits three regimes a transport regime where suboptimality increases with coverage a policy improvement regime where suboptimality may decrease with coverage depending on the verifiers roc and a saturation regime where suboptimality plateaus unaffected by coverage we further propose and analyze two classes of sampling algorithms sequential and batched and examine how their computational complexities shape these tradeoffs empirical results with qwen llama and gemma models corroborate our theoretical findings
planning with world models offers a powerful paradigm for robotic control conventional approaches train a model to predict future frames conditioned on current frames and actions which can then be used for planning however the objective of predicting future pixels is often at odds with the actual planning objective strong pixel reconstruction does not always correlate with good planning decisions this paper posits that instead of reconstructing future frames as pixels world models only need to predict taskrelevant semantic information about the future for such prediction the paper poses world modeling as a visual question answering problem about semantic information in future frames this perspective allows world modeling to be approached with the same tools underlying vision language models thus vision language models can be trained as semantic world models through a supervised finetuning process on imageactiontext data enabling planning for decisionmaking while inheriting many of the generalization and robustness properties from the pretrained visionlanguage models the paper demonstrates how such a semantic world model can be used for policy improvement on openended robotics tasks leading to significant generalization improvements over typical paradigms of reconstructionbased actionconditional world modeling website available at
reinforcement learning from verifiable rewards has emerged as a powerful technique for enhancing the complex reasoning abilities of large language models llms however these methods are fundamentally constrained by the learning cliff phenomenon when faced with problems far beyond their current capabilities models consistently fail yielding a persistent zeroreward signal in policy optimization algorithms like grpo this collapses the advantage calculation to zero rendering these difficult problems invisible to the learning gradient and stalling progress to overcome this we introduce scafgrpo scaffolded group relative policy optimization a progressive training framework that strategically provides minimal guidance only when a models independent learning has plateaued the framework first diagnoses learning stagnation and then intervenes by injecting tiered inprompt hints ranging from abstract concepts to concrete steps enabling the model to construct a valid solution by itself extensive experiments on challenging mathematics benchmarks demonstrate scafgrpos effectiveness boosting the pass score of the qwenmathb model on the aime benchmark by a relative over a vanilla grpo baseline this result demonstrates our framework provides a robust and effective methodology for unlocking a models ability to solve problems previously beyond its reach a critical step towards extending the frontier of autonomous reasoning in llm
public and nonprofit organizations often hesitate to adopt ai tools because most models are opaque even though standard approaches typically analyze aggregate patterns rather than offering actionable caselevel guidance this study tests a practitionerintheloop workflow that pairs transparent decisiontree models with large language models llms to improve predictive accuracy interpretability and the generation of practical insights using data from an ongoing collegesuccess program we build interpretable decision trees to surface key predictors we then provide each trees structure to an llm enabling it to reproduce caselevel predictions grounded in the transparent models practitioners participate throughout feature engineering model design explanation review and usability assessment ensuring that field expertise informs the analysis at every stage results show that integrating transparent models llms and practitioner input yields accurate trustworthy and actionable caselevel evaluations offering a viable pathway for responsible ai adoption in the public and nonprofit sectors
using generative artificial intelligence tools and systems in journalism is expected to increase journalists production rates transform newsrooms economic models and further personalize the audiences news consumption practices since its release in openais chatgpt and other large language models have raised the alarms inside news organizations not only for bringing new challenges to news reporting and factchecking but also for what these technologies would mean for journalists professional authority in journalism this paper examines how journalists in dutch media manage the integration of ai technologies into their daily routines drawing from interviews with editors journalists and innovation managers in different news outlets and media companies we propose the concept of controlled change as a heuristic to explain how journalists are proactively setting guidelines experimenting with ai tools and identifying their limitations and capabilities using professional authority as a theoretical framework we argue that journalists anticipate and integrate ai technologies in a supervised manner and identify three primary mechanisms through which journalists manage this integration developing adaptive guidelines that align ai use with ethical codes experimenting with ai technologies to determine their necessity and fit and critically assessing the capabilities and limitations of ai systems
speculative decoding sd accelerates large language model inference by employing a small draft model to generate predictions which are then verified by a larger target model the effectiveness of sd hinges on the alignment between these models which is typically enhanced by knowledge distillation kd however conventional kd methods aim to minimize the kl divergence between the draft and target models across all tokens a goal that is misaligned with the true objective of sd which is to maximize token acceptance rate therefore draft models often struggle to fully assimilate the target models knowledge due to capacity constraints leading to suboptimal performance to address this challenge we propose adaspec a novel method that incorporates selective token filtering into the kd process adaspec utilizes a reference model to identify and filter out difficulttofit tokens enabling the distillation of a draft model that better aligns with the target model on pler tokens this approach improves the overall token acceptance rate without compromising generation quality we evaluate adaspec across diverse tasks including arithmetic reasoning instructionfollowing coding and summarization using model configurations of mb and mb parameters our results demonstrate that adaspec consistently outperforms the stateoftheart distillspec method achieving higher acceptance rates across all tasks up to the code is publicly available at
the long chainofthought longcot capability is central to the recent breakthroughs achieved by large language models in complex reasoning tasks however the accompanying issue of underthinking where models exhibit shallow reasoning by frequently switching thoughts without sufficient exploration limits both performance and token efficiency to address this problem we propose a ple yet effective reasoning strategy the smartswitch inference framework this framework can be easily integrated into any large language model as a plugandplay solution continuously monitoring the models reasoning process to detect underthinking and guide it toward deeper exploration of promising but overlooked thoughts specifically the perception module identifies points where thoughts switch and evaluates the potential of the preceding thought using an offtheshelf process reward model prm if a highpotential thought is found to be prematurely abandoned the intervention module interrupts the ongoing inference backtracks to the point before the switch and inserts a deepening prompt to encourage further exploration along that promising path extensive experiments on challenging mathematical reasoning benchmarks demonstrate that our method significantly enhances the performance of various large language models of different sizes
diffusion models have become a cornerstone of modern generative ai for their exceptional generation quality and controllability however their inherent multistep iterations and complex backbone networks lead to prohibitive computational overhead and generation latency forming a major bottleneck for realtime applications although existing acceleration techniques have made progress they still face challenges such as limited applicability high training costs or quality degradation against this backdrop diffusion caching offers a promising trainingfree architectureagnostic and efficient inference paradigm its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process by enabling featurelevel crossstep reuse and interlayer scheduling it reduces computation without modifying model parameters this paper systematically reviews the theoretical foundations and evolution of diffusion caching and proposes a unified framework for its classification and analysis through comparative analysis of representative methods we show that diffusion caching evolves from static reuse to dynamic prediction this trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation paving the way for a unified efficient inference framework for future multimodal and interactive applications we argue that this paradigm will become a key enabler of realtime and efficient generative ai injecting new vitality into both theory and practice of efficient generative intelligence
solving complex realworld control tasks often takes multiple tries if we fail at first we reflect on what went wrong and change our strategy accordingly to avoid making the same mistake in robotics visionlanguageaction models vlas offer a promising path towards solving complex control tasks but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task in this work we introduce learning from inferencetime execution liten which connects a vla lowlevel policy to a highlevel vlm that conditions on past experiences by including them incontext allowing it to learn the affordances and capabilities of the lowlevel vla our approach iterates between a reasoning phase that generates and executes plans for the lowlevel vla and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts unlike ilar approaches to selfrefinement in nonrobotics domains liten must reflect on unstructured realworld robot trajectories eg raw videos which requires structured guiderails during assessment our experimental results demonstrate liten is able to effectively learn from past experience to generate plans that use highaffordance instructions to accomplish longhorizon tasks
we present a novel framework for leveraging synthetic icu timeseries data not only to train but also to rigorously and trustworthily evaluate predictive models both at the population level and within finegrained demographic subgroups building on prior diffusion and vaebased generators timediff healthgen timeautodiff we introduce enhanced timeautodiff which augments the latent diffusion objective with distributionalignment penalties we extensively benchmark all models on mimiciii and eicu on hour mortality and binary lengthofstay tasks our results show that enhanced timeautodiff reduces the gap between realonsynthetic and realonreal evaluation trts gap by over achieving deltatrts leq auroc while preserving training utility deltatstr approx crucially for intersectional subgroups large synthetic cohorts cut subgrouplevel auroc estimation error by up to relative to small real test sets and outperform them in of subgroups this work provides a practical privacypreserving roadmap for trustworthy granular model evaluation in critical care enabling robust and reliable performance analysis across diverse patient populations without exposing sensitive ehr data contributing to the overall trustworthiness of medical ai
prompting is a common approach for leveraging lms in zeroshot settings however the underlying mechanisms that enable lms to perform diverse tasks without taskspecific supervision remain poorly understood studying the relationship between prompting and the quality of internal representations can shed light on how pretrained embeddings may support incontext task solving in this empirical study we conduct a series of probing experiments on prompt embeddings analyzing various combinations of prompt templates for zeroshot classification our findings show that while prompting affects the quality of representations these changes do not consistently correlate with the relevance of the prompts to the target task this result challenges the assumption that more relevant prompts necessarily lead to better representations we further analyze potential factors that may contribute to this unexpected behavior
agentic ai is poised to usher in a seismic paradigm shift in software engineering se as technologists rush headalong to make agentic ai a reality se researchers are driven to establish agentic se as a research area while early visions of agentic se are primarily focused on coderelated activities early empirical evidence calls for a consideration of a range of sociotechnical concerns to make it work in practice this paper contributes to the emerging community vision by a recommending an expansion of its scope beyond code toward a whole of process vision grounding it in se foundations and evolution and emerging agentic se frameworks b proposing a preliminary set of values and principles to guide efforts and c sharing guidance on designingusing welldefined vocabulary for agentic se it is hoped that these ideas will encourage community collaborations and steer the se community towards laying strong foundations of agentic se so its not only inevitable but also deliberate and desirable in the long run
industrial and government organizations increasingly depend on datadriven analytics for workforce finance and regulated decision processes where timeliness cost efficiency and compliance are critical distributed frameworks such as spark and flink remain effective for massivescale batch or streaming analytics but introduce coordination complexity and auditing overheads that misalign with moderatescale latencysensitive inference meanwhile cloud providers now offer serverless gpus and models such as tabnet enable interpretable tabular ml motivating new deployment blueprints for regulated environments in this paper we present a productionoriented big data as a service bdaas blueprint that integrates a singlenode serverless gpu runtime with tabnet the design leverages gpu acceleration for throughput serverless elasticity for cost reduction and featuremask interpretability for ilfips compliance we conduct benchmarks on the hr adult and bls datasets comparing our approach against spark and cpu baselines our results show that gpu pipelines achieve up to x higher throughput x lower latency and lower cost per k inferences compared to spark baselines while compliance mechanisms add only ms latency with p ms interpretability remains stable under peak load ensuring reliable auditability taken together these findings provide a complianceaware benchmark a reproducible helmpackaged blueprint and a decision framework that demonstrate the practicality of secure interpretable and costefficient serverless gpu analytics for regulated enterprise and government settings
human communication is motivated people speak write and create content with a particular communicative intent in mind as a result information that large language models llms and ai agents process is inherently framed by humans intentions and incentives people are adept at navigating such nuanced information we routinely identify benevolent or selfserving motives in order to decide what statements to trust for llms to be effective in the real world they too must critically evaluate content by factoring in the motivations of the source for instance weighing the credibility of claims made in a sales pitch in this paper we undertake a comprehensive study of whether llms have this capacity for motivational vigilance we first employ controlled experiments from cognitive science to verify that llms behavior is consistent with rational models of learning from motivated testimony and find they successfully discount information from biased sources in a humanlike manner we then extend our evaluation to sponsored online adverts a more naturalistic reflection of llm agents information ecosystems in these settings we find that llms inferences do not track the rational models predictions nearly as closely partly due to additional information that distracts them from vigilancerelevant considerations however a ple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between llms and the rational model these results suggest that llms possess a basic sensitivity to the motivations of others but generalizing to novel realworld settings will require further improvements to these models
feedback is one of the most powerful influences on student learning with extensive research examining how best to implement it in educational settings increasingly feedback is being generated by artificial intelligence ai offering scalable and adaptive responses two widely studied approaches are directive feedback which gives explicit explanations and reduces cognitive load to speed up learning and metacognitive feedback which prompts learners to reflect track their progress and develop selfregulated learning srl skills while both approaches have clear theoretical advantages their comparative effects on engagement confidence and quality of work remain underexplored this study presents a semesterlong randomised controlled trial with students in an introductory design and programming course using an adaptive educational platform participants were assigned to receive directive metacognitive or hybrid aigenerated feedback that blended elements of both directive and metacognitive feedback results showed that revision behaviour differed across feedback conditions with hybrid prompting the most revisions compared to directive and metacognitive confidence ratings were uniformly high and resource quality outcomes were comparable across conditions these findings highlight the promise of ai in delivering feedback that balances clarity with reflection hybrid approaches in particular show potential to combine actionable guidance for immediate improvement with opportunities for selfreflection and metacognitive growth
multimodal large language models mllms achieve strong performance on visionlanguage tasks yet their visual processing is opaque most blackbox evaluations measure task accuracy but reveal little about underlying mechanisms drawing on cognitive psychology we adapt classic visual search paradigms originally developed to study human perception to test whether mllms exhibit the popout effect where salient visual features are detected independently of distractor set size using controlled experiments targeting colour size and lighting features we find that advanced mllms exhibit humanlike popout effects in colour or sizebased disjunctive single feature search as well as capacity limits for conjunctive multiple feature search we also find evidence to suggest that mllms like humans incorporate natural scene priors such as lighting direction into object representations we reinforce our findings using targeted finetuning and mechanistic interpretability analyses our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in mllms
memoryefficient training of deep neural networks has become increasingly important as models grow larger while deployment environments impose strict resource constraints we propose trady a novel transfer learning scheme leveraging two key insights layer importance for updates is architecturedependent and determinable a priori while dynamic stochastic channel selection provides superior gradient imation compared to static approaches we introduce a dynamic channel selection approach that stochastically resamples channels between epochs within preselected layers extensive experiments demonstrate trady achieves stateoftheart performance across various downstream tasks and architectures while maintaining strict memory constraints achieving up to activation sparsity weight derivative sparsity and reduction in flops for weight derivative computation
transformer models have significantly advanced the field of emotion recognition however there are still open challenges when exploring openended queries for large language models llms although current models offer good results automatic emotion analysis in open texts presents significant challenges such as contextual ambiguity linguistic variability and difficulty interpreting complex emotional expressions these limitations make the direct application of generalist models difficult accordingly this work compares the effectiveness of finetuning and prompt engineering in emotion detection in three distinct scenarios i performance of finetuned pretrained models and generalpurpose llms using ple prompts ii effectiveness of different emotion prompt designs with llms and iii impact of emotion grouping techniques on these models experimental tests attain metrics above with a finetuned pretrained model for emotion recognition moreover the findings highlight that llms require structured prompt engineering and emotion grouping to enhance their performance these advancements improve sentiment analysis humancomputer interaction and understanding of user behavior across various domains
despite remarkable progress in driving world models their potential for autonomous systems remains largely untapped the world models are mostly learned for world ulation and decoupled from trajectory planning while recent efforts aim to unify world modeling and planning in a single framework the synergistic facilitation mechanism of world modeling for planning still requires further exploration in this work we introduce a new driving paradigm named policy world model pwm which not only integrates world modeling and trajectory planning within a unified architecture but is also able to benefit planning using the learned world knowledge through the proposed actionfree future state forecasting scheme through collaborative stateaction prediction pwm can mimic the humanlike anticipatory perception yielding more reliable planning performance to facilitate the efficiency of video forecasting we further introduce a dynamically enhanced parallel token generation mechanism equipped with a contextguided tokenizer and an adaptive dynamic focal loss despite utilizing only front camera input our method matches or exceeds stateoftheart approaches that rely on multiview and multimodal inputs code and model weights will be released at
with social media growth users employ stylistic fonts and fontlike emoji to express individuality creating visually appealing text that remains humanreadable however these fonts introduce hidden vulnerabilities in nlp models while humans easily read stylistic text models process these characters as distinct tokens causing interference we identify this humanmodel perception gap and propose a stylebased attack style attack disguise sad we design two sizes light for query efficiency and strong for superior attack performance experiments on sentiment classification and machine translation across traditional models llms and commercial services demonstrate sads strong attack performance we also show sads potential threats to multimodal tasks including texttoimage and texttospeech generation
in the quest for scientific progress communicating research is as vital as the discovery itself yet researchers are often sidetracked by the manual repetitive chore of building project webpages to make their dense papers accessible while automation has tackled static slides and posters the dynamic interactive nature of webpages has remained an unaddressed challenge to bridge this gap we reframe the problem arguing that the solution lies not in a single command but in a collaborative hierarchical process we introduce autopage a novel multiagent system that embodies this philosophy autopage deconstructs papertopage creation into a coarsetofine pipeline from narrative planning to multimodal content generation and interactive rendering to combat ai hallucination dedicated checker agents verify each step against the source paper while optional human checkpoints ensure the final product aligns perfectly with the authors vision transforming the system from a mere tool into a powerful collaborative assistant to rigorously validate our approach we also construct pagebench the first benchmark for this new task experiments show autopage not only generates highquality visually appealing pages but does so with remarkable efficiency in under minutes for less than code and dataset will be released at href webpage
visionlanguage models vlms have recently shown remarkable zeroshot performance in medical image understanding yet their grounding ability the extent to which textual concepts align with visual evidence remains underexplored in the medical domain however reliable grounding is essential for interpretability and clinical adoption in this work we present the first systematic benchmark for evaluating crossmodal interpretability in chest xrays across seven clipstyle vlm variants we generate visual explanations using crossattention and ilaritybased localization maps and quantitatively assess their alignment with radiologistannotated regions across multiple pathologies our analysis reveals that while all vlm variants demonstrate reasonable localization for large and welldefined pathologies their performance substantially degrades for small or diffuse lesions models that are pretrained on chest xrayspecific datasets exhibit improved alignment compared to those trained on generaldomain data the overall recognition ability and grounding ability of the model are strongly correlated these findings underscore that current vlms despite their strong recognition ability still fall short in clinically reliable grounding highlighting the need for targeted interpretability benchmarks before deployment in medical practice xbench code is available at
root cause analysis rca is a crucial aspect of incident management in largescale cloud services while the term root cause analysis or rca has been widely used different studies formulate the task differently this is because the term rca implicitly covers tasks with distinct underlying goals for instance the goal of localizing a faulty service for rapid triage is fundamentally different from identifying a specific functional bug for a definitive fix however previous surveys have largely overlooked these goalbased distinctions conventionally categorizing papers by input data types eg metricbased vs tracebased methods this leads to the grouping of works with disparate objectives thereby obscuring the true progress and gaps in the field meanwhile the typical audience of an rca survey is either laymen who want to know the goals and big picture of the task or rca researchers who want to figure out past research under the same task formulation thus an rca survey that organizes the related papers according to their goals is in high demand to this end this paper presents a goaldriven framework that effectively categorizes and integrates papers on rca in the context of cloud incident management based on their diverse goals spanning the period from to in addition to the goaldriven categorization it discusses the ultimate goal of all rca papers as an umbrella covering different rca formulations moreover the paper discusses open challenges and future directions in rca
this paper presents a novel task of extracting latin fragments from mixedlanguage historical documents with varied layouts we benchmark and evaluate the performance of large foundation models against a multimodal dataset of annotated pages the results demonstrate that reliable latin detection with contemporary models is achievable our study provides the first comprehensive analysis of these models capabilities and limits for this task
multimodal colearning is emerging as an effective paradigm in machine learning enabling models to collaboratively learn from different modalities to enhance singlemodality predictions earth observation eo represents a quintessential domain for multimodal data analysis wherein diverse remote sensors collect data to sense our planet this unprecedented volume of data introduces novel challenges specifically the access to the same sensor modalities at both training and inference stages becomes increasingly complex based on realworld constraints affecting remote sensing platforms in this context multimodal colearning presents a promising strategy to leverage the vast amount of sensorderived data available at the training stage to improve singlemodality models for inferencetime deployment most current research efforts focus on designing customized solutions for either particular downstream tasks or specific modalities available at the inference stage to address this we propose a novel multimodal colearning framework capable of generalizing across various tasks without targeting a specific modality for inference our approach combines contrastive and modality discriminative learning together to guide singlemodality models to structure the internal model manifold into modalityshared and modalityspecific information we evaluate our framework on four eo benchmarks spanning classification and regression tasks across different sensor modalities where only one of the modalities available during training is accessible at inference time our results demonstrate consistent predictive improvements over stateoftheart approaches from the recent machine learning and computer vision literature as well as eospecific methods the obtained findings validate our framework in the singlemodality inference scenarios across a diverse range of eo applications
largescale visionlanguage models vlms such as clip have gained popularity for their generalizable and expressive multimodal representations by leveraging largescale training data with diverse textual metadata vlms acquire openvocabulary capabilities solving tasks beyond their training scope this paper investigates the temporal awareness of vlms assessing their ability to position visual content in time we introduce timek a benchmark dataset of over images with temporal ground truth and evaluate the timeawareness of vlms by a novel methodology our investigation reveals that temporal information is structured along a lowdimensional nonlinear manifold in the vlm embedding space based on this insight we propose methods to derive an explicit timeline representation from the embedding space these representations model time and its chronological progression and thereby facilitate temporal reasoning tasks our timeline approaches achieve competitive to superior accuracy compared to a promptbased baseline while being computationally efficient all code and data are available at
combinatorial optimization problems are central to both practical applications and the development of optimization methods while classical and quantum algorithms have been refined over decades machine learningassisted approaches are comparatively recent and have not yet consistently outperformed ple stateoftheart classical methods here we focus on a class of quadratic unconstrained binary optimization qubo problems specifically the challenge of finding minimum energy configurations in threedimensional ising spin glasses we use a global annealing monte carlo algorithm that integrates standard local moves with global moves proposed via machine learning we show that local moves play a crucial role in achieving optimal performance benchmarking against ulated annealing and population annealing we demonstrate that global annealing not only surpasses the performance of ulated annealing but also exhibits greater robustness than population annealing maintaining effectiveness across problem hardness and system size without hyperparameter tuning these results provide to our knowledge the first clear and robust evidence that a machine learningassisted optimization method can exceed the capabilities of classical stateoftheart techniques in a combinatorial optimization setting
ai methods are increasingly shaping pharmaceutical drug discovery however their translation to industrial applications remains limited due to their reliance on public datasets lacking scale and diversity of proprietary pharmaceutical data federated learning fl offers a promising approach to integrate private data into privacypreserving collaborative model training across data silos this federated data access complicates important datacentric tasks such as estimating dataset diversity performing informed data splits and understanding the structure of the combined chemical space to address this gap we investigate how well federated clustering methods can disentangle and represent distributed molecular data we benchmark three approaches federated kmeans fedkmeans federated principal component analysis combined with fedkmeans fedpcafedkmeans and federated localitysensitive hashing fedlsh against their centralized counterparts on eight diverse molecular datasets our evaluation utilizes both standard mathematical and a chemistryinformed evaluation metrics sficf that we introduce in this work the largescale benchmarking combined with an indepth explainability analysis shows the importance of incorporating domain knowledge through chemistryinformed metrics and onclient explainability analyses for federated diversity analysis on molecular data
optimizing national scientific investment requires a clear understanding of evolving research trends and the demographic and geographical forces shaping them particularly in light of commitments to equity diversity and inclusion this study addresses this need by analyzing years of research proposals funded by the natural sciences and engineering research council of canada nserc we conducted a comprehensive comparative evaluation of three topic modelling approaches latent dirichlet allocation lda structural topic modelling stm and bertopic we also introduced a novel algorithm named coffee designed to enable robust covariate effect estimation for bertopic this advancement addresses a significant gap as bertopic lacks a native function for covariate analysis unlike the probabilistic stm our findings highlight that while all models effectively delineate core scientific domains bertopic outperformed by consistently identifying more granular coherent and emergent themes such as the rapid expansion of artificial intelligence additionally the covariate analysis powered by coffee confirmed distinct provincial research specializations and revealed consistent genderbased thematic patterns across various scientific disciplines these insights offer a robust empirical foundation for funding organizations to formulate more equitable and impactful funding strategies thereby enhancing the effectiveness of the scientific ecosystem
climate change is intensifying the occurrence of harmful algal bloom hab particularly cyanobacteria which threaten aquatic ecosystems and human health through oxygen depletion toxin release and disruption of marine biodiversity traditional monitoring approaches such as manual water sampling remain laborintensive and limited in spatial and temporal coverage recent advances in visionlanguage models vlms for remote sensing have shown potential for scalable aidriven solutions yet challenges remain in reasoning over imagery and quantifying bloom severity in this work we introduce algae observation and segmentation algos a segmentationandreasoning system for hab monitoring that combines remote sensing image understanding with severity estimation our approach integrates geosamassisted human evaluation for highquality segmentation mask curation and finetunes vision language model on severity prediction using the cyanobacteria aggregated manual labels caml from nasa experiments demonstrate that algos achieves robust performance on both segmentation and severitylevel estimation paving the way toward practical and automated cyanobacterial monitoring systems
user queries in information retrieval are often ambiguous making it challenging for systems to identify a users target from a single query while recent dialoguebased interactive retrieval systems can clarify user intent they are inefficient as they often lack an explicit strategy to ask the most informative questions to address this limitation we propose sherlockllm a dialoguedriven retrieval framework that learns an optimal questioning strategy via reinforcement learning rl and avoids the need for largescale annotated dialogue data in our framework an agent is trained to generate a sequence of binary questions to efficiently narrow down the search space to validate our approach we introduce a benchmark with both structured and unstructured tasks experimental results show that sherlockllm is a robust and efficient solution on the structured tasks its performance matches strong baselines and approaches the theoretical optimal defined by binary search on the challenging unstructured task our agent significantly outperforms these baselines showcasing its ability to learn a highly effective informationseeking dialogue policy
retrievalaugmented generation rag systems address complex user requests by decomposing them into subqueries retrieving potentially relevant documents for each and then aggregating them to generate an answer efficiently selecting informative documents requires balancing a key tradeoff i retrieving broadly enough to capture all the relevant material and ii limiting retrieval to avoid excessive noise and computational cost we formulate query decomposition and document retrieval in an exploitationexploration setting where retrieving one document at a time builds a belief about the utility of a given subquery and informs the decision to continue exploiting or exploring an alternative we experiment with a variety of bandit learning methods and demonstrate their effectiveness in dynamically selecting the most informative subqueries our main finding is that estimating document relevance using rank information and human judgments yields a gain in documentlevel precision increase in alphandcg and better performance on the downstream task of longform generation
modelling qualitative uncertainty in formal argumentation is essential both for practical applications and theoretical understanding yet most of the existing works focus on abstract models for arguing with uncertainty following a recent trend in the literature we tackle the open question of studying plausible instantiations of these abstract models to do so we ground the uncertainty of arguments in their components structured within rules and premises our main technical contributions are i the introduction of a notion of expressivity that can handle abstract and structured formalisms and ii the presentation of both negative and positive expressivity results comparing the expressivity of abstract and structured models of argumentation with uncertainty these results affect incomplete abstract argumentation frameworks and their extension with dependencies on the abstract side and aspic on the structured side
we present a new approach to classification that combines data and knowledge in this approach data mining is used to derive association rules possibly with negations from data those rules are leveraged to increase the predictive performance of treebased models decision trees and random forests used for a classification task they are also used to improve the corresponding explanation task through the generation of abductive explanations that are more general than those derivable without taking such rules into account experiments show that for the two treebased models under consideration benefits can be offered by the approach in terms of predictive performance and in terms of explanation sizes
multimodal large language models mllms despite their advances are hindered by their high hallucination tendency and heavy reliance on brittle linear reasoning processes leading to failures in complex tasks to address these limitations we introduce visual attention reasoning var a novel framework that recasts grounded reasoning as a structured search over a reasoning trajectory space var decomposes the reasoning process into two key stages traceable evidence grounding and searchbased chainofthought cot generation which incorporates a backtracking mechanism for selfcorrection the search is guided by a multifaceted reward function with semantic and geometric selfverification components which penalize outputs that are not faithfully grounded in the visual input we provide a theoretical analysis for our search strategy validating its capability to find the correct solution with high probability experimental results show that our b model varb sets a new stateoftheart on a comprehensive suite of hallucination and safety benchmarks significantly outperforming existing opensource models and demonstrating competitive performance against leading proprietary systems
automating quantitative trading strategy development in dynamic markets is challenging especially with increasing demand for personalized investment solutions existing methods often fail to explore the vast strategy space while preserving the diversity essential for robust performance across changing market conditions we present quantevolve an evolutionary framework that combines qualitydiversity optimization with hypothesisdriven strategy generation quantevolve employs a feature map aligned with investor preferences such as strategy type risk profile turnover and return characteristics to maintain a diverse set of effective strategies it also integrates a hypothesisdriven multiagent system to systematically explore the strategy space through iterative generation and evaluation this approach produces diverse sophisticated strategies that adapt to both market regime shifts and individual investment needs empirical results show that quantevolve outperforms conventional baselines validating its effectiveness we release a dataset of evolved strategies to support future research
in this work we show that it is possible to extract significant amounts of alignment training data from a posttrained model useful to steer the model to improve certain capabilities such as longcontext reasoning safety instruction following and maths while the majority of related work on memorisation has focused on measuring success of training data extraction through string matching we argue that embedding models are better suited for our specific goals distances measured through a high quality embedding model can identify semantic ilarities between strings that a different metric such as edit distance will struggle to capture in fact in our investigation imate string matching would have severely undercounted by a conservative estimate of times the amount of data that can be extracted due to trivial artifacts that deflate the metric interestingly we find that models readily regurgitate training data that was used in posttraining phases such as sft or rl we show that this data can be then used to train a base model recovering a meaningful amount of the original performance we believe our work exposes a possibly overlooked risk towards extracting alignment data finally our work opens up an interesting discussion on the downstream effects of distillation practices since models seem to be regurgitating aspects of their training set distillation can therefore be thought of as indirectly training on the models original dataset
in this paper we present socianabla an endtoend agentic framework that treats ulator construction asinstance optimization over code within a textual computation graph specialized llmdriven agents are embedded as graph nodes and a workflow manager executes a lossdriven loop code synthesis execution evaluation code repair the optimizer performs textualgradient descent tgd while humanintheloop interaction is reserved for taskspec confirmation minimizing expert effort and keeping the code itself as the trainable object across three cps tasks ie user modeling mask adoption and personal mobility socianabla attains stateoftheart overall accuracy by unifying multiagent orchestration with a lossaligned optimization view socianabla converts brittle prompt pipelines into reproducible constraintaware ulator code generation that scales across domains and ulation granularities this work is under review and we will release the code soon
reliable hydrologic and flood forecasting requires models that remain stable when input data are delayed missing or inconsistent however most advances in rainfallrunoff prediction have been evaluated under ideal data conditions asizing accuracy rather than operational resilience here we develop an operationally ready emulator of the global flood awareness system glofas that couples long and shortterm memory networks with a relaxed waterbalance constraint to preserve physical coherence five architectures span a continuum of information availability from complete historical and forecast forcings to scenarios with data latency and outages allowing systematic evaluation of robustness trained in minimally managed catchments across the united states and tested in more than basins including heavily regulated rivers in india the emulator reproduces the hydrological core of glofas and degrades smoothly as information quality declines transfer across contrasting hydroclimatic and management regimes yields reduced yet physically consistent performance defining the limits of generalization under data scarcity and human influence the framework establishes operational robustness as a measurable property of hydrological machine learning and advances the design of reliable realtime forecasting systems
as large language models llms become increasingly integrated into applications serving users across diverse cultures communities and demographics it is critical to align llms with pluralistic human values beyond average principles eg hhh in psychological and social value theories such as schwartzs value theory pluralistic values are represented by multiple value dimensions paired with various priorities however existing methods encounter two challenges when aligning with such finegrained value objectives they often treat multiple values as independent and equally important ignoring their interdependence and relative priorities value complexity they struggle to precisely control nuanced value priorities especially those underrepresented ones value steerability to handle these challenges we propose couple a counterfactual reasoning framework for pluralistic value alignment it introduces a structural causal model scm to feature complex interdependency and prioritization among features as well as the causal relationship between highlevel value dimensions and behaviors moreover it applies counterfactual reasoning to generate outputs aligned with any desired value objectives benefitting from explicit causal modeling couple also provides better interpretability we evaluate couple on two datasets with different value systems and demonstrate that couple advances other baselines across diverse types of value objectives
control algorithms in production environments typically require domain experts to tune their parameters and logic for specific scenarios however existing research predominantly focuses on algorithmic performance under ideal or default configurations overlooking the critical aspect of tuning potential to bridge this gap we introduce crucible an agent that employs an llmdriven multilevel expert ulation to turn algorithms and defines a formalized metric to quantitatively evaluate their tuning potential we demonstrate crucibles effectiveness across a wide spectrum of case studies from classic control tasks to complex computer systems and validate its findings in a realworld deployment our experimental results reveal that crucible systematically quantifies the tunable space across different algorithms furthermore crucible provides a new dimension for algorithm analysis and design which ultimately leads to performance improvements our code is available at
ondevice virtual assistants like siri and google assistant are increasingly pivotal yet their capabilities are hamstrung by a reliance on rigid developerdependent apis gui agents offer a powerful apiindependent alternative but their adoption is hindered by the perception of poor performance as even the best models eg qwenvlb scores are capped at around on benchmarks like androidcontrol far from viability for realworld use our research reveals that issue lies not only with the models but with the benchmarks themselves we identified notable shortcomings in androidcontrol including ambiguities and factual errors which systematically underrates agent capabilities to address this critical oversight we enhanced androidcontrol into androidcontrolcurated a refined version of the benchmark improved through a rigorous purification pipeline on this enhanced benchmark stateoftheart models achieve success rates nearing on complex tasks improvement reflecting that ondevice gui agents are actually closer to practical deployment than previously thought we introduce our new sota model magmar b posttrained on just k curated samples using hours of an h gpu imately despite being times smaller in parameters this model delivers performance comparable to qwen vlb we release both androidcontrolcurated benchmark and magmar model to the research community encouraging adoption of this enhanced benchmark to better reflect model capabilities and accelerate the development of robust ondevice virtual assistants
human players do more than press buttons they ground what they see on screen into precise keyboardmouse actions and when stuck they seek information before trying again we ask whether current visionlanguage models vlms can do the same despite encouraging results under plified control or tool scaffolds humanlike play in a real client mapping raw screenshots to temporally coherent lowlevel actions while deciding when to ask for guidance remains an open challenge we introduce starbench a turnbased rpg benchmark derived from honkai star rail that targets these two humanlike competencies multimodal decisionmaking from pixels to actions and agentic information seeking starbench standardizes evaluation across eight combat tasks and two regimes with shared tasks and metrics i direct control where agents receive only screenshots and must emit lowlevel primitives click and keypress with no semantic hints and ii toolassisted control where higherlevel intents can be mapped to primitives by detectors and ocr outputs provide optional textualized observations to ease ui grounding to mirror human practice starbench also includes an askoract diagnostic that measures whether and when agents choose to request brief guidance before proceeding and how that choice affects subsequent performance we report reference baselines for contemporary vlms and a human reference results expose sizable gaps in perceptiontocontrol fidelity in the direct regime while showing that judicious information seeking correlates with improved success establishing starbench as a reproducible yardstick for agentic information seeking and multimodal decisionmaking in realclient play
large language models llms have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multioperation execution plans however existing llmagentbased analytics frameworks operate under the assumption of centralized data access offering little to no privacy protection in contrast federated analytics fa enables privacypreserving computation across distributed data sources but lacks support for natural language input and requires structured machinereadable queries in this work we present lafa the first system that integrates llmagentbased data analytics with fa lafa introduces a hierarchical multiagent architecture that accepts natural language queries and transforms them into optimized executable fa workflows a coarsegrained planner first decomposes complex queries into subqueries while a finegrained planner maps each subquery into a directed acyclic graph of fa operations using prior structural knowledge to improve execution efficiency an optimizer agent rewrites and merges multiple dags eliminating redundant operations and minimizing computational and communicational overhead our experiments demonstrate that lafa consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resourceintensive fa operations by a substantial margin this work establishes a practical foundation for privacypreserving llmdriven analytics that supports natural language input in the fa setting
we present a probabilistic intent modeling framework for large language model llm agents in multiturn social dialogue the framework maintains a belief distribution over a partners latent intentions initialized from contextual priors and dynamically updated through likelihood estimation after each utterance the evolving distribution provides additional contextual grounding for the policy enabling adaptive dialogue strategies under uncertainty preliminary experiments in the sotopia environment show consistent improvements the proposed framework increases the overall score by on sotopiaall and on sotopiahard compared with the qwenb baseline and slightly surpasses an oracle agent that directly observes partner intentions these early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent llm agents
large language models llms have demonstrated impressive reasoning capabilities but scaling their performance often relies on massive reasoning datasets that are computationally expensive to train on existing data selection methods aim to curate smaller highquality subsets but often rely on costly external models or opaque heuristics in this work we shift the focus from external heuristics to the models internal mechanisms we find that complex reasoning tasks consistently activate a sparse specialized subset of attention heads forming core reasoning circuits building on this insight we propose circuitseer a novel data selection method that quantifies the reasoning complexity of data by measuring its influence on these crucial circuits extensive experiments on models and datasets demonstrate circuitseers superiority notably finetuning qwenmathb on just of data selected by our method achieves a point gain in average pass over training on the full dataset highlighting its efficiency and effectiveness
large language models llms are increasingly being explored across a range of decisionmaking tasks however llms sometimes struggle with decisionmaking tasks under uncertainty that are relatively easy for humans such as planning actions in stochastic environments the adoption of llms for decisionmaking is impeded by uncertainty challenges such as llm uncertainty and environmental uncertainty llm uncertainty arises from the stochastic sampling process inherent to llms most llmbased decisionmaking ldm approaches address llm uncertainty through multiple reasoning chains or search trees however these approaches overlook environmental uncertainty which leads to poor performance in environments with stochastic state transitions some recent ldm approaches deal with uncertainty by forecasting the probability of unknown variables however they are not designed for multistep decisionmaking tasks that require interaction with the environment to address uncertainty in llm decisionmaking we introduce planu an llmbased planning method that captures uncertainty within monte carlo tree search mcts planu models the return of each node in the mcts as a quantile distribution which uses a set of quantiles to represent the return distribution to balance exploration and exploitation during tree search planu introduces an upper confidence bounds with curiosity ucc score which estimates the uncertainty of mcts nodes through extensive experiments we demonstrate the effectiveness of planu in llmbased decisionmaking tasks under uncertainty
optimization modeling enables critical decisions across industries but remains difficult to automate informal language must be mapped to precise mathematical formulations and executable solver code prior llm approaches either rely on brittle prompting or costly retraining with limited generalization we present alphaopt a selfimproving experience library that enables an llm to learn from limited demonstrations even answers alone without goldstandard programs and solver feedback without annotated reasoning traces or parameter updates alphaopt operates in a continual twophase cycle i a library learning phase that reflects on failed attempts extracting solververified structured insights as taxonomy condition explanation example and ii a library evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights improving transfer across tasks this design learns efficiently from limited demonstrations without curated rationales expands continually without costly retraining by updating the library rather than model weights and makes knowledge explicit and interpretable for human inspection and intervention experiments show that alphaopt steadily improves with more data to from to training items and surpasses the strongest baseline by on the outofdistribution optibench dataset when trained only on answers code and data are available at
with climate change intensifying urban waterlogging poses an increasingly severe threat to global public safety and infrastructure however existing monitoring approaches rely heavily on manual reporting and fail to provide timely and comprehensive assessments in this study we present urban waterlogging assessment uwassess a foundation modeldriven framework that automatically identifies waterlogged areas in surveillance images and generates structured assessment reports to address the scarcity of labeled data we design a semisupervised finetuning strategy and a chainofthought cot prompting strategy to unleash the potential of the foundation model for datascarce downstream tasks evaluations on challenging visual benchmarks demonstrate substantial improvements in perception performance gptbased evaluations confirm the ability of uwassess to generate reliable textual reports that accurately describe waterlogging extent depth risk and impact this dual capability enables a shift of waterlogging monitoring from perception to generation while the collaborative framework of multiple foundation models lays the groundwork for intelligent and scalable systems supporting urban management disaster response and climate resilience
visual language models vlms achieve promising results in medical reasoning but struggle with hallucinations vague descriptions inconsistent logic and poor localization to address this we propose a agent framework named medical visual reasoning agent medvragent the approach is based on visual guidance and selfreward paradigms and monte carlo tree search mcts by combining the visual guidance with tree search medvragent improves the medical visual reasoning capabilities of vlms we use the trajectories collected by medvragent as feedback to further improve the performance by finetuning the vlms with the proximal policy optimization ppo objective experiments on multiple medical vqa benchmarks demonstrate that our method outperforms existing approaches
in glass bottle manufacturing precise control of forming machines is critical for ensuring quality and minimizing defects this study presents a deep learningbased control algorithm designed to optimize the forming process in real production environments using real operational data from active manufacturing plants our neural network predicts the effects of parameter changes based on the current production setup through a specifically designed inversion mechanism the algorithm identifies the optimal machine settings required to achieve the desired glass gob characteristics experimental results on historical datasets from multiple production lines show that the proposed method yields promising outcomes suggesting potential for enhanced process stability reduced waste and improved product consistency these results highlight the potential of deep learning to process control in glass manufacturing
selfplay constitutes a fundamental paradigm for autonomous skill acquisition whereby agents iteratively enhance their capabilities through selfdirected environmental exploration conventional selfplay frameworks exploit agent symmetry within zerosum competitive settings yet this approach proves inadequate for openended learning scenarios characterized by inherent asymmetry human pedagogical systems exemplify asymmetric instructional frameworks wherein educators systematically construct challenges calibrated to individual learners developmental trajectories the principal challenge resides in operationalizing these asymmetric adaptive pedagogical mechanisms within artificial systems capable of autonomously synthesizing appropriate curricula without predetermined task hierarchies here we present heterogeneous adversarial play hap an adversarial automatic curriculum learning framework that formalizes teacherstudent interactions as a minimax optimization wherein taskgenerating instructor and problemsolving learner coevolve through adversarial dynamics in contrast to prevailing acl methodologies that employ static curricula or unidirectional task selection mechanisms hap establishes a bidirectional feedback system wherein instructors continuously recalibrate task complexity in response to realtime learner performance metrics experimental validation across multitask learning domains demonstrates that our framework achieves performance parity with sota baselines while generating curricula that enhance learning efficacy in both artificial agents and human subjects
this paper proposes memoryaugmented state machine prompting masmp a novel framework for llm agents in realtime strategy games addressing key challenges like hallucinations and fragmented decisionmaking in existing approaches masmp integrates state machine prompting with memory mechanisms to unify structured actions with longterm tactical coherence the framework features a natural languagedriven state machine architecture that guides llms to emulate finite state machines and behavior trees through prompts and a lightweight memory module preserving strategic variables eg tactics priority units across decision cycles experiments in starcraft ii demonstrate masmps win rate against the hardest builtin ai lv vastly outperforming baselines case studies reveal the method retains llms semantic comprehension while resolving the knowingdoing gap through strict stateaction mapping achieving both interpretability and fsmlike reliability this work establishes a new paradigm for combining neural and symbolic ai in complex decisionmaking
multiclass unsupervised anomaly detection muad has garnered growing research interest as it seeks to develop a unified model for anomaly detection across multiple classes ie eliminating the need to train separate models for distinct objects and thereby saving substantial computational resources under the muad setting while advanced transformerbased architectures have brought significant performance improvements identity shortcuts persist they directly copy inputs to outputs narrowing the gap in reconstruction errors between normal and abnormal cases and thereby making the two harder to distinguish therefore we propose shortcutbreaker a novel unified featurereconstruction framework for muad tasks featuring two key innovations to address the issue of shortcuts first drawing on matrix rank inequality we design a lowrank noisy bottleneck lrnb to project highdimensional features into a lowrank latent space and theoretically demonstrate its capacity to prevent trivial identity reproduction second leveraging vits global modeling capability instead of merely focusing on local features we incorporate a global perturbation attention to prevent information shortcuts in the decoders extensive experiments are performed on four widely used anomaly detection benchmarks including three industrial datasets mvtecad visa and realiad and one medical dataset universal medical the proposed method achieves a remarkable imagelevel auroc of and on these four datasets respectively consistently outperforming previous muad methods across different scenarios
geospatial data offers immense potential for understanding our planet however the sheer volume and diversity of this data along with its varied resolutions timescales and sparsity pose significant challenges for thorough analysis and interpretation this paper introduces earth ai a family of geospatial ai models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet this approach is built upon foundation models across three key domainsplanetscale imagery population and environmentand an intelligent geminipowered reasoning engine we present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities to handle complex multistep queries we developed a geminipowered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools on a new benchmark of realworld crisis scenarios our agent demonstrates the ability to deliver critical and timely insights effectively bridging the gap between raw geospatial data and actionable understanding
as large language model llm agents increasingly automate complex web tasks they boost productivity while ultaneously introducing new security risks however relevant studies on web agent attacks remain limited existing redteaming approaches mainly rely on manually crafted attack strategies or static models trained offline such methods fail to capture the underlying behavioral patterns of web agents making it difficult to generalize across diverse environments in web agent attacks success requires the continuous discovery and evolution of attack strategies to this end we propose genesis a novel agentic framework composed of three modules attacker scorer and strategist the attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation the scorer evaluates the target web agents responses to provide feedback the strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library which is then redeployed to enhance the attackers effectiveness extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines
humans do not just find mistakes after the fact we often catch them midstream because reflection is tied to the goal and its constraints todays large language models produce reasoning tokens and reflective text but is it functionally equivalent with human reflective reasoning prior work on closedended tasks with clear external correctness signals can make reflection look effective while masking limits in selfcorrection we therefore test eight frontier models on a ple realworld task that is openended yet ruleconstrained with auditable success criteria to produce valid scientific test items then revise after considering their own critique firstpass performance is poor often zero valid items out of required mean approx and reflection yields only modest gains also approx crucially the second attempt frequently repeats the same violation of constraint indicating corrective gains arise largely from chance production of a valid item rather than error detection and principled constraintsensitive repair performance before and after reflection deteriorates as openendedness increases and models marketed for reasoning show no advantage our results suggest that current llm reflection lacks functional evidence of the active goaldriven monitoring that helps humans respect constraints even on a first pass until such mechanisms are instantiated in the model itself reliable performance requires external structure that enforces constraints our code is available at
data quality plays a critical role in enhancing supervised finetuning sft for large language models llms and tokenlevel data selection has emerged as a promising direction for its finegrained nature despite their strong empirical performance existing tokenlevel selection methods share two key limitations requiring training or accessing an additional reference model and relying solely on loss information for token selection which cannot well preserve semantically important tokens that are not favored by lossbased metrics to address these challenges we propose sstoken a selfmodulated and semanticaware token selection approach sstoken leverages readily accessible history models to compute the pertoken loss difference with the current model which serves as a selfmodulated signal that enables the model to adaptively select tokens along its optimization trajectory rather than relying on excess loss from an offlinetrained reference model as in prior works we further introduce a semanticaware attentionbased token importance estimation metric orthogonal to lossbased selection and providing complementary semantic information for more effective filtering extensive experiments across different model families and scales demonstrate that both selfmodulated selection and semanticaware selection alone outperform fulldata finetuning while their integrationsstokenachieves synergistic gains and further surpasses prior tokenlevel selection methods delivering performance improvements while maintaining training efficiency
the lack of a concrete definition for artificial general intelligence agi obscures the gap between todays specialized ai and humanlevel cognition this paper introduces a quantifiable framework to address this defining agi as matching the cognitive versatility and proficiency of a welleducated adult to operationalize this we ground our methodology in cattellhorncarroll theory the most empirically validated model of human cognition the framework dissects general intelligence into ten core cognitive domainsincluding reasoning memory and perceptionand adapts established human psychometric batteries to evaluate ai systems application of this framework reveals a highly jagged cognitive profile in contemporary models while proficient in knowledgeintensive domains current ai systems have critical deficits in foundational cognitive machinery particularly longterm memory storage the resulting agi scores eg gpt at gpt at concretely quantify both rapid progress and the substantial gap remaining before agi
fair transparent and explainable decisionmaking remains a critical challenge in olympic and paralympic combat sports this paper presents emph an explainable ai ecosystem designed to support referees coaches and athletes in real time during taekwondo competitions and training the system integrates posebased action recognition using graph convolutional networks gcns epistemic uncertainty modeling through credal sets and explainability overlays for visual decision support a set of interactive dashboards enables humanai collaboration in referee evaluation athlete performance analysis and parataekwondo classification beyond automated scoring incorporates modules for referee training fairness monitoring and policylevel analytics within the world taekwondo ecosystem experimental validation on competition data demonstrates an reduction in decision review time and referee trust in aiassisted decisions the framework thus establishes a transparent and extensible pipeline for trustworthy datadriven officiating and athlete assessment by bridging realtime perception explainable inference and governanceaware design represents a step toward equitable accountable and humanaligned ai in sports
reinforcement learning with verifiable rewards rlvrbased posttraining of large language models llms has been shown to improve accuracy on reasoning tasks and continues to attract significant attention existing rlvr methods however typically treat all tokens uniformly without accounting for tokenlevel advantages these methods primarily evaluate performance based on final answer correctness or passk accuracy and yet make claims about rl posttraining leading to improved reasoning traces this motivates our investigation into the effect of rl posttraining on intermediate tokens which are not directly incentivized to study this we design an experimental setup using the grpo algorithm with qwenb model on the gsmk dataset we introduce trace coherence a firstorder logic folbased measure to capture the consistency of reasoning steps by identifying errors in the traces we distinguish between trace validity and trace coherence noting that the former implies logical soundness while the latter measures local coherence via lack of errors our results show that rl posttraining overall improves trace coherence with the most significant gains on problems where the base model fails but the rl model succeeds surprisingly rl enhances local coherence without necessarily producing valid or correct solutions this highlights a crucial distinction improved local coherence in reasoning steps does not guarantee final answer correctness we argue that claims of improved reasoning via rl must be examined with care as these may be based on improved trace coherence which may not translate into fully valid mathematical proofs
goal changes are a defining feature of real world multiturn interactions yet current agent benchmarks primarily evaluate static objectives or oneshot tool use we introduce agentchangebench a benchmark explicitly designed to measure how tool augmented language model agents adapt to mid dialogue goal shifts across three enterprise domains our framework formalizes evaluation through four complementary metrics task success rate tsr for effectiveness tool use efficiency tue for reliability tool call redundancy rate tcrr for wasted effort and goalshift recovery time gsrt for adaptation latency agentchangebench comprises task sequences and five user personas each designed to trigger realistic shift points in ongoing workflows using this setup we evaluate several frontier models and uncover sharp contrasts obscured by traditional textpassk scores for example gpto reaches recovery on airline booking shifts while gemini collapses to and retail tasks show near perfect parameter validity yet redundancy rates above revealing major inefficiencies these findings demonstrate that high raw accuracy does not imply robustness under dynamic goals and that explicit measurement of recovery time and redundancy is essential agentchangebench establishes a reproducible testbed for diagnosing and improving agent resilience in realistic enterprise settings
diffusion language models dlms are emerging as a powerful and promising alternative to the dominant autoregressive paradigm offering inherent advantages in parallel generation and bidirectional context modeling however the performance of dlms on code generation tasks which have stronger structural constraints is significantly hampered by the critical tradeoff between inference speed and output quality we observed that accelerating the code generation process by reducing the number of sampling steps usually leads to a catastrophic collapse in performance in this paper we introduce efficient sampling with adaptive acceleration and backtracking enhanced remasking ie saber a novel trainingfree sampling algorithm for dlms to achieve better inference speed and output quality in code generation specifically saber is motivated by two key insights in the dlm generation process it can be adaptively accelerated as more of the code context is established it requires a backtracking mechanism to reverse the generated tokens extensive experiments on multiple mainstream code generation benchmarks show that saber boosts pass accuracy by an average improvement of over mainstream dlm sampling methods meanwhile achieving an average inference speedup by leveraging the inherent advantages of dlms our work significantly narrows the performance gap with autoregressive models in code generation
ulating consumer decisionmaking is vital for designing and evaluating marketing strategies before costly real world deployment however postevent analyses and rulebased agentbased models abms struggle to capture the complexity of human behavior and social interaction we introduce an llmpowered multiagent ulation framework that models consumer decisions and social dynamics building on recent advances in large language model ulation in a sandbox envi ronment our framework enables generative agents to interact express internal reasoning form habits and make purchasing decisions without predefined rules in a pricediscount marketing scenario the system delivers actionable strategytesting outcomes and reveals emergent social patterns beyond the reach of con ventional methods this approach offers marketers a scalable lowrisk tool for preimplementation testing reducing reliance on timeintensive postevent evaluations and lowering the risk of underperforming campaigns
recent work has highlighted the importance of monitoring chainofthought reasoning for ai safety however current approaches that analyze textual reasoning steps can miss subtle harmful patterns and may be circumvented by models that hide unsafe reasoning we present a sentencelevel labeled dataset that enables activationbased monitoring of safety behaviors during llm reasoning our dataset contains reasoning sequences with sentencelevel annotations of safety behaviors such as expression of safety concerns or speculation on user intent which we use to extract steering vectors for detecting and influencing these behaviors within model activations the dataset fills a key gap in safety research while existing datasets label reasoning holistically effective application of steering vectors for safety monitoring could be improved by identifying precisely when specific behaviors occur within reasoning chains we demonstrate the datasets utility by extracting representations that both detect and steer safety behaviors in model activations showcasing the potential of activationlevel techniques for improving safety oversight on reasoning content warning this paper discusses ai safety in the context of harmful prompts and may contain references to potentially harmful content
small language models slms offer compelling advantages in deployment cost and latency but their accuracy often lags behind larger models particularly for complex domainspecific tasks while supervised finetuning can help bridge this performance gap it requires substantial manual effort in data preparation and iterative optimization we present padaagent patternguided data augmentation agent an evaluationdriven approach that streamlines the data augmentation process for slms through coordinated operations unlike stateoftheart approaches that focus on model training errors only and generating errorcorrecting samples padaagent discovers failure patterns from the validation data via evaluations and drafts targeted data augmentation strategies aiming to directly reduce the generalization gap our experimental results demonstrate significant improvements over stateoftheart llmbased data augmentation approaches for llama b instruct model finetuning
what does it truly mean for a language model to reason most current evaluations and benchmarks reward models correct standalone answersbut correctness alone reveals little about the process that produced them in this work we explore a different perspective reasoning is not a static chain of steps but a dynamic trajectory where ideas interact clash and evolve into deeper insights to capture this dynamic we draw on a wellestablished philosophical tradition dialectics where reasoning unfolds through thesis antithesis and synthesis building on this we present siev a structured framework that evaluates reasoning of llms through dialectics unlike conventional evaluations siev assesses not only the conclusion a model reaches but how it gets there its ability to resolve tension integrate distinct ideas and synthesize higherorder reasoning this lens uncovers significant reasoning gaps in stateoftheart models even under saturated benchmarks like gsm and mmlu for instance gptchat a recent model loses over points out of when evaluated with siev on gsm our findings highlight that adopting a processoriented philosophically grounded approach enables a deeper more rigorous and more discriminative assessment of llm reasoning
large language models llms have redefined complex task automation with exceptional generalization capabilities despite these advancements stateoftheart methods rely on singlestrategy prompting missing the synergy of diverse reasoning approaches no single strategy excels universally highlighting the need for frameworks that fuse strategies to maximize performance and ensure robustness we introduce the select mix and reinvent smart framework an innovative strategy fusion approach designed to overcome this constraint by creating balanced and efficient solutions through the seamless integration of diverse reasoning strategies unlike existing methods which employ llms merely as evaluators smart uses them as intelligent integrators unlocking the best of all worlds across tasks extensive empirical evaluations across benchmarks in reasoning planning and sequential decisionmaking highlight the robustness and adaptability of smart the framework consistently outperforms stateoftheart baselines in solution quality constraint adherence and performance metrics this work redefines llmdriven decisionmaking by pioneering a new paradigm in crossstrategy calibration unlocking superior outcomes for reasoning systems and advancing the boundaries of selfrefining methodologies
a central challenge in large language model inference is the tradeoff between generation speed and output quality autoregressive models produce highquality text but generate tokens sequentially diffusion models can generate tokens in parallel but often need many iterations to match the same quality we propose planned diffusion a hybrid method that combines the strengths of both paradigms planned diffusion works in two stages first the model creates a short autoregressive plan that breaks the output into smaller independent spans second the model generates these spans ultaneously using diffusion this approach expands the speedquality pareto frontier and provides a practical path to faster highquality text generation on alpacaeval a suite of instructionfollowing prompts planned diffusion achieves paretooptimal tradeoff between quality and latency achieving x to x speedup over autoregressive generation with only to drop in win rate respectively our sensitivity analysis shows that the planning mechanism of planned diffusion is minimal and reliable and ple runtime knobs exist to provide flexible control of the qualitylatency tradeoff
large language models llms deliver powerful reasoning and generation capabilities but incur substantial runtime costs when operating in agentic workflows that chain together lengthy prompts and process rich data streams we introduce compactprompt an endtoend pipeline that merges hard prompt compression with lightweight filelevel data compression compactprompt first prunes lowinformation tokens from prompts using selfinformation scoring and dependencybased phrase grouping in parallel it applies ngram abbreviation to recurrent textual patterns in attached documents and uniform quantization to numerical columns yielding compact yet semantically faithful representations integrated into standard llm agents compactprompt reduces total token usage and inference cost by up to on benchmark dataset like tatqa and finqa while preserving output quality results in less than accuracy drop for claudesonnet and gptmini compactprompt helps visualize realtime compression decisions and quantify costperformance tradeoffs laying the groundwork for leaner generative ai pipelines
a formalization of a subjectevent ontology is proposed for modeling complex dynamic systems without reliance on global time key principles event as an act of fixation a subject discerns and fixes changes according to models conceptual templates available to them causal order via happensbefore the order of events is defined by explicit dependencies not timestamps making the ontology executable via a declarative dataflow mechanism ensuring determinism models as epistemic filters a subject can only fix what falls under its known concepts and properties presumption of truth the declarative content of an event is available for computation from the moment of fixation without external verification the formalization includes nine axioms aa ensuring the correctness of executable ontologies monotonicity of history i acyclicity of causality i traceability i special attention is given to the modelbased approach a event validation via schemas actor authorization automatic construction of causal chains w without global time practical applicability is demonstrated on the boldsea system a workflow engine for executable ontologies where the theoretical constructs are implemented in bsl boldsea semantic language the formalization is applicable to distributed systems microservice architectures dlt platforms and multiperspectivity scenarios conflicting facts from different subjects
large language models llms have shown remarkable reasoning capabilities in mathematical and scientific tasks to enhance complex reasoning multiagent systems have been proposed to harness the collective intelligence of llm agents however existing collaboration structures are either predefined or rely on majority voting or roundtable debates which can suppress correct but less dominant agent contributions recent approaches model multiagent systems as graph networks but optimize purely for agent performance neglecting the quality of interactions we hypothesize that effective agent communication is crucial for multiagent reasoning and that debating quality plays a significant role to address this we propose ours a multiagent verbal reinforcement learning algorithm that dynamically constructs and refines multiagent collaboration structures our method defines action spaces and a feedback mechanism that evaluates communication robustness and coherence throughout the debate the final decision is achieved through a majority vote over all the agents we assess ours on various reasoning tasks including mathematical reasoning creative writing scientific reasoning and numerical sorting results demonstrate that our approach significantly outperforms singleagent prompting methods and stateoftheart multiagent frameworks on diverse tasks
large language models llms are increasingly deployed as agents expected to decompose goals invoke tools and verify results in dynamic environments realizing these capabilities requires access to agentic data structured interaction records that couple user intents with tool specifications argumentgrounded calls and verifiable execution traces however collecting such data from human annotators is costly timeconsuming and difficult to scale we present a unified framework for synthesizing agentic data using only llms without any humanintheloop supervision this framework decomposes generation into modular pipelines that produce complete interaction records spanning task specifications tool definitions policy pseudocode natural language exchanges and execution traces records conform to strict syntactic and semantic constraints ensuring machineparseability and faithful alignment across inputs outputs and tool calls beyond single tasks there is support for both multitask and multiturn agent interactions enabling the construction of datasets that reflect the full spectrum of tooluse competencies to ensure quality and consistency the framework integrates constrained generation formats jsonschema validation and judgebased filtering this paper formalizes the schema for agentic records details the prompt design principles that guide generation and introduces scalable pipelines for highquality synthetic data by providing a reproducible llmonly alternative to manual collection hence advancing the development of agentic llms capable of robust tool use
multi turn intent understanding is central to task oriented chatbots yet real deployments face tight token budgets and noisy contexts and most retrieval pipelines asize relevance while overlooking set level diversity and confounds such as more context or exemplar order we ask whether retrieval diversity rather than longer prompts systematically improves llm intent understanding under fixed budgets we present a diversity aware retrieval framework that selects in context exemplars to balance intent coverage and linguistic variety and integrates this selection with standard llm decoders the evaluation enforces budget matched prompts and randomized positions and includes sensitivity analyses over exemplar count diversity strength and backbone size on multiwoz and sgd the approach achieves strong gains in joint goal accuracy under equal token budgets surpassing strong llmdst baselines with consistent improvements across k from to and moderate latency overall the study isolates and validates the impact of content diversity in retrieval and offers a ple deployable selection principle for building accurate budget constrained multi turn intent systems
the proliferation of large language model llm architectures presents a fundamental challenge valuable taskspecific behaviors learned through finetuning methods like lowrank adaptation lora are effectively trapped within their source models architecture herein referred to architectural lockin existing transfer methods attempt to bridge this gap by aligning the static weight spaces of models a brittle and indirect approach that relies on tenuous correlations between parameter geometries this paper introduces a fundamentally different and more direct paradigm the cartridge activation space transfer cast a novel framework that liberates loraencoded behaviors by learning a direct nonlinear mapping between the activation manifolds the geometric structures formed by the models internal neuron activations of two distinct llm architectures cast treats a pretrained lora as a frozen behavioral kernel it learns a set of lightweight bidirectional projection heads that translate the target models activation stream into the source models latent space apply the frozen kernel and project the result back this process trained on a general text corpus without any taskspecific data effectively decouples the learned skill from the source architecture we demonstrate that cast enables true zeroshot translation of any standard lora adapter our experiments including transfers between heterogeneous model families like llama and mistral show that casttranslated adapters achieve of the performance of a lora fully retrained on the target model quantitatively outperforming current weightspace transfer techniques and establishing a new stateoftheart in model interoperability
while multimodal large language models mllms excel at holistic understanding they struggle in capturing the dense world with complex scenes requiring finegrained analysis of intricate details and object interrelationships regionlevel mllms have been a promising step however previous attempts are generally optimized to understand given regions in isolation neglecting crucial global contexts to address this we introduce grasp any region gar for comprehen sive regionlevel visual understanding empowered by an effective roialigned feature replay technique gar supports precise perception by leveraging necessary global contexts and modeling interactions between multiple prompts together it then naturally achieves advanced compositional reasoning to answer specific freeform questions about any region shifting the paradigm from passive description to active dialogue moreover we construct garbench which not only provides a more accurate evaluation of singleregion comprehension but also more importantly measures interactions and complex reasoning across multiple regions extensive experiments have demonstrated that garb not only maintains the stateoftheart captioning capabilities eg outperforming damb on dlcbench but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities even surpassing internvlb on garbenchvqa more importantly our zeroshot garb even outperforms indomain videoreferb on videoreferbenchq indicating its strong capabilities can be easily transferred to videos
growing evidence suggests that large language models do not use their depth uniformly yet we still lack a finegrained understanding of their layerwise prediction dynamics in this paper we trace the intermediate representations of several openweight models during inference and reveal a structured and nuanced use of depth specifically we propose a guessthenrefine framework that explains how llms internally structure their computations to make predictions we first show that the topranked predictions in early llm layers are composed primarily of highfrequency tokens which act as statistical guesses proposed by the model early on due to the lack of appropriate contextual information as contextual information develops deeper into the model these initial guesses get refined into contextually appropriate tokens even highfrequency token predictions from early layers get refined of the time indicating that correct token prediction is not oneanddone we then go beyond frequencybased prediction to examine the dynamic usage of layer depth across three case studies i partofspeech analysis shows that function words are on average the earliest to be predicted correctly ii fact recall task analysis shows that in a multitoken answer the first token requires more computational depth than the rest iii multiplechoice task analysis shows that the model identifies the format of the response within the first half of the layers but finalizes its response only toward the end together our results provide a detailed view of depth usage in llms shedding light on the layerbylayer computations that underlie successful predictions and providing insights for future works to improve computational efficiency in transformerbased models
despite their remarkable capabilities large language models llms struggle to effectively leverage historical interaction information in dynamic and complex environments memory systems enable llms to move beyond stateless interactions by introducing persistent information storage retrieval and utilization mechanisms however existing memory systems often introduce substantial time and computational overhead to this end we introduce a new memory system called lightmem which strikes a balance between the performance and efficiency of memory systems inspired by the atkinsonshiffrin model of human memory lightmem organizes memory into three complementary stages first cognitioninspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics next topicaware shortterm memory consolidates these topicbased groups organizing and summarizing content for more structured access finally longterm memory with sleeptime update employs an offline procedure that decouples consolidation from online inference experiments on longmemeval with gpt and qwen backbones show that lightmem outperforms strong baselines in accuracy up to gains while reducing token usage by up to x api calls by up to x and runtime by over x the code is available at
visionlanguage models vlms achieve strong results on multimodal tasks such as visual question answering yet they can still fail even when the correct visual evidence is present in this work we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively by examining layerwise attention dynamics we find that shallow layers focus primarily on text while deeper layers sparsely but reliably attend to localized evidence regions surprisingly vlms often perceive the visual evidence when outputting incorrect answers a phenomenon we term seeing but not believing that widely exists in major vlm families building on this we introduce an inferencetime intervention that highlights deeplayer evidence regions through selective attentionbased masking it requires no training and consistently improves accuracy across multiple families including llava qwen gemma and internvl these results show that vlms encode reliable evidence internally but underutilize it making such signals explicit can bridge the gap between perception and reasoning advancing the diagnostic understanding and reliability of vlms
large language models llms possess remarkable generalization capabilities but struggle with multitask adaptation particularly in balancing knowledge retention with taskspecific specialization conventional finetuning methods suffer from catastrophic forgetting and substantial resource consumption while existing parameterefficient methods perform suboptimally in complex multitask scenarios to address this we propose contextual attention modulation cam a novel mechanism that dynamically modulates the representations of selfattention modules in llms cam enhances taskspecific features while preserving general knowledge thereby facilitating more effective and efficient adaptation for effective multitask adaptation cam is integrated into our hybrid contextual attention modulation hycam framework which combines a shared fullparameter cam module with multiple specialized lightweight cam modules enhanced by a dynamic routing strategy for adaptive knowledge fusion extensive experiments on heterogeneous tasks including question answering code generation and logical reasoning demonstrate that our approach significantly outperforms existing approaches achieving an average performance improvement of the implemented code and data are available to ease reproducibility at
steering cooperative multiagent reinforcement learning marl towards desired outcomes is challenging particularly when the global guidance from a human on the whole multiagent system is impractical in a largescale marl on the other hand designing external mechanisms eg intrinsic rewards and human feedback to coordinate agents mostly relies on empirical studies lacking a easytouse research tool in this work we employ multiagent influence diagrams maids as a graphical framework to address the above issues first we introduce the concept of marl interaction paradigms using maids to analyze and visualize both unguided selforganization and global guidance mechanisms in marl then we design a new marl interaction paradigm referred to as the targeted intervention paradigm that is applied to only a single targeted agent so the problem of global guidance can be mitigated in our implementation we introduce a causal inference technique referred to as prestrategy intervention psi to realize the targeted intervention paradigm since maids can be regarded as a special class of causal diagrams a composite desired outcome that integrates the primary task goal and an additional desired outcome can be achieved by maximizing the corresponding causal effect through the psi moreover the bundled relevance graph analysis of maids provides a tool to identify whether an marl learning paradigm is workable under the design of an marl interaction paradigm in experiments we demonstrate the effectiveness of our proposed targeted intervention and verify the result of relevance graph analysis
forecasting is not only a fundamental intellectual pursuit but also is of significant importance to societal systems such as finance and economics with the rapid advances of large language models llms trained on internetscale data it raises the promise of employing llms to forecast realworld future events an emerging paradigm we call llmasaprophet this paper systematically investigates such predictive intelligence of llms to this end we build prophet arena a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages in order to support our controlled and largescale experimentation our comprehensive evaluation reveals that many llms already exhibit impressive forecasting capabilities reflected in eg their small calibration errors consistent prediction confidence and promising market returns however we also uncover key bottlenecks towards achieving superior predictive intelligence via llmasaprophet such as llms inaccurate event recalls misunderstanding of data sources and slower information aggregation compared to markets when resolution nears
clinicians need ranking systems that work in real time and still justify their choices motivated by the need for a lowlatency decoderbased reranker we present ogrank a singledecoder approach that pairs a pooled firsttoken scoring signal with an uncertaintygated explanation step the model scores all candidates in one pass and generates a brief structured rationale only when the list is genuinely ambiguous keeping latency predictable trained with a curriculum that concentrates effort on hard cases ogrank delivers strong effectiveness on encounterscoped order selection fast path recall ndcg and improves further when the gate activates recall ndcg at a gate rate while compact backbones show ilar gains under the same policy encoder baselines trail in both effectiveness and flexibility the result is a practical recipe rank fast by default and explain when it helps a pattern that applies broadly to decision tasks where selective generation buys accuracy at acceptable cost the singlepolicy design plifies deployment and budget planning and the curriculum principle spend more on the hard cases less on the easy ones readily transfers beyond clinical order selection
effective code generation with language models hinges on two critical factors accurately understanding the intent of the prompt and generating code that applies algorithmic reasoning to produce correct solutions capable of passing diverse test cases while adhering to the syntax of the target programming language unlike other language tasks code generation requires more than accurate token prediction it demands comprehension of solutionlevel and structural relationships rather than merely generating the most likely tokens very large language model vllm are capable of generating detailed steps toward the correct solution of complex tasks where reasoning is crucial in solving the problem such reasoning capabilities may be absent in smaller language models therefore in this work we distill the reasoning capabilities of a vllm into a smaller more efficient model that is faster and cheaper to deploy our approach trains the model to emulate the reasoning and problemsolving abilities of the vllm by learning to identify correct solution pathways and establishing a structural correspondence between problem definitions and potential solutions through a novel method of structureaware loss optimization this enables the model to transcend tokenlevel generation and to deeply grasp the overarching structure of solutions for given problems experimental results show that our finetuned model developed through a cheap and ple to implement process significantly outperforms our baseline model in terms of pass average data flow and average syntax match metrics across the mbpp mbpp plus and humaneval benchmarks
misinformation spreads across web platforms through billions of daily multimodal posts that combine text and images overwhelming manual factchecking capacity supervised detection models require domainspecific training data and fail to generalize across diverse manipulation tactics we present mirage an inferencetime modelpluggable agentic framework that decomposes multimodal verification into four sequential modules visual veracity assessment detects aigenerated images crossmodal consistency analysis identifies outofcontext repurposing retrievalaugmented factual checking grounds claims in web evidence through iterative question generation and a calibrated judgment module integrates all signals mirage orchestrates visionlanguage model reasoning with targeted web retrieval outputs structured and citationlinked rationales on mmfakebench validation set samples mirage with gptomini achieves f and accuracy outperforming the strongest zeroshot baseline gptv with mmdagent at f by points while maintaining false positive rate versus for a judgeonly baseline test set results samples confirm generalization with f and accuracy ablation studies show visual verification contributes f points and retrievalaugmented reasoning contributes points our results demonstrate that decomposed agentic reasoning with web retrieval can match supervised detector performance without domainspecific training enabling misinformation detection across modalities where labeled data remains scarce
machine learning is increasingly used in the legal domain where it typically operates retrospectively by treating past case outcomes as ground truth however legal outcomes are often shaped by human interventions that are not captured in most machine learning approaches a final decision may result from a settlement an appeal or other procedural actions this creates label indeterminacy the outcome could have been different if the intervention had or had not taken place we argue that legal machine learning applications need to account for label indeterminacy methods exist that can impute these indeterminate labels but they are all grounded in unverifiable assumptions in the context of classifying cases from the european court of human rights we show that the way that labels are constructed during training can significantly affect model behaviour we therefore position label indeterminacy as a relevant concern in ai law and demonstrate how it can shape model behaviour
we develop an active inference routeplanning method for the autonomous control of intelligent agents the aim is to reconnoiter a geographical area to maintain a common operational picture to achieve this we construct an evidence map that reflects our current understanding of the situation incorporating both positive and negative sensor observations of possible target objects collected over time and diffusing the evidence across the map as time progresses the generative model of active inference uses dempstershafer theory and a gaussian sensor model which provides input to the agent the generative process employs a bayesian approach to update a posterior probability distribution we calculate the variational free energy for all positions within the area by assessing the divergence between a pignistic probability distribution of the evidence map and a posterior probability distribution of a target object based on the observations including the level of surprise associated with receiving new observations using the free energy we direct the agents movements in a ulation by taking an incremental step toward a position that minimizes the free energy this approach addresses the challenge of exploration and exploitation allowing agents to balance searching extensive areas of the geographical map while tracking identified target objects
autonomous agents rely on automated planning algorithms to achieve their objectives ulationbased planning offers a significant advantage over declarative models in modelling complex environments however relying solely on a planner that produces a single plan may not be practical as the generated plans may not always satisfy the agents preferences to address this limitation we introduce textttfbitextttltl a diverse planner explicitly designed for ulationbased planning problems textttfbitextttltl utilises linear temporal logic ltl to define semantic diversity criteria enabling agents to specify what constitutes meaningfully different plans by integrating these ltlbased diversity models directly into the search process textttfbitextttltl ensures the generation of semantically diverse plans addressing a critical limitation of existing diverse planning approaches that may produce syntactically different but semantically identical solutions extensive evaluations on various benchmarks consistently demonstrate that textttfbitextttltl generates more diverse plans compared to a baseline approach this work establishes the feasibility of semanticallyguided diverse planning in ulationbased environments paving the way for innovative approaches in realistic nonsymbolic domains where traditional modelbased approaches fail
finding nearoptimal solutions for dense multiagent pathfinding mapf problems in realtime remains challenging even for stateoftheart planners to this end we develop a hybrid framework that integrates a learned heuristic derived from magat a neural mapf policy with a graph attention scheme into a leading searchbased algorithm lacam while prior work has explored learningguided search in mapf such methods have historically underperformed in contrast our approach termed lagat outperforms both purely searchbased and purely learningbased methods in dense scenarios this is achieved through an enhanced magat architecture a pretrainthenfinetune strategy on maps of interest and a deadlock detection scheme to account for imperfect neural guidance our results demonstrate that when carefully designed hybrid search offers a powerful solution for tightly coupled challenging multiagent coordination problems
the evaluation of academic theses is a cornerstone of higher education ensuring rigor and integrity traditional methods though effective are timeconsuming and subject to evaluator variability this paper presents rubiscot an aisupported framework designed to enhance thesis evaluation from proposal to final submission using advanced natural language processing techniques including large language models retrievalaugmented generation and structured chainofthought prompting rubiscot offers a consistent scalable solution the framework includes preliminary assessments multidimensional assessments content extraction rubricbased scoring and detailed reporting we present the design and implementation of rubiscot discussing its potential to optimize academic assessment processes through consistent scalable and transparent evaluation
the cryptocurrency market offers significant investment opportunities but faces challenges including high volatility and fragmented information data integration and analysis are essential for informed investment decisions currently investors use three main approaches manual analysis across various sources which depends heavily on individual experience and is timeconsuming and prone to bias data aggregation platformslimited in functionality and depth of analysis large language model agentsbased on static pretrained models lacking realtime data integration and multistep reasoning capabilities to address these limitations we present coinvisor a reinforcement learningbased chatbot that provides comprehensive analytical support for cryptocurrency investment through a multiagent framework coinvisor integrates diverse analytical capabilities through specialized tools its key innovation is a reinforcement learningbased tool selection mechanism that enables multistep planning and flexible integration of diverse data sources this design supports realtime interaction and adaptive analysis of dynamic content delivering accurate and actionable investment insights we evaluated coinvisor through automated benchmarks on tool calling accuracy and user studies with cryptocurrency investors using our interface results show that coinvisor improves recall by and f score by over the base model in tool orchestration user studies show high satisfaction with participants preferring coinvisor to both general llms and existing crypto platforms
disease progression modeling aims to characterize and predict how a patients disease complications worsen over time based on longitudinal electronic health records ehrs accurate modeling of disease progression such as type diabetes can enhance patient subphenotyping and inform effective and timely interventions however the problem is challenging due to the need to learn continuoustime dynamics of progression patterns based on irregulartime event samples and patient heterogeneity eg different progression rates and pathways existing mechanistic and datadriven methods either lack adaptability to learn from realworld data or fail to capture complex continuoustime dynamics on progression trajectories to address these limitations we propose temporally detailed hypergraph neural ordinary differential equation tdhnode which represents disease progression on clinically recognized trajectories as a temporally detailed hypergraph and learns the continuoustime progression dynamics via a neural ode framework tdhnode contains a learnable tdhypergraph laplacian that captures the interdependency of disease complication markers within both intra and interprogression trajectories experiments on two realworld clinical datasets demonstrate that tdhnode outperforms multiple baselines in modeling the progression of type diabetes and related cardiovascular diseases
we study a webdeployed toolaugmented llm health coach with real users in a pilot with seven users rated turns offline policy evaluation ope over factorized decision heads toolstyle shows that a uniform heavytool policy raises average value on logs but harms specific subgroups most notably lowhealthliteracyhighselfefficacy users a lightweight ulator with hidden archetypes further shows that adding a small early informationgain bonus reliably shortens trait identification and improves goal success and pass together these early findings indicate an evaluationfirst path to personalization freeze the generator learn subgroupaware decision heads on typed rewards objective tool outcomes and satisfaction and always report perarchetype metrics to surface subgroup harms that averages obscure
malignant ventricular arrhythmias vtvf following acute myocardial infarction ami are a major cause of inhospital death yet early identification remains a clinical challenge while traditional risk scores have limited performance endtoend deep learning models often lack the interpretability needed for clinical trust this study aimed to develop a hybrid predictive framework that integrates a largescale electrocardiogram ecg foundation model ecgfounder with an interpretable xgboost classifier to improve both accuracy and interpretability we analyzed ecg recordings from ami patients among whom experienced inhospital vtvf the ecgfounder model was used to extract dimensional diagnostic probability features which were then refined through feature selection to train the xgboost classifier model performance was evaluated using auc and fscore and the shap method was used for interpretability the ecgfounder xgboost hybrid model achieved an auc of outperforming knn auc rnn auc and an endtoend dcnn auc shap analysis revealed that modelidentified key features such as premature ventricular complexes risk predictor and normal sinus rhythm protective factor were highly consistent with clinical knowledge we conclude that this hybrid framework provides a novel paradigm for vtvf risk prediction by validating the use of foundation model outputs as effective automated feature engineering for building trustworthy explainable aibased clinical decision support systems
as largescale multiagent systems evolve the communication protocol layer has become a critical yet underevaluated factor shaping performance and reliability despite the existence of diverse protocols aa acp anp agora etc selection is often intuitiondriven and lacks standardized guidance we introduce protocolbench a benchmark that systematically compares agent protocols along four measurable axes task success endtoend latency message or byte overhead and robustness under failures on protocolbench protocol choice significantly influences system behavior in the streaming queue scenario overall completion time varies by up to across protocols and mean endtoend latency differs by s under failstorm recovery resilience also differs consistently across protocols beyond evaluation we present protocolrouter a learnable protocol router that selects perscenario or permodule protocols from requirement and runtime signals protocolrouter reduces failstorm recovery time by up to versus the best singleprotocol baseline and achieves scenariospecific gains such as higher success in gaia we also release protocolrouterbench to standardize protocol evaluation and improve reliability at scale
heating ventilation and airconditioning hvac systems account for a substantial share of global building energy use making reliable anomaly detection essential for improving efficiency and reducing emissions classical rulebased approaches offer explainability but lack adaptability while deep learning methods provide predictive power at the cost of transparency efficiency and physical plausibility recent attempts to use large language models llms for anomaly detection improve interpretability but largely ignore the physical principles that govern hvac operations we present pillm a physicsinformed llm framework that operates within an evolutionary loop to automatically generate evaluate and refine anomaly detection rules our approach introduces physicsinformed reflection and crossover operators that embed thermodynamic and controltheoretic constraints enabling rules that are both adaptive and physically grounded experiments on the public building fault detection dataset show that pillm achieves stateoftheart performance while producing diagnostic rules that are interpretable and actionable advancing trustworthy and deployable ai for smart building systems
accurate assessment of fish freshness remains a major challenge in the food industry with direct consequences for product quality market value and consumer health conventional sensory evaluation is inherently subjective inconsistent and difficult to standardize across contexts often limited by subtle speciesdependent spoilage cues to address these limitations we propose a handcrafted featurebased approach that systematically extracts and incrementally fuses complementary descriptors including color statistics histograms across multiple color spaces and texture features such as local binary patterns lbp and graylevel cooccurrence matrices glcm from fish eye images our method captures global chromatic variations from full images and localized degradations from roi segments fusing each independently to evaluate their effectiveness in assessing freshness experiments on the freshness of the fish eyes ffe dataset demonstrate the approachs effectiveness in a standard traintest setting a lightgbm classifier achieved accuracy a improvement over the previous deep learning baseline of with augmented data an artificial neural network ann reached accuracy surpassing the prior best of by these results demonstrate that carefully engineered handcrafted features when strategically processed yield a robust interpretable and reliable solution for automated fish freshness assessment providing valuable insights for practical applications in food quality monitoring
despite advances in financial ai the automation of evidencebased reasoning remains unresolved in corporate credit assessment where qualitative nonfinancial indicators exert decisive influence on loan repayment outcomes yet resist formalization existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation this study develops and evaluates two operational large language model llmbased systems designed to generate structured reasoning from nonfinancial evidence the first is a nonadversarial singleagent system nas that produces bidirectional analysis through a singlepass reasoning pipeline the second is a debatebased multiagent system kpdmads that operationalizes adversarial verification through a tenstep structured interaction protocol grounded in karl poppers critical dialogue framework both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals compared to manual expert reporting both systems achieved substantial productivity gains nas s per case kpdmads s human baseline s the kpdmads demonstrated superior reasoning quality receiving higher median ratings in explanatory adequacy vs practical applicability vs and usability vs these findings show that structured multiagent interaction can enhance reasoning rigor and interpretability in financial ai advancing scalable and defensible automation in corporate credit assessment
singlecell rna sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures however annotating these signaturesespecially those involving poorly characterized genesremains a major challenge traditional methods such as gene set enrichment analysis gsea depend on wellcurated annotations and often perform poorly in these contexts large language models llms offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies to address this we present braincellaid braincellaid a novel multiagent ai system that integrates freetext descriptions with ontology labels to enable more accurate and robust gene set annotation by incorporating retrievalaugmented generation rag we developed a robust agentic workflow that refines predictions using relevant pubmed literature reducing hallucinations and enhancing interpretability using this workflow we achieved correct annotations for of mouse gene sets among their top predictions applying this approach we annotated brain cell clusters from the comprehensive mouse brain cell atlas generated by the brain initiative cell census network enabling novel insights into brain cell function by identifying regionspecific gene coexpression patterns and inferring functional roles of gene ensembles braincellaid also identifies basal gangliarelated cell types with neurologically meaningful descriptions hence we create a valuable resource to support communitydriven cell type annotation
toolaugmented large language models llms are increasingly employed in realworld applications but tool usage errors still hinder their reliability we introduce toolcritic a diagnostic framework that evaluates and improves llm behavior in multiturn toolaugmented dialogues toolcritic detects eight distinct error types specific to toolcalling eg premature invocation argument misalignment and misinterpretation of tool outputs and provides targeted feedback to the main llm the main llm assumed to have strong reasoning task understanding and orchestration capabilities then revises its response based on toolcritics feedback we systematically define these error categories and construct a synthetic dataset to train toolcritic experimental results on the schemaguided dialogue sgd dataset demonstrate that toolcritic improves toolcalling accuracy by up to over baselines including zeroshot prompting and selfcorrection techniques this represents a promising step toward more robust llm integration with external tools in realworld dialogue applications
the efficiency of gpu kernels is central to the progress of modern ai yet optimizing them remains a difficult and laborintensive task due to complex interactions between memory hierarchies thread scheduling and hardwarespecific characteristics while recent advances in large language models llms provide new opportunities for automated code generation existing approaches largely treat llms as singleshot generators or naive refinement tools limiting their effectiveness in navigating the irregular kernel optimization landscape we introduce an llm agentic framework for gpu kernel optimization that systematically explores the design space through multiagent collaboration grounded instruction dynamic context management and strategic search this framework mimics the workflow of expert engineers enabling llms to reason about hardware tradeoffs incorporate profiling feedback and refine kernels iteratively we evaluate our approach on kernelbench a benchmark for llmbased kernel optimization and demonstrate substantial improvements over baseline agents our system produces correct solutions where baselines often fail and achieves kernels with up to x faster runtime performance these results highlight the potential of agentic llm frameworks to advance fully automated scalable gpu kernel optimization
debugging is a core application of explainable reinforcement learning xrl algorithms however limited comparative evaluations have been conducted to understand their relative performance we propose a novel evaluation methodology to test whether users can identify an agents goal from an explanation of its decisionmaking utilising the ataris ms pacman environment and four xrl algorithms we find that only one achieved greater than random accuracy for the tested goals and that users were generally overconfident in their selections further we find that users selfreported ease of identification and understanding for every explanation did not correlate with their accuracy
a key challenge in training visionlanguage model vlm agents compared to language model llm agents lies in the shift from textual states to complex visual observations this transition introduces partial observability and demands robust world modeling we ask can vlm agents construct internal world models through explicit visual state reasoning to address this question we architecturally enforce and reward the agents reasoning process via reinforcement learning rl formulating it as a partially observable markov decision process pomdp we find that decomposing the agents reasoning into state estimation what is the current state and transition modeling what comes next is critical for success as demonstrated through five reasoning strategies our investigation into how agents represent internal beliefs reveals that the optimal representation is taskdependent natural language excels at capturing semantic relationships in general tasks while structured formats are indispensable for precise manipulation and control building on these insights we design a world modeling reward that provides dense turnlevel supervision for accurate state prediction and introduce bilevel general advantage estimation bilevel gae for turnaware credit assignment through this form of visual state reasoning a bparameter model achieves a score of across five diverse agent benchmarks representing a times improvement over its untrained counterpart and outperforming proprietary reasoning models such as gpt gemini pro and claude all experiments are conducted within our vagen framework a scalable system for training and analyzing multiturn vlm agents in diverse visual environments code and data are publicly available at
autonomous data science from raw data sources to analystgrade deep research reports has been a longstanding challenge and is now becoming feasible with the emergence of powerful large language models llms recent workflowbased data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows in this paper we introduce deepanalyzeb the first agentic llm designed for autonomous data science capable of automatically completing the endtoend pipeline from data sources to analystgrade deep research reports to tackle highcomplexity data science tasks we propose a curriculumbased agentic training paradigm that emulates the learning trajectory of human data scientists enabling llms to progressively acquire and integrate multiple capabilities in realworld environments we also introduce a datagrounded trajectory synthesis framework that constructs highquality training data through agentic training deepanalyze learns to perform a broad spectrum of data tasks ranging from data question answering and specialized analytical tasks to openended data research experiments demonstrate that with only b parameters deepanalyze outperforms previous workflowbased agents built on most advanced proprietary llms the model code and training data of deepanalyze are opensourced paving the way toward autonomous data science
traditional knowledge graphs are constrained by fixed ontologies that organize concepts within rigid hierarchical structures the root cause lies in treating domains as implicit context rather than as explicit reasoninglevel components to overcome these limitations we propose the domaincontextualized concept graph cdc a novel knowledge modeling framework that elevates domains to firstclass elements of conceptual representation cdc adopts a cdc triple structure concept relationdomain concept where domain specifications serve as dynamic classification dimensions defined on demand grounded in a cognitivelinguistic isomorphic mapping principle cdc operationalizes how humans understand concepts through contextual frames we formalize more than twenty standardized relation predicates structural logical crossdomain and temporal and implement cdc in prolog for full inference capability case studies in education enterprise knowledge systems and technical documentation demonstrate that cdc enables contextaware reasoning crossdomain analogy and personalized knowledge modeling capabilities unattainable under traditional ontologybased frameworks
visionlanguage models vlms have shown promise in graph understanding but remain limited by inputtoken constraints facing scalability bottlenecks and lacking effective mechanisms to coordinate textual and visual modalities to address these challenges we propose graphvista a unified framework that enhances both scalability and modality coordination in graph understanding for scalability graphvista organizes graph information hierarchically into a lightweight graphrag base which retrieves only taskrelevant textual descriptions and highresolution visual subgraphs compressing redundant context while preserving key reasoning elements for modality coordination graphvista introduces a planning agent that routes tasks to the most suitable modalityusing the text modality for ple property reasoning and the visual modality for local and structurally complex reasoning grounded in explicit topology extensive experiments demonstrate that graphvista scales to large graphs up to times larger than those used in existing benchmarks and consistently outperforms existing textual visual and fusionbased methods achieving up to times quality improvement over the stateoftheart baselines by fully exploiting the complementary strengths of both modalities
human interaction is inherently multimodal and fullduplex we listen while watching speak while acting and fluidly adapt to turntaking and interruptions realizing these capabilities is essential for building models ulating humans we present ellsa endtoend listen look speak and act which to our knowledge is the first fullduplex endtoend model that ultaneously perceives and generates across vision text speech and action within a single architecture enabling interaction patterns previously out of reach yielding more natural humanlike behaviors at its core is a novel samoe architecture selfattention mixtureofexperts that routes each modality to specialized experts and fuses them through a unified attention backbone this provides a generalizable solution for joint multimodal perception and concurrent generation leveraging strong pretrained components while enabling efficient modality integration and mitigating modality interference on speechinteraction and robotmanipulation benchmarks ellsa matches modalityspecific baselines while uniquely supporting advanced multimodal and fullduplex behaviors such as dialogue and action turntaking defective instruction rejection speakingwhileacting contextgrounded visual question answering and action bargeins we contend that ellsa represents a step toward more natural and general interactive intelligence contributing to the broader pursuit of artificial general intelligence all data code and model checkpoints will be released upon acceptance
multimodal knowledge graphs mkgs extend traditional knowledge graphs by incorporating visual and textual modalities enabling richer and more expressive entity representations however existing mkgs often suffer from incompleteness which hinder their effectiveness in downstream tasks therefore multimodal knowledge graph completion mkgc task is receiving increasing attention while large language models llms have shown promise for knowledge graph completion kgc their application to the multimodal setting remains underexplored moreover applying multimodal large language models mllms to the task of mkgc introduces significant challenges the large number of image tokens per entity leads to semantic noise and modality conflicts and the high computational cost of processing large token inputs to address these issues we propose efficient lightweight multimodal large language models elmm for mkgc elmm proposes a multiview visual token compressor mvtc based on multihead attention mechanism which adaptively compresses image tokens from both textual and visual views thereby effectively reducing redundancy while retaining necessary information and avoiding modality conflicts additionally we design an attention pruning strategy to remove redundant attention layers from mllms thereby significantly reducing the inference cost we further introduce a linear projection to compensate for the performance degradation caused by pruning extensive experiments on benchmark fbkimg and wnimg demonstrate that elmm achieves stateoftheart performance while substantially improving computational efficiency establishing a new paradigm for multimodal knowledge graph completion
complex systems are increasingly explored through ulationdriven engineering workflows that combine physicsbased and empirical models with optimization and analytics despite their power these workflows face two central obstacles high computational cost since accurate exploration requires many expensive ulator runs and limited transparency and reliability when decisions rely on opaque blackbox components we propose a workflow that addresses both challenges by training lightweight emulators on compact designs of experiments that i provide fast lowlatency imations of expensive ulators ii enable rigorous uncertainty quantification and iii are adapted for global and local explainable artificial intelligence xai analyses this workflow unifies every ulationbased complexsystem analysis tool ranging from engineering design to agentbased models for socioenvironmental understanding in this paper we proposea comparative methodology and practical recommendations for using surrogatebased explainability tools within the proposed workflow the methodology supports continuous and categorical inputs combines globaleffect and uncertainty analyses with local attribution and evaluates the consistency of explanations across surrogate models thereby diagnosing surrogate adequacy and guiding further data collection or model refinement we demonstrate the approach on two contrasting case studies a multidisciplinary design analysis of a hybridelectric aircraft and an agentbased model of urban segregation results show that the surrogate model and xai coupling enables largescale exploration in seconds uncovers nonlinear interactions and emergent behaviors identifies key design and policy levers and signals regions where surrogates require more data or alternative architectures
the advent of large language models llms has transformed information access and reasoning through openended natural language interaction however llms remain limited by static knowledge factual hallucinations and the inability to retrieve realtime or domainspecific information retrievalaugmented generation rag mitigates these issues by grounding model outputs in external evidence but traditional rag pipelines are often single turn and heuristic lacking adaptive control over retrieval and reasoning recent advances in agentic search address these limitations by enabling llms to plan retrieve and reflect through multistep interaction with search environments within this paradigm reinforcement learning rl offers a powerful mechanism for adaptive and selfimproving search behavior this survey provides the first comprehensive overview of rlbased agentic search organizing the emerging field along three complementary dimensions i what rl is for functional roles ii how rl is used optimization strategies and iii where rl is applied scope of optimization we summarize representative methods evaluation protocols and applications and discuss open challenges and future directions toward building reliable and scalable rl driven agentic search systems we hope this survey will inspire future research on the integration of rl and agentic search our repository is available at
the rapid evolution of agentic ai marks a new phase in artificial intelligence where large language models llms no longer merely respond but act reason and adapt this survey traces the paradigm shift in building agentic ai from pipelinebased systems where planning tool use and memory are orchestrated by external logic to the emerging modelnative paradigm where these capabilities are internalized within the models parameters we first position reinforcement learning rl as the algorithmic engine enabling this paradigm shift by reframing learning from imitating static data to outcomedriven exploration rl underpins a unified solution of llm rl task across language vision and embodied domains building on this the survey systematically reviews how each capability planning tool use and memory has evolved from externally scripted modules to endtoend learned behaviors furthermore it examines how this paradigm shift has reshaped major agent applications specifically the deep research agent asizing longhorizon reasoning and the gui agent asizing embodied interaction we conclude by discussing the continued internalization of agentic capabilities like multiagent collaboration and reflection alongside the evolving roles of the system and model layers in future agentic ai together these developments outline a coherent trajectory toward modelnative agentic ai as an integrated learning and interaction framework marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience
complex vehicle routing problems vrps remain a fundamental challenge demanding substantial expert effort for intent interpretation and algorithm design while large language models llms offer a promising path toward automation current approaches still rely on external intervention which restrict autonomy and often lead to execution errors and low solution feasibility to address these challenges we propose an agentic framework with llms afl for solving complex vehicle routing problems achieving full automation from problem instance to solution afl directly extracts knowledge from raw inputs and enables selfcontained code generation without handcrafted modules or external solvers to improve trustworthiness afl decomposes the overall pipeline into three manageable subtasks and employs four specialized agents whose coordinated interactions enforce crossfunctional consistency and logical soundness extensive experiments on complex vrps ranging from standard benchmarks to practical variants validate the effectiveness and generality of our framework showing comparable performance against meticulously designed algorithms notably it substantially outperforms existing llmbased baselines in both code reliability and solution feasibility achieving rates close to on the evaluated benchmarks
the advent of largescale artificial intelligence ai models has a transformative effect on neuroscience research which represents a paradigm shift from the traditional computational methods through the facilitation of endtoend learning from raw brain signals and neural data in this paper we explore the transformative effects of largescale ai models on five major neuroscience domains neuroimaging and data processing braincomputer interfaces and neural decoding molecular neuroscience and genomic modeling clinical assistance and translational frameworks and diseasespecific applications across neurological and psychiatric disorders these models are demonstrated to address major computational neuroscience challenges including multimodal neural data integration spatiotemporal pattern interpretation and the derivation of translational frameworks for clinical deployment moreover the interaction between neuroscience and ai has become increasingly reciprocal as biologically informed architectural constraints are now incorporated to develop more interpretable and computationally efficient models this review highlights both the notable promise of such technologies and key implementation considerations with particular asis on rigorous evaluation frameworks effective domain knowledge integration and comprehensive ethical guidelines for clinical use finally a systematic listing of critical neuroscience datasets used to derive and validate largescale ai models across diverse research applications is provided
reinforcement learning rl has become a compelling way to strengthen the multi step reasoning ability of large language models llms however prevalent rl paradigms still lean on sparse outcomebased rewards and limited exploration which often drives llms toward repetitive and suboptimal reasoning patterns in this paper we study the central question of how to design exploration for llm reasoning and introduce merci motivating exploration in llm reasoning with countbased intrinsic rewards a novel rl algorithm that augments policy optimization with a principled intrinsic reward building on the idea of countbased exploration merci leverages a lightweight coin flipping network cfn to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards we integrate merci into some advanced rl frameworks like group relative policy optimization grpo experiments on complex reasoning benchmarks demonstrate that merci encourages richer and more varied chains of thought significantly improves performance over strong baselines and helps the policy escape local routines to discover better solutions it indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning
uncertain knowledge graphs ukgs associate each triple with a confidence score to provide more precise knowledge representations recently since realworld ukgs suffer from the incompleteness uncertain knowledge graph ukg completion attracts more attention aiming to complete missing triples and confidences current studies attempt to learn ukg embeddings to solve this problem but they neglect the extremely imbalanced distributions of triple confidences this causes that the learnt embeddings are insufficient to highquality ukg completion thus in this paper to address the above issue we propose a new semisupervised confidence distribution learning sscdl method for ukg completion where each triple confidence is transformed into a confidence distribution to introduce more supervision information of different confidences to reinforce the embedding learning process sscdl iteratively learns ukg embedding by relational learning on labeled data ie existing triples with confidences and unlabeled data with pseudo labels ie unseen triples with the generated confidences which are predicted by metalearning to augment the training data and rebalance the distribution of triple confidences experiments on two ukg datasets demonstrate that sscdl consistently outperforms stateoftheart baselines in different evaluation metrics
retrievalaugmented generation rag based on knowledge graphs kgs enhances large language models llms by providing structured and interpretable external knowledge however existing kgbased rag methods struggle to retrieve accurate and diverse information from textrich kgs for complex realworld queries process reward models prms offer a way to align the retrieval process of kgbased rag with queryspecific knowledge requirements but they heavily rely on processlevel supervision signals that are expensive and hard to obtain on kgs to address this challenge we propose graphflow a framework that efficiently retrieves accurate and diverse knowledge required for realworld queries from textrich kgs graphflow employs a transitionbased flow matching objective to jointly optimize a retrieval policy and a flow estimator the flow estimator factorizes the reward of the retrieval outcome into the intermediate retrieval states such reward factorization guides the retrieval policy to retrieve candidates from kgs in proportion to their reward this allows graphflow to explore highquality regions of kgs that yield diverse and relevant results we evaluate graphflow on the stark benchmark which includes realworld queries from multiple domains over textrich kgs graphflow outperforms strong kgrag baselines including gpto by on average in hit rate and recall it also shows strong generalization to unseen kgs demonstrating its effectiveness and robustness
modern ai agents can exchange messages using protocols such as aa and acp yet these mechanisms asize communication over coordination as agent populations grow this limitation produces brittle collective behavior where individually smart agents converge on poor group outcomes we introduce the ripple effect protocol rep a coordination protocol in which agents share not only their decisions but also lightweight sensitivities signals expressing how their choices would change if key environmental variables shifted these sensitivities ripple through local networks enabling groups to align faster and more stably than with agentcentric communication alone we formalize reps protocol specification separating required message schemas from optional aggregation rules and evaluate it across scenarios with varying incentives and network topologies benchmarks across three domains i supply chain cascades beer game ii preference aggregation in sparse networks movie scheduling and iii sustainable resource allocation fishbanks show that rep improves coordination accuracy and efficiency over aa by to while flexibly handling multimodal sensitivity signals from llms by making coordination a protocollevel capability rep provides scalable infrastructure for the emerging internet of agents
engineering construction automation aims to transform natural language specifications into physically viable structures requiring complex integrated reasoning under strict physical constraints while modern llms possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain their construction competencies remain largely unevaluated to address this gap we introduce buildarena the first physicsaligned interactive benchmark designed for languagedriven engineering construction it contributes to the community in four aspects a highly customizable benchmarking framework for indepth comparison and analysis of llms an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers a d spatial geometric computation library for supporting construction based on language instructions a baseline llm agentic workflow that effectively evaluates diverse model capabilities on eight frontier llms buildarena comprehensively evaluates their capabilities for languagedriven and physicsgrounded construction automation the project page is at
rapid urbanization intensifies the demand for urban general intelligence ugi referring to ai systems that can understand and reason about complex urban environments recent studies have built urban foundation models using supervised finetuning sft of llms and mllms yet these models exhibit persistent geospatial bias producing regionally skewed predictions and limited generalization to this end we propose urbanr a reinforcement learningbased posttraining framework that aligns mllms with the objectives of ugi urbanr adopts group relative policy optimization grpo to optimize reasoning across geographic groups and employs urban region profiling as a proxy task to provide measurable rewards from multimodal urban data extensive experiments across diverse regions and tasks show that urbanr effectively mitigates geobias and improves crossregion generalization outperforming both sfttrained and closedsource models our results highlight reinforcement learning alignment as a promising pathway toward equitable and trustworthy urban intelligence
we present a typed computer language doug in which all typed programs may be proved to halt in polynomial time encoded in a vectorsymbolic architecture vsa doug is just an encoding of the light linear functional programming language llfpl described by schimanski ch the types of doug are encoded using a slotvalue encoding scheme based on holographic declarative memory hdm kelly the terms of doug are encoded using a variant of the lisp vsa defined by flanagan doug allows for some points on the embedding space of a neural network to be interpreted as types where the types of nearby points are ilar both in structure and content types in doug are therefore learnable by a neural network following chollet card and newell we view skill as the application of a procedure or program of action that causes a goal to be satisfied skill acquisition may therefore be expressed as program synthesis using doug we hope to describe a form of learning of skilled behaviour that follows a humanlike pace of skill acquisition ie substantially faster than brute force heathcote exceeding the efficiency of all currently existing approaches kaplan jones chollet our approach brings us one step closer to modeling human mental representations as they must actually exist in the brain and those representations acquisition as they are actually learned
large language models llms have shown strong reasoning capabilities with models like openais oseries and deepseek r excelling at tasks such as mathematics coding logic and puzzles through reinforcement learning with verifiable rewards rlvr however their ability to solve more complex optimization problems particularly nphard tasks remains underexplored to bridge this gap we propose npengine the first comprehensive framework for training and evaluating llms on nphard problems npengine covers tasks across five domains each equipped with i a controllable instance generator ii a rulebased verifier and iii a heuristic solver that provides imate optimal solutions as ground truth this generatorverifierheuristic pipeline enables scalable and verifiable rlvr training under hierarchical difficulties we also introduce npbench a benchmark derived from npenginedata specifically designed to evaluate llms ability to tackle nphard level reasoning problems focusing not only on feasibility but also on solution quality additionally we present qwenbnp a model trained via zerorlvr with curriculum learning on qwenbinstruct which significantly outperforms gpto on npbench and achieves sota performance with the same model size beyond indomain tasks we demonstrate that rlvr training on npenginedata enables strong outofdomain ood generalization to reasoning tasks logic puzzles math and knowledge as well as nonreasoning tasks such as instruction following we also observe a scaling trend increasing task diversity improves ood generalization these findings suggest that taskrich rlvr training is a promising direction for advancing llms reasoning ability revealing new insights into the scaling laws of rlvr
as customer feedback becomes increasingly central to strategic growth the ability to derive actionable insights from unstructured reviews is essential while traditional aidriven systems excel at predicting user preferences far less work has focused on transforming customer reviews into prescriptive businessfacing recommendations this paper introduces reviewsense a novel prescriptive decision support framework that leverages advanced large language models llms to transform customer reviews into targeted actionable business recommendations by identifying key trends recurring issues and specific concerns within customer sentiments reviewsense extends beyond preferencebased systems to provide businesses with deeper insights for sustaining growth and enhancing customer loyalty the novelty of this work lies in integrating clustering llm adaptation and expertdriven evaluation into a unified businessfacing pipeline preliminary manual evaluations indicate strong alignment between the models recommendations and business objectives highlighting its potential for driving datainformed decisionmaking this framework offers a new perspective on aidriven sentiment analysis demonstrating its value in refining business strategies and maximizing the impact of customer feedback
personalized and continuous interactions are the key to enhancing user experience in todays large language model llmbased conversational systems however the finite context windows and static parametric memory make it difficult to model the crosssession longterm user states and behavioral consistency currently the existing solutions to this predicament such as retrievalaugmented generation rag and explicit memory systems primarily focus on factlevel storage and retrieval lacking the capability to distill latent preferences and deep traits from the multiturn dialogues which limits the longterm and effective user modeling directly leading to the personalized interactions remaining shallow and hindering the crosssession continuity to realize the longterm memory and behavioral consistency for language agents in llm era we propose a selfevolving memory framework rgmem inspired by the ideology of classic renormalization group rg in physics this framework enables to organize the dialogue history in multiple scales it first extracts semantics and user insights from episodic fragments then through hierarchical coarsegraining and rescaling operations progressively forms a dynamicallyevolved user profile the core innovation of our work lies in modeling memory evolution as a multiscale process of information compression and emergence which accomplishes the highlevel and accurate user profiles from noisy and microscopiclevel interactions
this paper proposes the humanoidinspired structural causal model hscm a novel causal framework inspired by human intelligence designed to overcome the limitations of conventional domain generalization models unlike approaches that rely on statistics to capture datalabel dependencies and learn distortioninvariant representations hscm replicates the hierarchical processing and multilevel learning of human vision systems focusing on modeling finegrained causal mechanisms by disentangling and reweighting key image attributes such as color texture and shape hscm enhances generalization across diverse domains ensuring robust performance and interpretability leveraging the flexibility and adaptability of human intelligence our approach enables more effective transfer and learning in dynamic complex environments through both theoretical and empirical evaluations we demonstrate that hscm outperforms existing domain generalization models providing a more principled method for capturing causal relationships and improving model robustness the code is available at
current approaches to enhancing llm reasoning follows two isolated paradigms monitorgenerate methods like planandsolve wang et al and selfdiscover zhou et al excel at strategic planning but lack mechanisms to verify whether selected strategies succeed while generateverify approaches like selfverification weng et al and selfrefine madaan et al iteratively refine outputs but commence generation blindly without task assessment this separation creates inefficiencies strategies fail without feedback and refinement occurs without strategic grounding we address this gap by implementing flavells cognitive monitoring model from the broader monitorgenerateverify framework oh and gobet operationalising it as a threephase iterative system on gsmk preliminary results show accuracy versus for selfrefine and for selfverification while requiring fewer attempts vs at increased inference cost these initial findings suggest upfront monitoring produces higherquality initial solutions that reduce refinement needs though evaluation beyond arithmetic reasoning is needed to establish generalisability
from media platforms to chatbots algorithms shape how people interact learn and discover information such interactions between users and an algorithm often unfold over multiple steps during which strategic users can guide the algorithm to better align with their true interests by selectively engaging with content however users frequently exhibit inconsistent preferences they may spend considerable time on content that offers little longterm value inadvertently signaling that such content is desirable focusing on the user side this raises a key question what does it take for such users to align the algorithm with their true interests to investigate these dynamics we model the users decision process as split between a rational system that decides whether to engage and an impulsive system that determines how long engagement lasts we then study a multileader singlefollower extensive stackelberg game where users specifically system lead by committing to engagement strategies and the algorithm bestresponds based on observed interactions we define the burden of alignment as the minimum horizon over which users must optimize to effectively steer the algorithm we show that a critical horizon exists users who are sufficiently foresighted can achieve alignment while those who are not are instead aligned to the algorithms objective this critical horizon can be long imposing a substantial burden however even a small costly signal eg an extra click can significantly reduce it overall our framework explains how users with inconsistent preferences can align an engagementdriven algorithm with their interests in a stackelberg equilibrium highlighting both the challenges and potential remedies for achieving alignment
existing concept erasure methods for texttoimage diffusion models commonly rely on fixed anchor strategies which often lead to critical issues such as concept reemergence and erosion to address this we conduct causal tracing to reveal the inherent sensitivity of erasure to anchor selection and define sibling exclusive concepts as a superior class of anchors based on this insight we propose select siblingexclusive evaluation for contextual targeting a dynamic anchor selection framework designed to overcome the limitations of fixed anchors our framework introduces a novel twostage evaluation mechanism that automatically discovers optimal anchors for precise erasure while identifying critical boundary anchors to preserve related concepts extensive evaluations demonstrate that select as a universal anchor solution not only efficiently adapts to multiple erasure frameworks but also consistently outperforms existing baselines across key performance metrics averaging only seconds for anchor mining of a single concept
large language models llms often produce fluent reasoning steps while violating ple mathematical or logical constraints we introduce medrulekg a compact typed knowledge graph coupled with a symbolic verifier designed to enforce mathematically interpretable rules in reasoning tasks medrulekg encodes entities relations and three domaininspired rules while the verifier checks predictions and applies minimal corrections to guarantee consistency on a example fdaderived benchmark grounding in medrulekg improves exact match em from to and adding the verifier yields em while eliminating rule violations entirely we demonstrate how medrulekg provides a general scaffold for safe mathematical reasoning discuss ablations and release code and data to encourage reproducibility
nowadays there has been a growing trend in the field of highenergy physics hep in both its experimental and phenomenological studies to incorporate machine learning ml and its specialized branch deep learning dl this review paper provides a thorough illustration of these applications using different ml and dl approaches the first part of the paper examines the basics of various particle physics types and establishes guidelines for assessing particle physics alongside the available learning models next a detailed classification is provided for representing jets that are reconstructed in highenergy collisions mainly in protonproton collisions at welldefined beam energies this section covers various datasets preprocessing techniques and feature extraction and selection methods the presented techniques can be applied to future hadronhadron colliders hhc such as the highluminosity lhc hllhc and the future circular collider hadronhadron fcchh the authors then explore several ai techniques analyses designed specifically for both image and pointcloud pc data in hep additionally a closer look is taken at the classification associated with jet tagging in hadron collisions in this review various stateoftheart sota techniques in ml and dl are examined with a focus on their implications for hep demands more precisely this discussion addresses various applications in extensive detail such as jet tagging jet tracking particle classification and more the review concludes with an analysis of the current state of hep using dl methodologies it highlights the challenges and potential areas for future research which are illustrated for each application
precisely defining consciousness and identifying the mechanisms that effect it is a longstanding question particularly relevant with advances in artificial intelligence the scientific community is divided between physicalism and natural dualism physicalism posits consciousness is a physical process that can be modeled computationally natural dualism rejects this hypothesis finding a computational model has proven elusive particularly because of conflation of consciousness with other cognitive capabilities exhibited by humans such as intelligence and physiological sensations here we show such a computational model that precisely models consciousness natural or artificial identifying the structural and functional mechanisms that effect it confirming the physicalism hypothesis we found such a model is obtainable when including the underlying biological or digital substrate and accounting for reactive behavior in substrate subsystems eg autonomous physiological responses results show that unlike all other computational processes consciousness is not independent of its substrate and possessing it is an evolutionary advantage for intelligent entities our result shows there is no impediment to the realization of fully artificial consciousness but surprisingly that it is also possible to realize artificial intelligence of arbitrary level without consciousness whatsoever and that there is no advantage in imbuing artificial systems with consciousness
the presence of interictal epileptiform discharges ieds in electroencephalogram eeg recordings is a critical biomarker of epilepsy even trained neurologists find detecting ieds difficult leading many practitioners to turn to machine learning for help while existing machine learning algorithms can achieve strong accuracy on this task most models are uninterpretable and cannot justify their conclusions absent the ability to understand model reasoning doctors cannot leverage their expertise to identify incorrect model predictions and intervene accordingly to improve the humanmodel interaction we introduce protoeegknn an inherently interpretable model that follows a ple casebased reasoning process protoeegknn reasons by comparing an eeg to ilar eegs from the training set and visually demonstrates its reasoning both in terms of ied morphology shape and spatial distribution location we show that protoeegknn can achieve stateoftheart accuracy in ied detection while providing explanations that experts prefer over existing approaches
the extent to which different neural or artificial neural networks models rely on equivalent representations to support ilar tasks remains a central question in neuroscience and machine learning prior work has typically compared systems using a single representational ilarity metric yet each captures only one facet of representational structure to address this we leverage a suite of representational ilarity metricseach capturing a distinct facet of representational correspondence such as geometry unitlevel tuning or linear decodabilityand assess brain region or model separability using multiple complementary measures metrics that preserve geometric or tuning structure eg rsa soft matching yield stronger regionbased discrimination whereas more flexible mappings such as linear predictivity show weaker separation these findings suggest that geometry and tuning encode brainregion or modelfamilyspecific signatures while linearly decodable information tends to be more globally shared across regions or models to integrate these complementary representational facets we adapt ilarity network fusion snf a framework originally developed for multiomics data integration snf produces substantially sharper regional and model familylevel separation than any single metric and yields robust composite ilarity profiles moreover clustering cortical regions using snfderived ilarity scores reveals a clearer hierarchical organization that aligns closely with established anatomical and functional hierarchies of the visual cortexsurpassing the correspondence achieved by individual metrics
the role of reasoning in audio large language models remains widely underexplored as introducing a reasoning process often degrades rather than improves performance during inference a phenomenon we term testtime inverse scaling where longer reasoning chains yield progressively worse results we demonstrate that this stems not from fundamental limitations of reasoning itself but from inadequate training models without proper guidance for the reasoning process produce hallucinatory inconsistent reasoning that accumulates errors over longer chains to address these challenges we introduce cesar consistent effective and scalable audio reasoners shifting from outcome verification to rewarding the reasoning process our online reinforcement learning framework employs group relative policy optimization with a multifaceted reward suite that incentivizes not only correctness and format but also consistency structured analytical patterns causal reasoning domainknowledge integration and calibrated reasoning depth cesar resolves testtime inverse scaling transforming reasoning from detriments into gains while revealing modelspecific reasoning sweet spots where performance peaks during testtime scaling we achieve stateoftheart results on mmau testmini substantially outperforming gemini pro and gpto audio and nearhumanlevel performance on mmsu reasoning tasks through aiasjudge evaluations and qualitative comparisons we provide both quantitative and qualitative validation of our improved reasoning quality importantly enhanced reasoning creates synergistic effects ultaneously improving multimodal reasoning and perception capabilities overall cesar establishes a principled method for developing robust and scalable reasoning in audio llms
financial time series forecasting faces a fundamental challenge predicting optimal asset allocations requires understanding regimedependent correlation structures that transform during crisis periods existing graphbased spatiotemporal learning approaches rely on predetermined graph topologiescorrelation thresholds sector classificationsthat fail to adapt when market dynamics shift across different crisis mechanisms credit contagion pandemic shocks or inflationdriven selloffs we present crisp crisisresilient investment through spatiotemporal patterns a graphbased spatiotemporal learning framework that encodes spatial relationships via graph convolutional networks and temporal dynamics via bilstm with selfattention then learns sparse structures through multihead graph attention networks unlike fixedtopology methods crisp discovers which asset relationships matter through attention mechanisms filtering of connections as noise while preserving crisisrelevant dependencies for accurate regimespecific predictions trained on data encompassing credit and pandemic crises crisp demonstrates robust generalization to inflationdriven marketsa fundamentally different regimeby accurately forecasting regimeappropriate correlation structures this enables adaptive portfolio allocation that maintains profitability during downturns achieving sharpe ratio improvement over equalweight baselines and improvement over static graph methods learned attention weights provide interpretable regime detection with defensive cluster attention strengthening during crises versus marketwideemergent behavior from learning to forecast rather than imposing assumptions
landslides are a growing climate induced hazard with severe environmental and human consequences particularly in high mountain asia despite increasing access to satellite and temporal datasets timely detection and disaster response remain underdeveloped and fragmented this work introduces ccgrmas a framework leveraging a series of satellite observations and environmental signals to enhance the accuracy of landslide forecasting the system is structured around three interlinked agents prediction planning and execution which collaboratively enable real time situational awareness response planning and intervention by incorporating local environmental factors and operationalizing multi agent coordination this approach offers a scalable and proactive solution for climate resilient disaster preparedness across vulnerable mountainous terrains
multimodal learning systems often encounter challenges related to modality imbalance where a dominant modality may overshadow others thereby hindering the learning of weak modalities conventional approaches often force weak modalities to align with dominant ones in learning to be the same positive learning which risks suppressing the unique information inherent in the weak modalities to address this challenge we offer a new learning paradigm learning not to be negative learning instead of enhancing weak modalities targetclass predictions the dominant modalities dynamically guide the weak modality to suppress nontarget classes this stabilizes the decision space and preserves modalityspecific information allowing weak modalities to preserve unique information without being overaligned we proceed to reveal multimodal learning from a robustness perspective and theoretically derive the multimodal negative learning mnl framework which introduces a dynamic guidance mechanism tailored for negative learning our method provably tightens the robustness lower bound of multimodal learning by increasing the unimodal confidence margin ucom and reduces the empirical error of weak modalities particularly under noisy and imbalanced scenarios extensive experiments across multiple benchmarks demonstrate the effectiveness and generalizability of our approach against competing methods the code will be available at
retrievalaugmented generation rag improves model output accuracy by leveraging external knowledge bases serving as an effective solution to address hallucination issues and knowledgeupdate delays in large language models llms however the introduction of external knowledge bases presents rag with challenges in longcontext processing significantly increasing memory consumption and inference latency existing research accelerates inference by precomputing key and value kv of the knowledge base and loading them ondemand during inference based on the access frequency of different kv chunks within the external knowledge base this paper proposes a hotnessaware rag harag inference optimization system first leveraging the numerical distribution of kv chunks we introduce a hotnessaware mixedprecision compressing and loading method to reduce disk io and memory access overhead second we design a hotnessaware data placement strategy that prioritizes storing frequently accessed kv chunks in highspeed memory to improve data access efficiency experimental results demonstrate that compared with turborag the proposed harag achieves an average speedup of x and maximum speedup of x in timetofirsttoken ttft with negligible accuracy loss
adapterbased training has emerged as a key mechanism for extending the capabilities of powerful foundation image generators enabling personalized and stylized texttoimage synthesis these adapters are typically trained to capture a specific target attribute such as subject identity using singleimage reconstruction objectives however because the input image inevitably contains a mixture of visual factors adapters are prone to entangle the target attribute with incidental ones such as pose expression and lighting this spurious correlation problem limits generalization and obstructs the models ability to adhere to the input text prompt in this work we uncover a ple yet effective solution provide the very shortcuts we wish to eliminate during adapter training in shortcutrerouted adapter training confounding factors are routed through auxiliary modules such as controlnet or lora eliminating the incentive for the adapter to internalize them the auxiliary modules are then removed during inference when applied to tasks like facial and fullbody identity injection our approach improves generation quality diversity and prompt adherence these results point to a general design principle in the era of large models when seeking disentangled representations the most effective path may be to establish shortcuts for what should not be learned
unified generalizable semantic control in video generation remains a critical open challenge existing methods either introduce artifacts by enforcing inappropriate pixelwise priors from structurebased controls or rely on nongeneralizable conditionspecific finetuning or taskspecific architectures we introduce videoasprompt vap a new paradigm that reframes this problem as incontext generation vap leverages a reference video as a direct semantic prompt guiding a frozen video diffusion transformer dit via a plugandplay mixtureoftransformers mot expert this architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval to power this approach and catalyze future research we built vapdata the largest dataset for semanticcontrolled video generation with over k paired videos across semantic conditions as a single unified model vap sets a new stateoftheart for opensource methods achieving a user preference rate that rivals leading conditionspecific commercial models vaps strong zeroshot generalization and support for various downstream applications mark a significant advance toward generalpurpose controllable video generation
aircraft collision avoidance systems is critical to modern aviation these systems are designed to predict potential collisions between aircraft and recommend appropriate avoidance actions creating effective collision avoidance systems requires solutions to a variety of technical challenges related to surveillance decision making and validation these challenges have sparked significant research and development efforts over the past several decades that have resulted in a variety of proposed solutions this article provides an overview of these challenges and solutions with an asis on those that have been put through a rigorous validation process and accepted by regulatory bodies the challenges posed by the collision avoidance problem are often present in other domains and aircraft collision avoidance systems can serve as case studies that provide valuable insights for a wide range of safetycritical systems
understanding adversarial behavior in cybersecurity has traditionally relied on highlevel intelligence reports and manual interpretation of attack chains however realtime defense requires the ability to infer attacker intent and cognitive strategy directly from lowlevel system telemetry such as intrusion detection system ids logs in this paper we propose a novel framework that leverages large language models llms to analyze suricata ids logs and infer attacker actions in terms of mitre attck techniques our approach is grounded in the hypothesis that attacker behavior reflects underlying cognitive biases such as loss aversion risk tolerance or goal persistence that can be extracted and modeled through careful observation of log sequences this lays the groundwork for future work on behaviorally adaptive cyber defense and cognitive trait inference we develop a strategydriven prompt system to segment large amounts of network logs data into distinct behavioral phases in a highly efficient manner enabling the llm to associate each phase with likely techniques and underlying cognitive motives by mapping networklayer events to highlevel attacker strategies our method reveals how behavioral signals such as tool switching protocol transitions or pivot patterns correspond to psychologically meaningful decision points the results demonstrate that llms can bridge the semantic gap between packetlevel logs and strategic intent offering a pathway toward cognitiveadaptive cyber defense keywords cognitive cybersecurity large language models llms cyberpsychology intrusion detection systems ids mitre attck cognitive biases
this study investigates the vulnerabilities of autonomous navigation and landing systems in urban air mobility uam vehicles specifically it focuses on trojan attacks that target deep learning models such as convolutional neural networks cnns trojan attacks work by embedding covert triggers within a models training data these triggers cause specific failures under certain conditions while the model continues to perform normally in other situations we assessed the vulnerability of urban autonomous aerial vehicles uaavs using the dronet framework our experiments showed a significant drop in accuracy from on clean data to on data triggered by trojan attacks to conduct this study we collected a custom dataset and trained models to ulate realworld conditions we also developed an evaluation framework designed to identify trojaninfected models this work demonstrates the potential security risks posed by trojan attacks and lays the groundwork for future research on enhancing the resilience of uam systems
medical image segmentation is essential for clinical applications such as disease diagnosis treatment planning and disease development monitoring because it provides precise morphological and spatial information on anatomical structures that directly influence treatment decisions convolutional neural networks significantly impact image segmentation however since convolution operations are local capturing global contextual information and longrange dependencies is still challenging their capacity to precisely segment structures with complicated borders and a variety of sizes is impacted by this restriction since transformers use selfattention methods to capture global context and longrange dependencies efficiently integrating transformerbased architecture with cnns is a feasible approach to overcoming these challenges to address these challenges we propose the focal modulation and bidirectional feature fusion network for medical image segmentation referred to as fmbffnet in the remainder of this paper the network combines convolutional and transformer components employs a focal modulation attention mechanism to refine context awareness and introduces a bidirectional feature fusion module that enables efficient interaction between encoder and decoder representations across scales through this design fmbffnet enhances boundary precision and robustness to variations in lesion size shape and contrast extensive experiments on eight publicly available datasets including polyp detection skin lesion segmentation and ultrasound imaging show that fmbffnet consistently surpasses recent stateoftheart methods in jaccard index and dice coefficient confirming its effectiveness and adaptability for diverse medical imaging scenarios
protein mutations can have profound effects on biological function making accurate prediction of property changes critical for drug discovery protein engineering and precision medicine current approaches rely on finetuning proteinspecific transformers for individual datasets but struggle with crossdataset generalization due to heterogeneous experimental conditions and limited target domain data we introduce two key innovations the first application of modelagnostic metalearning maml to protein mutation property prediction and a novel mutation encoding strategy using separator tokens to directly incorporate mutations into sequence context we build upon transformer architectures integrating them with maml to enable rapid adaptation to new tasks through minimal gradient steps rather than learning datasetspecific patterns our mutation encoding addresses the critical limitation where standard transformers treat mutation positions as unknown tokens significantly degrading performance evaluation across three diverse protein mutation datasets functional fitness thermal stability and solubility demonstrates significant advantages over traditional finetuning in crosstask evaluation our metalearning approach achieves better accuracy for functional fitness with less training time and better accuracy for solubility with faster training the framework maintains consistent training efficiency regardless of dataset size making it particularly valuable for industrial applications and earlystage protein design where experimental data is limited this work establishes a systematic application of metalearning to protein mutation analysis and introduces an effective mutation encoding strategy offering transformative methodology for crossdomain generalization in protein engineering
current visionlanguage models vlms struggle to ground anatomical regions in d medical images and reason about them in a stepbystep manner a key requirement of realworld diagnostic assessment this ability is essential for aligning model outputs with the diagnostic workflows clinicians use in practice enabling trustworthy clinicianai collaboration existing d datasets provide localization labels but none support this grounded reasoning ability to address this gap we introduce dreasonknee the first d grounded reasoning dataset for medical images which provides k highquality quintuples derived from d knee mri volumes each quintuple includes the d mri volume a diagnostic question targeting a specific anatomical region a d bounding box localizing the relevant anatomical structures cliniciangenerated diagnostic reasoning steps that explicitly detail the d reasoning process and structured severity assessments for the relevant anatomical region the creation and validation of dreasonknee involving over hours of expert clinician time for manually segmenting mris and generating reasoning chains ensures its superior quality and clinical relevance we establish reasonkneebench to evaluate localization and diagnostic accuracy providing insight into vlm ability to perform grounding and severity assessment across anatomical regions and diagnostic inquiries we benchmark five stateoftheart vlms providing baseline performance for reasonkneebench by providing this unique resource of expertannotated d reasoning pathways dreasonknee serves as a repository of orthopedic surgeons diagnostic expertise and offers a vital testbed for advancing multimodal medical ai systems towards d clinically aligned localized decisionmaking capabilities the dataset can be found in
reverse engineering re of x binaries is indispensable for malware and firmware analysis but remains slow due to stripped metadata and adversarial obfuscation large language models llms offer potential for improving re efficiency through automated comprehension and commenting but cloudhosted closedweight models pose privacy and security risks and cannot be used in closednetwork facilities we evaluate parameterefficient finetuned local llms for assisting with x re tasks in these settings eight openweight models across the codellama qwencoder and codegemma series are finetuned on a custom curated dataset of x assembly examples we evaluate them quantitatively and identify the finetuned qwencoderb as the top performer which we name rex rex reduces testset crossentropy loss by and improves semantic cosine ilarity against ground truth by over its base model in a limited user case study n rex significantly enhanced linelevel code understanding p and increased the correctsolve rate from to p though the latter did not reach statistical significance qualitative analysis shows more accurate concise comments with fewer hallucinations rex delivers stateoftheart assistance in x re among local openweight llms our findings demonstrate the value of domainspecific finetuning and highlight the need for more commented disassembly data to further enhance llm performance in re rex its dataset and lora adapters are publicly available at and
ondevice neural network training faces critical memory constraints that limit the adaptation of pretrained models to downstream tasks we present medyate a theoreticallygrounded framework for memoryconstrained dynamic subnetwork adaptation our approach introduces two key innovations lara layer ranking an improved layer importance metric that enables principled layer preselection and a dynamic channel sampling strategy that exploits the temporal stability of channel importance distributions during finetuning medyate dynamically resamples channels between epochs according to importanceweighted probabilities ensuring comprehensive parameter space exploration while respecting strict memory budgets extensive evaluation across a large panel of tasks and architectures demonstrates that medyate achieves stateoftheart performance under extreme memory constraints consistently outperforming existing static and dynamic approaches while maintaining high computational efficiency our method represents a significant step towards enabling efficient ondevice learning by demonstrating effective finetuning with memory budgets as low as a few hundred kb of ram
large language models llms have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference posttraining quantization ptq can effectively reduce these demands by storing weights in lower bitwidth formats however standard uniform quantization often leads to notable performance degradation particularly in lowbit scenarios in this work we introduce a grouped lattice vector quantization glvq framework that assigns each group of weights a customized lattice codebook defined by a learnable generation matrix to address the nondifferentiability of the quantization process we adopt babai rounding to imate nearestlatticepoint search during training which enables stable optimization of the generation matrices once trained decoding reduces to a ple matrixvector multiplication yielding an efficient and practical quantization pipeline experiments on multiple benchmarks show that our approach achieves a better tradeoff between model size and accuracy compared to existing posttraining quantization baselines highlighting its effectiveness in deploying large models under stringent resource constraints our source code is available on github repository
in response to the increasingly critical demand for accurate prediction of gpu memory resources in deep learning tasks this paper deeply analyzes the current research status and innovatively proposes a deep learning model that integrates bidirectional gated recurrent units bigru to optimize the transformer architecture aiming to improve the accuracy of memory demand prediction to verify the effectiveness of the model a carefully designed comparative experiment was conducted selecting four representative basic machine learning models decision tree random forest adaboost and xgboost as benchmarks the detailed experimental results show that the bigru transformer optimization model proposed in this paper exhibits significant advantages in key evaluation indicators in terms of mean square error mse and root mean square error rmse the model achieves the lowest value among all comparison models and its predicted results have the smallest deviation from the actual values in terms of mean absolute error mae and coefficient of determination r indicators the model also performs well and the results are balanced and stable with comprehensive predictive performance far exceeding the benchmark machine learning methods compared in summary the transformer model based on bidirectional gated recurrent unit optimization successfully constructed in this study can efficiently and accurately complete gpu memory demand prediction tasks in deep learning tasks and its prediction accuracy has been significantly improved compared to traditional machine learning methods this research provides strong technical support and reliable theoretical basis for optimizing resource scheduling and management of deep learning tasks and improving the utilization efficiency of computing clusters
foundation models have advanced computer vision by enabling strong performance across diverse tasks through largescale pretraining and supervised finetuning however they may underperform in domains with distribution shifts and scarce labels where supervised finetuning may be infeasible while continued selfsupervised learning for model adaptation is common for generative language models this strategy has not proven effective for visioncentric encoder models to address this challenge we introduce a novel formulation of selfsupervised finetuning for vision foundation models where the model is adapted to a new domain without requiring annotations leveraging only short multiview objectcentric videos our method is referred to as vessa videobased objectcentric selfsupervised adaptation for visual foundation models vessas training technique is based on a selfdistillation paradigm where it is critical to carefully tune prediction heads and deploy parameterefficient adaptation techniques otherwise the model may quickly forget its pretrained knowledge and reach a degraded state vessa benefits significantly from multiview object observations sourced from different frames in an objectcentric video efficiently learning robustness to varied capture conditions without the need of annotations through comprehensive experiments with vision foundation models on datasets vessa demonstrates consistent improvements in downstream classification tasks compared to the base models and previous adaptation methods code is publicly available at
we present a general framework for training spiking neural networks snns to perform binary classification on multivariate time series with a focus on stepwise prediction and high precision at low false alarm rates the approach uses the evolutionary optimization of neuromorphic systems eons algorithm to evolve sparse stateful snns by jointly optimizing their architectures and parameters inputs are encoded into spike trains and predictions are made by thresholding a single output neurons spike counts we also incorporate ple voting ensemble methods to improve performance and robustness to evaluate the framework we apply it with applicationspecific optimizations to the task of detecting low signaltonoise ratio radioactive sources in gammaray spectral data the resulting snns with as few as neurons and synapses achieve a true positive rate tpr at a false alarm rate of hr outperforming pca and deep learning baselines a threemodel anyvote ensemble increases tpr to at the same false alarm rate hardware deployment on the microcaspian neuromorphic platform demonstrates mw power consumption and ms inference latency we also demonstrate generalizability by applying the same framework without domainspecific modification to seizure detection in eeg recordings an ensemble achieves tpr with a false positive rate comparable to recent deep learning approaches with significant reduction in parameter count
generative ai tools are increasingly used to create portrayals of people in occupations raising concerns about how race and gender are represented we conducted a largescale audit of over million occupational personas across us occupations generated by four large language models with different ai safety commitments and countries of origin us china france compared with bureau of labor statistics data we find two recurring patterns systematic shifts where some groups are consistently under or overrepresented and stereotype exaggeration where existing demographic skews are amplified on average white pp and black pp workers are underrepresented while hispanic pp and asian pp workers are overrepresented these distortions can be extreme for example across all four models housekeepers are portrayed as nearly hispanic while black workers are erased from many occupations for hci these findings show provider choice materially changes who is visible motivating modelspecific audits and accountable design practices
accurate longterm forecasting of spatiotemporal dynamics remains a fundamental challenge across scientific and engineering domains existing machine learning methods often neglect governing physical laws and fail to quantify inherent uncertainties in spatiotemporal predictions to address these challenges we introduce a physicsconsistent neural operator pcno that enforces physical constraints by projecting surrogate model outputs onto function spaces satisfying predefined laws a physicsconsistent projection layer within pcno efficiently computes mass and momentum conservation in fourier space building upon deterministic predictions we further propose a diffusion modelenhanced pcno diffpcno which leverages a consistency model to quantify and mitigate uncertainties thereby improving the accuracy and reliability of forecasts pcno and diffpcno achieve highfidelity spatiotemporal predictions while preserving physical consistency and uncertainty across diverse systems and spatial resolutions ranging from turbulent flow modeling to realworld floodatmospheric forecasting our twostage framework provides a robust and versatile approach for accurate physically grounded and uncertaintyaware spatiotemporal forecasting
the integration of machine learning ml systems into critical industries such as healthcare finance and cybersecurity has transformed decisionmaking processes but it also brings new challenges around trust security and accountability as ai systems become more ubiquitous ensuring the transparency and correctness of aidriven decisions is crucial especially when they have direct consequences on privacy security or fairness verifiable ai powered by zeroknowledge machine learning zkml offers a robust solution to these challenges zkml enables the verification of ai model inferences without exposing sensitive data providing an essential layer of trust and privacy however traditional zkml systems typically require deep cryptographic expertise placing them beyond the reach of most ml engineers in this paper we introduce jstprove a specialized zkml toolkit built on polyhedra networks expander backend to enable ai developers and ml engineers to generate and verify proofs of ai inference jstprove provides an endtoend verifiable ai inference pipeline that hides cryptographic complexity behind a ple commandline interface while exposing auditable artifacts for reproducibility we present the design innovations and realworld use cases of jstprove as well as our blueprints and tooling to encourage community review and extension jstprove therefore serves both as a usable zkml product for current engineering needs and as a reproducible foundation for future research and production deployments of verifiable ai
the emergence of foundation models fms has enabled the development of highly capable and autonomous agents unlocking new application opportunities across a wide range of domains evaluating the architecture of agents is particularly important as the architectural decisions significantly impact the quality attributes of agents given their unique characteristics including compound architecture autonomous and nondeterministic behaviour and continuous evolution however these traditional methods fall short in addressing the evaluation needs of agent architecture due to the unique characteristics of these agents therefore in this paper we present agentarceval a novel agent architecture evaluation method designed specially to address the complexities of fmbased agent architecture and its evaluation moreover we present a catalogue of agentspecific general scenarios which serves as a guide for generating concrete scenarios to design and evaluate the agent architecture we demonstrate the usefulness of agentarceval and the catalogue through a case study on the architecture evaluation of a realworld tax copilot named luna
policy optimization po is a cornerstone of modern reinforcement learning rl with diverse applications spanning robotics healthcare and large language model training the increasing deployment of po in sensitive domains however raises significant privacy concerns in this paper we initiate a theoretical study of differentially private policy optimization focusing explicitly on its sample complexity we first formalize an appropriate definition of differential privacy dp tailored to po addressing the inherent challenges arising from onpolicy learning dynamics and the subtlety involved in defining the unit of privacy we then systematically analyze the sample complexity of widelyused po algorithms including policy gradient pg natural policy gradient npg and more under dp constraints and various settings via a unified framework our theoretical results demonstrate that privacy costs can often manifest as lowerorder terms in the sample complexity while also highlighting subtle yet important observations in private po settings these offer valuable practical insights for privacypreserving po algorithms
timely assessment of integrity of structures after seismic events is crucial for public safety and emergency response this study focuses on assessing the structural damage conditions using deep learning methods to detect exposed steel reinforcement in concrete buildings and bridges after large earthquakes steel bars are typically exposed after concrete spalling or large flexural or shear cracks the amount and distribution of exposed steel reinforcement is an indication of structural damage and degradation to automatically detect exposed steel bars new datasets of images collected after the turkey earthquakes were labeled to represent a wide variety of damaged concrete structures the proposed method builds upon a deep learning framework enhanced with finetuning data augmentation and testing on public datasets an automated classification framework is developed that can be used to identify insideoutside buildings and structural components then a yolov you only look once model is trained to detect cracking and spalling damage and exposed bars another yolo model is finetuned to distinguish different categories of structural damage levels all these trained models are used to create a hybrid framework to automatically and reliably determine the damage levels from input images this research demonstrates that rapid and automated damage detection following disasters is achievable across diverse damage contexts by utilizing image data collection annotation and deep learning approaches
applying complex legal rules characterized by multiple heterogeneously weighted criteria presents a fundamental challenge in judicial decisionmaking often hindering the consistent realization of legislative intent this challenge is particularly evident in the quantification of nonpecuniary damages in personal injury cases this paper introduces soppia a structured prompting framework designed to assist legal professionals in navigating this complexity by leveraging advanced ai the system ensures a comprehensive and balanced analysis of all stipulated criteria fulfilling the legislators intent that compensation be determined through a holistic assessment of each case using the twelve criteria for nonpecuniary damages established in the brazilian clt art g as a case study we demonstrate how soppia system for ordered proportional and pondered intelligent assessment operationalizes nuanced legal commands into a practical replicable and transparent methodology the framework enhances consistency and predictability while providing a versatile and explainable tool adaptable across multicriteria legal contexts bridging normative interpretation and computational reasoning toward auditable legal ai
molecular property prediction mpp is a cornerstone of drug discovery and materials science yet conventional deep learning approaches depend on large labeled datasets that are often unavailable fewshot molecular property prediction fsmpp addresses this scarcity by incorporating relational inductive bias through a context graph that links molecule nodes to property nodes but such moleculeproperty graphs offer limited structural guidance we propose a comprehensive solution motif driven globallocal context graph for fewshot molecular property prediction which enriches contextual information at both the global and local levels at the global level chemically meaningful motif nodes representing shared substructures such as rings or functional groups are introduced to form a global tripartite heterogeneous graph yielding motifmoleculeproperty connections that capture longrange compositional patterns and enable knowledge transfer among molecules with common motifs at the local level we build a subgraph for each node in the moleculeproperty pair and encode them separately to concentrate the models attention on the most informative neighboring molecules and motifs experiments on five standard fsmpp benchmarks demonstrate that our framework consistently outperforms stateoftheart methods these results underscore the effectiveness of integrating global motif knowledge with finegrained local context to advance robust fewshot molecular property prediction
in partially observable markov decision processes pomdps maintaining and updating belief distributions over possible underlying states provides a principled way to summarize actionobservation history for effective decisionmaking under uncertainty as environments grow more realistic belief distributions develop complexity that standard mathematical models cannot accurately capture creating a fundamental challenge in maintaining representational accuracy despite advances in deep learning and probabilistic modeling existing pomdp belief imation methods fail to accurately represent complex uncertainty structures such as highdimensional multimodal belief distributions resulting in estimation errors that lead to suboptimal agent behaviors to address this challenge we present escort efficient steinvariational and sliced consistencyoptimized representation for temporal beliefs a particlebased framework for capturing complex multimodal distributions in highdimensional belief spaces escort extends svgd with two key innovations correlationaware projections that model dependencies between state dimensions and temporal consistency constraints that stabilize updates while preserving correlation structures this approach retains svgds attractiverepulsive particle dynamics while enabling accurate modeling of intricate correlation patterns unlike particle filters prone to degeneracy or parametric methods with fixed representational capacity escort dynamically adapts to belief landscape complexity without resampling or restrictive distributional assumptions we demonstrate escorts effectiveness through extensive evaluations on both pomdp domains and synthetic multimodal distributions of varying dimensionality where it consistently outperforms stateoftheart methods in terms of belief imation accuracy and downstream decision quality
highdefinition d city maps underpin smart transportation digital twins and autonomous driving where object level change detection across bi temporal lidar enables hd map maintenance construction monitoring and reliable localization classical dsm differencing and image based methods are sensitive to small vertical bias ground slope and viewpoint mismatch and yield cellwise outputs without object identity point based neural models and voxel encodings demand large memory assume near perfect pre alignment degrade thin structures and seldom enforce class consistent association which leaves split or merge cases unresolved and ignores uncertainty we propose an object centric uncertainty aware pipeline for city scale lidar that aligns epochs with multi resolution ndt followed by point to plane icp normalizes height and derives a per location level of detection from registration covariance and surface roughness to calibrate decisions and suppress spurious changes geometry only proxies seed cross epoch associations that are refined by semantic and instance segmentation and a class constrained bipartite assignment with augmented dummies to handle splits and merges while preserving per class counts tiled processing bounds memory without eroding narrow ground changes and instance level decisions combine d overlap normal direction displacement and height and volume differences with a histogram distance all gated by the local level of detection to remain stable under partial overlap and sampling variation on representative subiaco blocks the method attains accuracy mf and miou exceeding triplet kpconv by percentage points in accuracy in mf and in miou with the largest gain on decreased where iou reaches and improves by points
we present generalizable hierarchical skill learning gsl a novel framework for hierarchical policy learning that significantly improves policy generalization and sample efficiency in robot manipulation one core idea of gsl is to use objectcentric skills as an interface that bridges the highlevel visionlanguage model and the lowlevel visualmotor policy specifically gsl decomposes demonstrations into transferable and objectcanonicalized skill primitives using foundation models ensuring efficient lowlevel skill learning in the object frame at test time the skillobject pairs predicted by the highlevel agent are fed to the lowlevel module where the inferred canonical actions are mapped back to the world frame for execution this structured yet flexible design leads to substantial improvements in sample efficiency and generalization of our method across unseen spatial arrangements object appearances and task compositions in ulation gsl trained with only demonstrations per task outperforms baselines trained with times more data by percent on unseen tasks in realworld experiments gsl also surpasses the baseline trained with times more data
despite rapid advancements in sensor networks conventional batterypowered sensor networks suffer from limited operational lifespans and frequent maintenance requirements that severely constrain their deployment in remote and inaccessible environments as such wireless rechargeable sensor networks wrsns with mobile charging capabilities offer a promising solution to extend network lifetime however wrsns face critical challenges from the inherent tradeoff between maximizing the node survival rates and maximizing charging energy efficiency under dynamic operational conditions in this paper we investigate a typical scenario where mobile chargers move and charge the sensor thereby maintaining the network connectivity while minimizing the energy waste specifically we formulate a multiobjective optimization problem that ultaneously maximizes the network node survival rate and mobile charger energy usage efficiency across multiple time slots which presents nphard computational complexity with longterm temporal dependencies that make traditional optimization approaches ineffective to address these challenges we propose an enhanced evolutionary multiobjective deep reinforcement learning algorithm which integrates a long shortterm memory lstmbased policy network for temporal pattern recognition a multilayer perceptronbased prospective increment model for future state prediction and a timevarying pareto policy evaluation method for dynamic preference adaptation extensive ulation results demonstrate that the proposed algorithm significantly outperforms existing approaches in balancing node survival rate and energy efficiency while generating diverse paretooptimal solutions moreover the lstmenhanced policy network converges faster than conventional networks with the timevarying evaluation method effectively adapting to dynamic conditions
frontier large language models llms pose unprecedented dualuse risks through the potential proliferation of chemical biological radiological and nuclear cbrn weapons knowledge we present the first comprehensive evaluation of leading commercial llms against both a novel prompt cbrn dataset and a prompt subset of the fortress benchmark using a rigorous threetier attack methodology our findings expose critical safety vulnerabilities deep inception attacks achieve success versus for direct requests demonstrating superficial filtering mechanisms model safety performance varies dramatically from claudeopus to mistralsmalllatest attack success rates and eight models exceed vulnerability when asked to enhance dangerous material properties we identify fundamental brittleness in current safety alignment where ple prompt engineering techniques bypass safeguards for dangerous cbrn information these results challenge industry safety claims and highlight urgent needs for standardized evaluation frameworks transparent safety metrics and more robust alignment techniques to mitigate catastrophic misuse risks while preserving beneficial capabilities
we present a multiagent aidriven framework for fundamental investing that integrates macro indicators industrylevel and firmspecific information to construct optimized equity portfolios the architecture comprises i a macro agent that dynamically screens and weights sectors based on evolving economic indicators and industry performance ii four firmlevel agents fundamental technical report and news that conduct indepth analyses of individual firms to ensure both breadth and depth of coverage iii a portfolio agent that uses reinforcement learning to combine the agent outputs into a unified policy to generate the trading strategy and iv a risk control agent that adjusts portfolio positions in response to market volatility we evaluate the system on the constituents by the csi index of chinas ashare market and find that it consistently outperforms standard benchmarks and a stateoftheart multiagent trading system on riskadjusted returns and drawdown control our core contribution is a hierarchical multiagent design that links topdown macro screening with bottomup fundamental analysis offering a robust and extensible approach to factorbased portfolio construction
designing de novo d molecules with desirable properties remains a fundamental challenge in drug discovery and molecular engineering while diffusion models have demonstrated remarkable capabilities in generating highquality d molecular structures they often struggle to effectively control complex multiobjective constraints critical for realworld applications in this study we propose an uncertaintyaware reinforcement learning rl framework to guide the optimization of d molecular diffusion models toward multiple property objectives while enhancing the overall quality of the generated molecules our method leverages surrogate models with predictive uncertainty estimation to dynamically shape reward functions facilitating balance across multiple optimization objectives we comprehensively evaluate our framework across three benchmark datasets and multiple diffusion model architectures consistently outperforming baselines for molecular quality and property optimization additionally molecular dynamics md ulations and admet profiling of top generated candidates indicate promising druglike behavior and binding stability comparable to known epidermal growth factor receptor egfr inhibitors our results demonstrate the strong potential of rlguided generative diffusion models for advancing automated molecular design
split federated learning sfl enables scalable training on edge devices by combining the parallelism of federated learning fl with the computational offloading of split learning sl despite its great success sfl suffers significantly from the wellknown straggler issue in distributed learning systems this problem is exacerbated by the dependency between split server and clients the split server side model update relies on receiving activations from clients such synchronization requirement introduces significant time latency making straggler a critical bottleneck to the scalability and efficiency of the system to mitigate this problem we propose musplitfed a stragglerresilient sfl algorithm in zerothorder optimization that decouples training progress from straggler delays via a ple yet effective unbalanced update mechanism by enabling the server to perform tau local updates per client round musplitfed achieves a convergence rate of osqrtdtau t for nonconvex objectives demonstrating a linear speedup of tau in communication rounds experiments demonstrate that musplitfed consistently outperforms baseline methods with the presence of stragglers and effectively mitigates their impact through adaptive tuning of tau the code for this project is available at
reinforcement learning rl has become a predominant technique to align language models lms with human preferences or promote outputs which are deemed to be desirable by a given reward function standard rl approaches optimize average reward while methods explicitly focused on reducing the probability of undesired outputs typically come at a cost to averagecase performance to improve this tradeoff we introduce repulse a new training method that augments the standard rl loss with an additional loss that uses learned proposals to guide sampling lowreward outputs and then reduces those outputs probability we run experiments demonstrating that repulse produces a better tradeoff of expected reward versus the probability of undesired outputs and is more adversarially robust compared to standard rl alignment approaches and alternatives
continual learning cl requires models to continuously adapt to new tasks without forgetting past knowledge in this work we propose underlineproactive underlinelowrank underlineallocatiounderlinen plan a framework that extends lowrank adaptation lora to enable efficient and interferenceaware finetuning of large pretrained models in cl settings plan proactively manages the allocation of taskspecific subspaces by introducing orthogonal basis vectors for each task and optimizing them through a perturbationbased strategy that minimizes conflicts with previously learned parameters furthermore plan incorporates a novel selection mechanism that identifies and assigns basis vectors with minimal sensitivity to interference reducing the risk of degrading past knowledge while maintaining efficient adaptation to new tasks empirical results on standard cl benchmarks demonstrate that plan consistently outperforms existing methods establishing a new stateoftheart for continual learning with foundation models
large language models llms have evolved into ai agents that interact with external tools and environments to perform complex tasks the model context protocol mcp has become the de facto standard for connecting agents with such resources but security has lagged behind thousands of mcp servers execute with unrestricted access to host systems creating a broad attack surface in this paper we introduce agentbound the first access control framework for mcp servers agentbound combines a declarative policy mechanism inspired by the android permission model with a policy enforcement engine that contains malicious behavior without requiring mcp server modifications we build a dataset containing the most popular mcp servers and show that access control policies can be generated automatically from source code with accuracy we also show that agentbound blocks the majority of security threats in several malicious mcp servers and that policy enforcement engine introduces negligible overhead our contributions provide developers and project managers with a practical foundation for securing mcp servers while maintaining productivity enabling researchers and tool builders to explore new directions for declarative access control and mcp security
as communication networks evolve towards greater complexity eg g and beyond a deep understanding of the wireless environment becomes increasingly crucial when explicit knowledge of the environment is unavailable geometryaware feature extraction from channel state information csi emerges as a pivotal methodology to bridge physicallayer measurements with network intelligence this paper proposes to explore the received signal strength rss data without explicit d environment knowledge to jointly construct the radio beam map and environmental geometry for a multipleinput multipleoutput mimo system unlike existing methods that only learn blockage structures we propose an oriented virtual obstacle model that captures the geometric features of both blockage and reflection reflective zones are formulated to identify relevant reflected paths according to the geometry relation of the environment we derive an analytical expression for the reflective zone and further analyze its geometric characteristics to develop a reformulation that is more compatible with deep learning representations a physicsinformed deep learning framework that incorporates the reflectivezonebased geometry model is proposed to learn the blockage reflection and scattering components along with the beam pattern which leverages physics prior knowledge to enhance network transferability numerical experiments demonstrate that in addition to reconstructing the blockage and reflection geometry the proposed model can construct a more accurate mimo beam map with a accuracy improvement
generative recommendation gr models tokenize each action into a few discrete tokens called semantic ids and autoregressively generate the next tokens as predictions showing advantages such as memory efficiency scalability and the potential to unify retrieval and ranking despite these benefits existing tokenization methods are static and nonpersonalized they typically derive semantic ids solely from item features assuming a universal item ilarity that overlooks userspecific perspectives however under the autoregressive paradigm semantic ids with the same prefixes always receive ilar probabilities so a single fixed mapping implicitly enforces a universal item ilarity standard across all users in practice the same item may be interpreted differently depending on user intentions and preferences to address this issue we propose a personalized contextaware tokenizer that incorporates a users historical interactions when generating semantic ids this design allows the same item to be tokenized into different semantic ids under different user contexts enabling gr models to capture multiple interpretive standards and produce more personalized predictions experiments on three public datasets demonstrate up to improvement in ndcg over nonpersonalized action tokenization baselines our code is available at
while recent sound event detection sed systems can identify baleen whale calls in marine audio challenges related to false positive and minorityclass detection persist we propose the boundary proposal network bpn which extends an existing lightweight sed system the bpn is inspired by work in image object detection and aims to reduce the number of false positive detections it achieves this by using intermediate latent representations computed within the backbone classification model to gate the final output when added to an existing sed system the bpn achieves a absolute increase in precision as well as and improvements in the fscore for minorityclass dcalls and bpcalls respectively we further consider two approaches to the selection of postprocessing hyperparameters a forwardsearch and a backwardsearch by separately optimising eventlevel and framelevel hyperparameters these two approaches lead to considerable performance improvements over parameters selected using empirical methods the complete whalevadbpn system achieves a crossvalidated development fscore of which is a absolute improvement over the baseline
the rapid scaling of large language models llms has made lowprecision training essential for reducing memory improving efficiency and enabling larger models and datasets existing convergence theories for adaptive optimizers however assume all components are exact and neglect hardwareaware quantization leaving open the question of why lowprecision training remains effective we introduce the first theoretical framework for analyzing the convergence of adaptive optimizers including adam and muon under floatingpoint quantization of gradients weights and optimizer states eg moment estimates within this framework we derive convergence rates on smooth nonconvex objectives under standard stochastic gradient assumptions explicitly characterizing how quantization errors from different components affect convergence we show that both algorithms retain rates close to their fullprecision counterparts provided mantissa length scales only logarithmically with the number of iterations our analysis further reveals that adam is highly sensitive to weights and secondmoment quantization due to its reliance on beta to while muon requires weaker error control and is thus potentially more robust these results narrow the gap between empirical success and theoretical understanding of lowprecision training methods numerical experiments on synthetic and realworld data corroborate our theory
biological circuits have evolved to incorporate multiple modules that perform ilar functions in the fly olfactory circuit both lateral inhibition li and neuronal spike frequency adaptation sfa are thought to enhance pattern separation for odor learning however it remains unclear whether these mechanisms play redundant or distinct roles in this process in this study we present a computational model of the fly olfactory circuit to investigate odor discrimination under varying noise conditions that ulate complex environments our results show that li primarily enhances odor discrimination in low and mediumnoise scenarios but this benefit diminishes and may reverse under highernoise conditions in contrast sfa consistently improves discrimination across all noise levels li is preferentially engaged in low and mediumnoise environments whereas sfa dominates in highnoise settings when combined these two sparsification mechanisms enable optimal discrimination performance this work demonstrates that seemingly redundant modules in biological circuits can in fact be essential for achieving optimal learning in complex contexts
as future superhuman models become increasingly complex accurately supervising their behavior may exceed human capabilities recent works have demonstrated that in such scenarios weak models can effectively supervise strong models a phenomenon known as weaktostrong generalization however we find that naive weaktostrong generalization fails under distribution shifts often leading to worse performance of the strong model than its weak supervisors to address this we propose raven a robust weaktostrong generalization framework that dynamically learns the optimal combinations of weak models in addition to parameters of the strong model we demonstrate the effectiveness of raven on image classification text classification and preference alignment tasks raven outperforms alternative baselines by over on outofdistribution tasks while matching or surpassing existing methods on indistribution tasks moreover our results show that raven assigns higher weights to more accurate weak models demonstrating its ability to automatically identify trustworthy supervision
recent advances in correlationbased sequential recommendation systems have demonstrated substantial success specifically the attentionbased model outperforms other rnnbased and markov chainsbased models by capturing both short and longterm dependencies more effectively however solely focusing on item cooccurrences overlooks the underlying motivations behind user behaviors leading to spurious correlations and potentially inaccurate recommendations to address this limitation we present a novel framework that integrates causal attention for sequential recommendation causalrec it incorporates a causal discovery block and a causalbooster the causal discovery block learns the causal graph in user behavior sequences and we provide a theory to guarantee the identifiability of the learned causal graph the causalbooster utilizes the discovered causal graph to refine the attention mechanism prioritizing behaviors with causal significance experimental evaluations on realworld datasets indicate that causalrec outperforms several stateoftheart methods with average improvements of in hit rate hr and in normalized discounted cumulative gain ndcg to the best of our knowledge this is the first model to incorporate causality through the attention mechanism in sequential recommendation demonstrating the value of causality in generating more accurate and reliable recommendations
recently foursquare released a global dataset with more than million points of interest pois each representing a realworld business on its platform however many entries lack complete metadata such as addresses or categories and some correspond to nonexistent or fictional locations in contrast openstreetmap osm offers a rich usercontributed poi dataset with detailed and frequently updated metadata though it does not formally verify whether a poi represents an actual business in this data paper we present a methodology that integrates the strengths of both datasets foursquare as a comprehensive baseline of commercial pois and osm as a source of enriched metadata the combined dataset totals imately tb while this full version is not publicly released we provide filtered releases with adjustable thresholds that reduce storage needs and make the data practical to download and use across domains we also provide stepbystep instructions to reproduce the full gb build record linkage is achieved by computing name ilarity scores and spatial distances between foursquare and osm pois these measures identify and retain highconfidence matches that correspond to real businesses in foursquare have representations in osm and show strong name ilarity finally we use this filtered dataset to construct a graphbased representation of pois enriched with attributes from both sources enabling advanced spatial analyses and a range of downstream applications
finetuning has proven to be highly effective in adapting pretrained models to perform better on new desired tasks with minimal data samples among the most widely used approaches are reparameterization methods which update a target module by augmenting its frozen weight matrix with an additional trainable weight matrix the most prominent example is low rank adaption lora which gained significant attention in recent years in this paper we introduce a new class of reparameterization methods for transfer learning designed to enhance the generalization ability of finetuned models we establish the effectiveness of our approach in a highdimensional binary classification setting using tools from random matrix theory and further validate our theoretical findings through more realistic experiments such as finetuning llms
in this work we explored the use of patient specific reinforced learning to generate d activity maps from two d planar images anterior and posterior the solution of this problem remains unachievable using conventional methodologies and is of particular interest for dosimetry in nuclear medicine where approaches for posttherapy distribution of radiopharmaceuticals such as lupsma are typically done via either expensive and long d spect acquisitions or fast yet only d planar scintigraphy being able to generate d activity maps from planar scintigraphy opens the gate for new dosimetry applications removing the need for spect and facilitating multitime point dosimetry studies our solution comprises the generation of a patient specific dataset with possible d uptake maps of the radiopharmaceuticals withing the anatomy of the individual followed by an ai approach we explored both the use of dunet and diffusion models able to generate d activity maps from d planar images we have validated our method both in ulation and real planar acquisitions we observed enhanced results using patient specific reinforcement learning reduction on mae and increase in ssim and better organ delineation and patient anatomy especially when combining diffusion models with patient specific training yielding a ssim compared to the ground truth for ulations and when compared to a spect acquisition performed half an hour after the planar we believe that our methodology can set a change of paradigm for nuclear medicine dosimetry allowing for d quantification using only planar scintigraphy without the need of expensive and timeconsuming spect leveraging the pretherapy information of the patients
conventional convolutional neural networks cnns in the real domain have been widely used for audio classification however their convolution operations process multichannel inputs independently limiting the ability to capture correlations among channels this can lead to suboptimal feature learning particularly for complex audio patterns such as multichannel spectrogram representations quaternion convolutional neural networks qcnns address this limitation by employing quaternion algebra to jointly capture interchannel dependencies enabling more compact models with fewer learnable parameters while better exploiting the multidimensional nature of audio signals however qcnns exhibit higher computational complexity due to the overhead of quaternion operations resulting in increased inference latency and reduced efficiency compared to conventional cnns posing challenges for deployment on resourceconstrained platforms to address this challenge this study explores knowledge distillation kd and pruning to reduce the computational complexity of qcnns while maintaining performance our experiments on audio classification reveal that pruning qcnns achieves ilar or superior performance compared to kd while requiring less computational effort compared to conventional cnns and transformerbased architectures pruned qcnns achieve competitive performance with a reduced learnable parameter count and computational complexity on the audioset dataset pruned qcnns reduce computational cost by and parameter count by while maintaining performance comparable to the conventional cnns furthermore pruned qcnns generalize well across multiple audio classification benchmarks including gtzan for music genre recognition esc for environmental sound classification and ravdess for speech emotion recognition
artificial intelligence ai systems increasingly match or surpass human experts in biomedical signal interpretation however their effective integration into clinical practice requires more than high predictive accuracy clinicians must discern when and why to trust algorithmic recommendations this work presents an applicationgrounded user study with eight professional sleep medicine practitioners who score nocturnal arousal events in polysomnographic data under three conditions i manual scoring ii blackbox bb ai assistance and iii transparent whitebox wb ai assistance assistance is provided either from the start of scoring or as a posthoc qualitycontrol qc review we systematically evaluate how the type and timing of assistance influence eventlevel and clinically most relevant countbased performance time requirements and user experience when evaluated against the clinical standard used to train the ai both ai and humanai teams significantly outperform unaided experts with collaboration also reducing interrater variability notably transparent ai assistance applied as a targeted qc step yields median eventlevel performance improvements of imately over blackbox assistance and qc timing further enhances countbased outcomes while wb and qc approaches increase the time required for scoring starttime assistance is faster and preferred by most participants participants overwhelmingly favor transparency with seven out of eight expressing willingness to adopt the system with minor or no modifications in summary strategically timed transparent ai assistance effectively balances accuracy and clinical efficiency providing a promising pathway toward trustworthy ai integration and user acceptance in clinical workflows
large language models llms are used for registertransfer level rtl code generation but they face two main challenges functional correctness and power performance and area ppa optimization iterative feedbackbased methods partially address these but they are limited to local search hindering the discovery of a global optimum this paper introduces revolution a framework that combines evolutionary computation ec with llms for automatic rtl generation and optimization revolution evolves a population of candidates in parallel each defined by a design strategy rtl implementation and evaluation feedback the framework includes a dualpopulation algorithm that divides candidates into fail and success groups for bug fixing and ppa optimization respectively an adaptive mechanism further improves search efficiency by dynamically adjusting the selection probability of each prompt strategy according to its success rate experiments on the verilogeval and rtllm benchmarks show that revolution increased the initial pass rate of various llms by up to percentage points the deepseekv model achieved a final pass rate of comparable to stateoftheart results without the need for separate training or domainspecific tools additionally the generated rtl designs showed significant ppa improvements over reference designs this work introduces a new rtl design approach by combining llms generative capabilities with ecs broad search power overcoming the localsearch limitations of previous methods
associative learningforming links between cooccurring itemsis fundamental to human cognition reshaping internal representations in complex ways testing hypotheses on how representational changes occur in biological systems is challenging but large language models llms offer a scalable alternative building on llms incontext learning we adapt a cognitive neuroscience associative learning paradigm and investigate how representations evolve across six models our initial findings reveal a nonmonotonic pattern consistent with the nonmonotonic plasticity hypothesis with moderately ilar items differentiating after learning leveraging the controllability of llms we further show that this differentiation is modulated by the overlap of associated items with the broader vocabularya factor we term vocabulary interference capturing how new associations compete with prior knowledge we find that higher vocabulary interference amplifies differentiation suggesting that representational change is influenced by both item ilarity and global competition our findings position llms not only as powerful tools for studying representational dynamics in humanlike learning systems but also as accessible and general computational models for generating new hypotheses about the principles underlying memory reorganization in the brain
we introduce dreamervxp an extension of dreamerv that improves exploration and learning efficiency this includes i a prioritized replay buffer scoring trajectories by return reconstruction loss and value error and ii an intrinsic reward based on disagreement over predicted environment rewards from an ensemble of world models dreamervxp is evaluated on a subset of atarik and deepmind control visual benchmark tasks confirming the original dreamerv results and showing that our extensions lead to faster learning and lower dynamics model loss particularly in sparsereward settings
transformerbased methods have achieved stateoftheart performance in time series forecasting tsf by capturing positional and semantic topological relationships among input tokens however it remains unclear whether existing transformers fully leverage the intrinsic topological structure among tokens throughout intermediate layers through empirical and theoretical analyses we identify that current transformer architectures progressively degrade the original positional and semantic topology of input tokens as the network deepens thus limiting forecasting accuracy furthermore our theoretical results demonstrate that explicitly enforcing preservation of these topological structures within intermediate layers can tighten generalization bounds leading to improved forecasting performance motivated by these insights we propose the topology enhancement method tem a novel transformerbased tsf method that explicitly and adaptively preserves tokenlevel topology tem consists of two core modules the positional topology enhancement module ptem which injects learnable positional constraints to explicitly retain original positional topology the semantic topology enhancement module stem which incorporates a learnable ilarity matrix to preserve original semantic topology to determine optimal injection weights adaptively tem employs a bilevel optimization strategy the proposed tem is a plugandplay method that can be integrated with existing transformerbased tsf methods extensive experiments demonstrate that integrating tem with a variety of existing methods significantly improves their predictive performance validating the effectiveness of explicitly preserving original tokenlevel topology our code is publicly available at href
lanechanging decisions which are crucial for autonomous vehicle path planning face practical challenges due to rulebased constraints and limited data deep reinforcement learning has become a major research focus due to its advantages in data acquisition and interpretability however current models often overlook collaboration which affects not only impacts overall traffic efficiency but also hinders the vehicles own normal driving in the long run to address the aforementioned issue this paper proposes a method named mix qlearning for lane changingmqlc that integrates a hybrid value q network taking into account both collective and individual benefits for the greater good at the collective level our method coordinates the individual q and global q networks by utilizing global information this enables agents to effectively balance their individual interests with the collective benefit at the individual level we integrated a deep learningbased intent recognition module into our observation and enhanced the decision network these changes provide agents with richer decision information and more accurate feature extraction for improved lanechanging decisions this strategy enables the multiagent system to learn and formulate optimal decisionmaking strategies effectively our mqlc model through extensive experimental results impressively outperforms other stateoftheart multiagent decisionmaking methods achieving significantly safer and faster lanechanging decisions the code is available at
in this paper we explore semantic clustering properties of deep reinforcement learning drl to improve its interpretability and deepen our understanding of its internal semantic organization in this context semantic clustering refers to the ability of neural networks to cluster inputs based on their semantic ilarity in the feature space we propose a drl architecture that incorporates a novel semantic clustering module that combines feature dimensionality reduction with online clustering this module integrates seamlessly into the drl training pipeline addressing the instability of tsne and eliminating the need for extensive manual annotation inherent to prior semantic analysis methods we experimentally validate the effectiveness of the proposed module and demonstrate its ability to reveal semantic clustering properties within drl furthermore we introduce new analytical methods based on these properties to provide insights into the hierarchical structure of policies and semantic organization within the feature space our code is available at
inference in both brains and machines can be formalized by optimizing a shared objective maximizing the evidence lower bound elbo in machine learning or minimizing variational free energy f in neuroscience elbo f while this equivalence suggests a unifying framework it leaves open how inference is implemented in neural systems here we introduce fond free energy online naturalgradient dynamics a framework that derives neural inference dynamics from three principles natural gradients on f online belief updating and iterative refinement we apply fond to derive ipvae iterative poisson variational autoencoder a recurrent spiking neural network that performs variational inference through membrane potential dynamics replacing amortized encoders with iterative inference updates theoretically ipvae yields several desirable features such as emergent normalization via lateral competition and hardwareefficient integer spike count representations empirically ipvae outperforms both standard vaes and gaussianbased predictive coding models in sparsity reconstruction and biological plausibility and scales to complex color image datasets such as celeba ipvae also exhibits strong generalization to outofdistribution inputs exceeding hybrid iterativeamortized vaes these results demonstrate how deriving inference algorithms from first principles can yield concrete architectures that are ultaneously biologically plausible and empirically effective
user prompts for generative ai models are often underspecified leading to a misalignment between the user intent and models understanding as a result users commonly have to painstakingly refine their prompts we study this alignment problem in texttoimage ti generation and propose a prototype for proactive ti agents equipped with an interface to actively ask clarification questions when uncertain and present their uncertainty about user intent as an understandable and editable belief graph we build ple prototypes for such agents and propose a new scalable and automated evaluation approach using two agents one with a ground truth intent an image while the other tries to ask as few questions as possible to align with the ground truth we experiment over three imagetext datasets imageinwords garg et al coco lin et al and designbench a benchmark we curated with strong artistic and design elements experiments over the three datasets demonstrate the proposed ti agents ability to ask informative questions and elicit crucial information to achieve successful alignment with at least times higher vqascore lin et al than the standard ti generation moreover we conducted human studies and observed that at least of human subjects found these agents and their belief graphs helpful for their ti workflow highlighting the effectiveness of our approach code and designbench can be found at
this paper develops an agentic framework that employs large language models llms for grounded persuasive language generation in automated copywriting with real estate marketing as a focal application our method is designed to align the generated content with user preferences while highlighting useful factual attributes this agent consists of three key modules grounding module mimicking expert human behavior to predict marketable features personalization module aligning content with user preferences marketing module ensuring factual accuracy and the inclusion of localized features we conduct systematic humansubject experiments in the domain of real estate marketing with a focus group of potential house buyers the results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin while maintaining the same level of factual accuracy our findings suggest a promising agentic approach to automate largescale targeted copywriting while ensuring factuality of content generation
recent advances demonstrate that increasing inferencetime computation can significantly boost the reasoning capabilities of large language models llms although repeated sampling ie generating multiple candidate outputs is a highly effective strategy it does not leverage external feedback signals for refinement which are often available in tasks like coding in this work we propose adaptive branching monte carlo tree search abmcts a novel inferencetime framework that generalizes repeated sampling with principled multiturn exploration and exploitation at each node in the search tree abmcts dynamically decides whether to go wider by expanding new candidate responses or go deeper by revisiting existing ones based on external feedback signals we evaluate our method on complex coding and engineering tasks using frontier models empirical results show that abmcts consistently outperforms both repeated sampling and standard mcts underscoring the importance of combining the response diversity of llms with multiturn solution refinement for effective inferencetime scaling code is available at
in recent years large language models llms have shown remarkable capabilities in various artificial intelligence problems however they fail to plan reliably even when prompted with a detailed definition of the planning task attempts to improve their planning capabilities such as chainofthought prompting finetuning and explicit reasoning still yield incorrect plans and usually fail to generalize to larger tasks in this paper we show how to use llms to generate correct plans even for outofdistribution tasks of increasing size for a given planning domain we ask an llm to generate several domaindependent heuristic functions in the form of python code evaluate them on a set of training tasks within a greedy bestfirst search and choose the strongest one the resulting llmgenerated heuristics solve many more unseen test tasks than stateoftheart domainindependent heuristics for classical planning they are even competitive with the strongest learning algorithm for domaindependent planning these findings are especially remarkable given that our proofofconcept implementation is based on an unoptimized python planner and the baselines all build upon highly optimized c code in some domains the llmgenerated heuristics expand fewer states than the baselines revealing that they are not only efficiently computable but sometimes even more informative than the stateoftheart heuristics overall our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of llms
reward shaping in multiagent reinforcement learning marl for complex tasks remains a significant challenge existing approaches often fail to find optimal solutions or cannot efficiently handle such tasks we propose hyprl a specificationguided reinforcement learning framework that learns control policies wrt hyperproperties expressed in hyperltl hyperproperties constitute a powerful formalism for specifying objectives and constraints over sets of execution traces across agents to learn policies that maximize the satisfaction of a hyperltl formula phi we apply skolemization to manage quantifier alternations and define quantitative robustness functions to shape rewards over execution traces of a markov decision process with unknown transitions a suitable rl algorithm is then used to learn policies that collectively maximize the expected reward and consequently increase the probability of satisfying phi we evaluate hyprl on a diverse set of benchmarks including safetyaware planning deep sea treasure and the post correspondence problem we also compare with specificationdriven baselines to demonstrate the effectiveness and efficiency of hyprl
a generalizable reward model is crucial in reinforcement learning from human feedback rlhf as it enables correctly evaluating unseen promptresponse pairs however existing reward models lack this ability as they are typically trained by increasing the reward gap between chosen and rejected responses while overlooking the prompts that the responses are conditioned on consequently when the trained reward model is evaluated on promptresponse pairs that lie outside the data distribution neglecting the effect of prompts may result in poor generalization of the reward model to address this issue we decompose the reward value into two independent components promptfree reward and promptrelated reward promptfree reward represents the evaluation that is determined only by responses while the promptrelated reward reflects the reward that derives from both the prompt and the response we extract these two components from an informationtheoretic perspective which requires no extra models subsequently we propose a new reward learning algorithm by prioritizing data samples based on their promptfree reward values through toy examples we demonstrate that the extracted promptfree and promptrelated rewards effectively characterize two parts of the reward model further standard evaluations show that our method improves both the alignment performance and the generalization capability of the reward model
we introduce mlrcbench a benchmark designed to quantify how effectively language agents can tackle challenging machine learning ml research competitions with a focus on open research problems that demand novel methodologies unlike prior work eg ai scientist which evaluates the endtoend agentic pipeline by using llmasajudge mlrcbench measures the key steps of proposing and implementing novel research methods and evaluates them with rigorous protocol and objective metrics our curated suite of competition tasks reveals significant challenges for llm agents even the bestperforming tested agent geminiexp under mlab closes only of the gap between baseline and top human participant scores furthermore our analysis reveals a misalignment between the llmjudged innovation and actual performance on cuttingedge ml research problems mlrcbench is a dynamic benchmark designed to grow with new ml competitions and encourage rigorous objective evaluations of ai research capabilities our leaderboard and code are available at
formal reasoning and automated theorem proving constitute a challenging subfield of machine learning in which machines are tasked with proving mathematical theorems using formal languages like lean a formal verification system can check whether a formal proof is correct or not almost instantaneously but generating a completely correct formal proof with large language models llms remains a formidable task the usual approach in the literature is to prompt the llm many times up to several thousands until one of the generated proofs passes the verification system in this work we present apollo automated proof repair viallm and lean collaboration a modular modelagnostic agentic framework that combines the strengths of the lean compiler with an llms reasoning abilities to achieve better proofgeneration results at a low token and sampling budgets apollo directs a fully automated process in which the llm generates proofs for theorems a set of agents analyze the proofs fix the syntax errors identify the mistakes in the proofs using lean isolate failing sublemmas utilize automated solvers and invoke an llm on each remaining goal with a low topk budget the repaired subproofs are recombined and reverified iterating up to a usercontrolled maximum number of attempts on the miniff benchmark we establish a new stateoftheart accuracy of among sub bparameter models as of august while keeping the sampling budget below one hundred moreover apollo raises the stateoftheart accuracy for goedelproversft to while cutting sample complexity from to a few hundred generalpurpose models omini omini jump from to over accuracy our results demonstrate that targeted compilerguided repair of llm outputs yields dramatic gains in both efficiency and correctness suggesting a general paradigm for scalable automated theorem proving
we present causal head gating chg a scalable method for interpreting the functional roles of attention heads in transformer models chg learns soft gates over heads and assigns them a causal taxonomy facilitating interfering or irrelevant based on their impact on task performance unlike prior approaches in mechanistic interpretability which are hypothesisdriven and require prompt templates or target labels chg applies directly to any dataset using standard nexttoken prediction we evaluate chg across multiple large language models llms in the llama model family and diverse tasks including syntax commonsense and mathematical reasoning and show that chg scores yield causal not merely correlational insight validated via ablation and causal mediation analyses we also introduce contrastive chg a variant that isolates subcircuits for specific task components our findings reveal that llms contain multiple sparse tasksufficient subcircuits that individual head roles depend on interactions with others low modularity and that instruction following and incontext learning rely on separable mechanisms
large language models llms can sometimes report the strategies they actually use to solve tasks yet at other times seem unable to recognize those strategies that govern their behavior this suggests a limited degree of metacognition the capacity to monitor ones own cognitive processes for subsequent reporting and selfcontrol metacognition enhances llms capabilities in solving complex tasks but also raises safety concerns as models may obfuscate their internal processes to evade neuralactivationbased oversight eg safety detector given societys increased reliance on these models it is critical that we understand their metacognitive abilities to address this we introduce a neuroscienceinspired neurofeedback paradigm that uses incontext learning to quantify metacognitive abilities of llms to report and control their activation patterns we demonstrate that their abilities depend on several factors the number of incontext examples provided the semantic interpretability of the neural activation direction to be reportedcontrolled and the variance explained by that direction these directions span a metacognitive space with dimensionality much lower than the models neural space suggesting llms can monitor only a small subset of their neural activations our paradigm provides empirical evidence to quantify metacognition in llms with significant implications for ai safety eg adversarial attack and defense
large language models llms have demonstrated impressive reasoning capabilities in complex problemsolving tasks sparking growing interest in their application to preference reasoning in recommendation systems existing methods typically rely on finetuning with explicit chainofthought cot data however these methods face significant practical limitations due to the difficulty of obtaining highquality cot data in recommendation and the high inference latency caused by generating cot reasoning in this work we explore an alternative approach that shifts from explicit cot reasoning to compact informationdense latent reasoning this approach eliminates the need for explicit cot generation and improves inference efficiency as few latent tokens can effectively capture the entire reasoning process building on this idea we propose underlinereinforced underlinelatent underlinereasoning for underlinerecommendation latentr a novel endtoend training framework that leverages reinforcement learning rl to optimize latent reasoning without relying on any cot data latentr adopts a twostage training strategy first supervised finetuning to initialize the latent reasoning module followed by pure rl training to encourage exploration through a rulebased reward design our rl implementation is based on a modified grpo algorithm which reduces computational overhead during training and introduces continuous reward signals for more efficient learning extensive experiments demonstrate that latentr enables effective latent reasoning without any direct supervision of the reasoning process significantly improving performance when integrated with different llmbased recommendation methods our codes are available at
llmbased agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine robotics and programming however maintaining these systems requires substantial effort as they are inevitably prone to bugs and continually evolve to meet changing external requirements therefore automatically resolving agent issues ie bug reports or feature requests is a crucial and challenging task while recent software engineering se agents eg sweagent have shown promise in addressing issues in traditional software systems it remains unclear how effectively they can resolve realworld issues in agent systems which differ significantly from traditional software to fill this gap we first manually analyze realworld agent issues and identify common categories of agent issues we then spend personhours constructing agentissuebench a reproducible benchmark comprising agent issue resolution tasks each with an executable environment and failuretriggering tests we further evaluate stateoftheart se agents on agentissuebench and reveal their limited effectiveness ie with only resolution rates these results underscore the unique challenges of maintaining agent systems compared to traditional software highlighting the need for further research to develop advanced se agents for resolving agent issues data and code are available at
large language models llms have exhibited remarkable capabilities and achieved significant breakthroughs across various domains leading to their widespread adoption in recent years building on this progress we investigate their potential in the realm of local life services in this study we establish a comprehensive benchmark and systematically evaluate the performance of diverse llms across a wide range of tasks relevant to local life services to further enhance their effectiveness we explore two key approaches model finetuning and agentbased workflows our findings reveal that even a relatively compact b model can attain performance levels comparable to a much larger b model effectively balancing inference cost and model capability this optimization greatly enhances the feasibility and efficiency of deploying llms in realworld online services making them more practical and accessible for local life applications
large language models llms are increasingly explored for legal argument generation yet they pose significant risks of manipulation through hallucination and ungrounded persuasion and often fail to utilize provided factual bases effectively or abstain when arguments are untenable this paper introduces a novel reflective multiagent method designed to address these challenges in the context of legally compliant persuasion our approach employs specialized agents factor analyst and argument polisher in an iterative refinement process to generate ply legal arguments plaintiff defendant rebuttal we evaluate reflective multiagent against singleagent enhancedprompt singleagent and nonreflective multiagent baselines using four diverse llms gpto gptomini llamamaverickbe llamascoutbe across three legal scenarios arguable mismatched and nonarguable results demonstrate that the reflective multiagent approach excels at successful abstention by preventing generation when arguments cannot be grounded improves hallucination accuracy by reducing fabricated and misattributed factors and enhances factor utilization recall by better using the provided case facts these findings suggest that structured reflection within a multiagent framework offers a robust method for fostering ethical persuasion and mitigating manipulation in llmbased legal argumentation systems
reinforcement learning rl has become the dominant paradigm for improving the performance of language models on complex reasoning tasks despite the substantial empirical gains demonstrated by rlbased training methods like grpo a granular understanding of why and how rl enhances performance is still lacking to bridge this gap we introduce sparkle a finegrained analytic framework to dissect the effects of rl across three key dimensions plan following and execution knowledge integration and chain of subproblems using this framework we gain insights beyond mere accuracy for instance providing models with explicit humancrafted stepbystep plans can surprisingly degrade performance on the most challenging benchmarks yet rltuned models exhibit greater robustness experiencing markedly smaller performance drops than base or sft models this suggests that rl may not primarily enhance the execution of external plans but rather empower models to formulate and follow internal strategies better suited to their reasoning processes conversely we observe that rl enhances models ability to integrate provided knowledge into their reasoning process yielding consistent gains across diverse tasks finally we study whether difficult problems those yielding no rl signals and mixedquality reasoning traces can still be effectively used for training we introduce sparklerlpss a multistage rl pipeline that reuses hard problems with partial step scaffolding guiding exploration effectively without additional data generation together our findings provide a principled foundation for understanding how rl shapes model behavior offering practical insights for building more adaptive dataefficient and interpretable rl pipelines for reasoning tasks our code data and checkpoints are available at
large language models llms continue to exhibit vulnerabilities despite deliberate safety alignment efforts posing significant risks to users and society to safeguard against the risk of policyviolating content systemlevel moderation via external guard modelsdesigned to monitor llm inputs and outputs and block potentially harmful contenthas emerged as a prevalent mitigation strategy existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with outofdistribution threats such as emerging harmful categories or jailbreak attacks to address these limitations we propose rsafe an adaptive reasoningbased safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies rsafe operates in two stages guided reasoning where it analyzes safety risks of input content through policyguided stepbystep reasoning and reinforced alignment where rulebased rl optimizes its reasoning paths to align with accurate safety prediction this twostage training paradigm enables rsafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation scenarios during inference rsafe accepts userspecified safety policies to provide enhanced safeguards tailored to specific safety requirements
diffusion models have recently emerged as a powerful approach for trajectory planning however their inherently nonsequential nature limits their effectiveness in longhorizon reasoning tasks at test time the recently proposed monte carlo tree diffusion mctd offers a promising solution by combining diffusion with treebased search achieving stateoftheart performance on complex planning problems despite its strengths our analysis shows that mctd incurs substantial computational overhead due to the sequential nature of tree search and the cost of iterative denoising to address this we propose fastmctd a more efficient variant that preserves the strengths of mctd while significantly improving its speed and scalability fastmctd integrates two techniques parallel mctd which enables parallel rollouts via delayed tree updates and redundancyaware selection and sparse mctd which reduces rollout length through trajectory coarsening experiments show that fastmctd achieves up to x speedup over standard mctd while maintaining or improving planning performance remarkably it even outperforms diffuser in inference speed on some tasks despite diffuser requiring no search and yielding weaker solutions these results position fastmctd as a practical and scalable solution for diffusionbased inferencetime reasoning
a challenge in humanai decisionmaking is to balance three factors the correctness of predictions the cost of knowledge and reasoning complexity and the confidence about whether to abstain from automated answers or escalate to human experts in this work we present a cascaded llm decision framework that adaptively delegates tasks across multiple tiers of expertise a base model for initial candidate answers a more capable and knowledgeable but costlier large model and a human expert for when the model cascade abstains our method proceeds in two stages first a deferral policy determines whether to accept the base models answer or regenerate it with the large model based on the confidence score second an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention moreover to overcome static policies and accommodate changing task difficulty we incorporate an online learning mechanism which uses human feedback we demonstrate this approach to general questionanswering arceasy arcchallenge and mmlu and medical questionanswering medqa and medmcqa our results demonstrate that our cascaded strategy outperforms singlemodel baselines in most cases achieving higher accuracy while reducing costs and providing a principled approach to handling abstentions
llmbased web agents have recently made significant progress but much of it has occurred in closedsource systems widening the gap with opensource alternatives progress has been held back by two key challenges first a narrow focus on singlestep tasks that overlooks the complexity of multistep web interactions and second the high compute costs required to posttrain llmbased web agents to address this we present the first statistically grounded study on compute allocation for llm webagent posttraining our approach uses a twostage pipeline training a llama b student to imitate a llama b teacher via supervised finetuning sft followed by onpolicy reinforcement learning we find this process highly sensitive to hyperparameter choices making exhaustive sweeps impractical to spare others from expensive trialanderror we sample configurations and use bootstrapping to estimate effective hyperparameters our results show that combining sft with onpolicy rl consistently outperforms either approach alone on both workarena and miniwob further this strategy requires only of the compute to match the peak performance of pure sft on miniwob effectively pushing the computeperformance pareto frontier and is the only strategy that can close the gap with closedsource models
reasoning llms have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach to ensure llms do not assist with harmful requests safety alignment finetuning is necessary in the posttraining phase however safety alignment finetuning has recently been shown to significantly degrade reasoning abilities a phenomenon known as the safety tax in this work we show that using lora for sft on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities this is because restricting the safety weight updates to a lowrank space minimizes the interference with the reasoning weights our extensive experiments across four benchmarks covering math science and coding show that this approach produces highly safe llmswith safety levels comparable to fullmodel finetuningwithout compromising their reasoning abilities our ablation studies further identify three key factors in lora rank updates are sufficient to achieve the best reasoning and safety performance the up projection layers are the most critical modules with lora applied to them alone achieving even better results and middle layers are more effective than early or late layers together these findings show that strong safety and reasoning can be achieved at minimal computational cost when updates are applied in the right places additionally we observe that lora induces weight updates with smaller overlap with the initial weights compared to fullmodel finetuning finally while our attempts to further reduce this overlap yield only modest improvements on some tasks they highlight the potential of developing methods that more reliably optimize the reasoningsafety tradeoff
ai agents built on foundation models hold enormous promise current practice however focuses on a onetaskoneagent approach which not only falls short of scalability and generality but also faces practical limitations from blackbox autoregressive reasoning where decisions unfold token by token without explicit ulation or counterfactual evaluation of outcomes humans on the other hand reason and plan by mentally ulating the consequences of actions within an internal model of the world a capability that supports flexible goaldirected behavior across diverse contexts moving towards a more general and powerful ai agent we introduce ura a goaloriented architecture for generalized agentic reasoning based on a principled formulation of an optimal agent in any general environment ura addresses the limitations of blackbox autoregressive reasoning by incorporating the world model for planning via ulation our prototype world model is implemented using llms as a substrate leveraging the natural language as a discrete hierarchical representation grounded in concepts for planning while remaining modelagnostic on complex webbrowsing tasks such as flight search ura improves the success rate from to compared to a representative openweb agent baseline across tasks worldmodelbased planning achieves up to higher task completion rates than a matched blackbox autoregressive baseline demonstrating the advantages of ulative reasoning we release reasoneragentweb a webbrowsing agent built on ura as an opensource research demo
in the ongoing quest for hybridizing discrete reasoning with neural nets there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs a task that large language models seem to struggle with objectives we introduce a differentiable neurosymbolic architecture and a loss function dedicated to learning how to solve nphard reasoning problems methods our new probabilistic loss allows for learning both the constraints and the objective thus delivering a complete model that can be scrutinized and completed with side constraints by pushing the combinatorial solver out of the training loop our architecture also offers scalable training while exact inference gives access to maximum accuracy results we empirically show that it can efficiently learn how to solve nphard reasoning problems from natural inputs on three variants of the sudoku benchmark symbolic visual and manysolution our approach requires a fraction of training time of other hybrid methods on a visual mincutmaxcut task it optimizes the regret better than a decisionfocusedlearning regretdedicated loss finally it efficiently learns the energy optimization formulation of the large realworld problem of designing proteins
artificial intelligence ai systems and large language models llms in particular are increasingly employed for creative tasks like scientific idea generation constituting a form of generalization from training data unaddressed by existing conceptual frameworks despite its ilarities to compositional generalization cg combinatorial creativity cc is an openended ability instead of evaluating for accuracy or correctness against fixed targets which would contradict the openended nature of cc we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility from here we make several important empirical contributions we obtain the first insights into the scaling behavior of creativity for llms we discover that for fixed compute budgets there exist optimal model depths and widths for creative ability we find that the ideationexecution gap whereby llms excel at generating novel scientific ideas but struggle to ensure their practical feasibility may be explained by a more fundamental noveltyutility tradeoff characteristic of creativity algorithms in general importantly this tradeoff remains persistent even at scale casting doubt on the longterm creative potential of llms in their current form together our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern ai models bridging the gap between human and machine intelligence
recent advances in large language models llms and agent system designs have empowered agents with unprecedented levels of capability however existing agent benchmarks are showing a trend of rapid ceilinghitting by newly developed agents making it difficult to meet the demands for evaluating agent abilities to address this problem we propose the trajectorybased validatedbyreproducing agentbenchmark complexity evolution trace framework this framework takes an original task from an existing benchmark and encourages agents to freely explore and evolve it into a new task with higher difficulty while recording validatable agent trajectories the framework proceeds in three stages evolutionary proposal mining which provides task evolution proposals through preliminary exploration and divergent thinking problem formation and free exploration where proposals are conceptualized into feasible problem candidates and the agents then explore them freely while recording their execution trajectories and multilevel validation which ensures that the evolved tasks are accompanied by validatable and reproducible trajectories experiments on the gaia benchmark demonstrate that the trace framework consistently enhances task complexity while improving the reliability of correctness through validatable execution trajectories in addition our framework can successfully adapt to and improve reasoning datasets represented by aime this work marks a paradigm shift from static manually curated benchmarks to dynamic selfevolving evaluation systems providing a sustainable and challenging runway for agent development
ulating human reasoning in openended tasks has been a longstanding aspiration in ai and cognitive science while large language models now imate human responses at scale they remain tuned to populationlevel consensus often erasing the individuality of reasoning styles and belief trajectories to advance the vision of more humanlike reasoning in machines we introduce hugagent humangrounded agent benchmark a benchmark for averagetoindividual reasoning adaptation the task is to predict how a specific person would reason and update their beliefs in novel scenarios given partial evidence of their past views hugagent adopts a dualtrack design a synthetic track for scale and systematic stress tests and a human track for ecologically valid outloud reasoning data this design enables scalable reproducible evaluation of intraagent fidelity whether models can capture not just what people believe but how their reasoning evolves experiments with stateoftheart llms reveal persistent adaptation gaps positioning hugagent as the first extensible benchmark for aligning machine reasoning with the individuality of human thought our benchmark and chatbot are opensourced as hugagent and traceyourthinking
transformerbased text classifiers such as bert roberta t and gpt have shown strong performance in natural language processing tasks but remain vulnerable to adversarial examples these vulnerabilities raise significant security concerns as small input perturbations can cause severe misclassifications existing robustness methods often require heavy computation or lack interpretability this paper presents a unified framework called explainabilitydriven detection identification and transformation edit to strengthen inferencetime defenses edit integrates explainability tools including attention maps and integrated gradients with frequencybased features to automatically detect and identify adversarial perturbations while offering insight into model behavior after detection edit refines adversarial inputs using an optimal transformation process that leverages pretrained embeddings and model feedback to replace corrupted tokens to enhance security assurance edit incorporates automated alerting mechanisms that involve human analysts when necessary beyond static defenses edit also provides adaptive resilience by enforcing internal feature ilarity and transforming inputs thereby disrupting the attackers optimization process and limiting the effectiveness of adaptive adversarial attacks experiments using bert and roberta on imdb yelp agnews and sst datasets against seven word substitution attacks demonstrate that edit achieves an average fscore of percent and balanced accuracy of percent compared to four stateoftheart defenses edit improves balanced accuracy by times and fscore by times while being times faster in feature extraction the framework provides robust interpretable and efficient protection against both standard zeroday and adaptive adversarial threats in text classification models
aligning robot behavior with human preferences is crucial for deploying embodied ai agents in humancentered environments a promising solution is interactive imitation learning from human intervention where a human expert observes the policys execution and provides interventions as feedback however existing methods often fail to utilize the prior policy efficiently to facilitate learning thus hindering sample efficiency in this work we introduce mereq maximumentropy residualq inverse reinforcement learning designed for sampleefficient alignment from human intervention instead of inferring the complete human behavior characteristics mereq infers a residual reward function that captures the discrepancy between the human experts and the prior policys underlying reward functions it then employs residual qlearning rql to align the policy with human preferences using this residual reward function extensive evaluations on ulated and realworld tasks demonstrate that mereq achieves sampleefficient policy alignment from human intervention
time series forecasting tsf possesses great practical values in various fields including power and energy transportation etc tsf methods have been studied based on knowledge from classical statistics to modern deep learning yet all of them were developed based on one fundamental concept the numerical data fitting thus the models developed have long been known to be problemspecific and lacking application generalizability practitioners expect a tsf foundation model that serves tsf tasks in different applications the central question is then how to develop such a tsf foundation model this paper offers one pioneering study in the tsf foundation model development method and proposes a vision intelligencepowered framework vitime for the first time vitime fundamentally shifts tsf from numerical fitting to operations based on a binary imagebased time series metric space and naturally supports both point and probabilistic forecasting we also provide rigorous theoretical analyses of vitime including quantizationinduced system error bounds and principled strategies for optimal parameter selection furthermore we propose realts an innovative synthesis algorithm generating diverse and realistic training samples effectively enriching the training data and significantly enhancing model generalizability extensive experiments demonstrate vitimes stateoftheart performance in zeroshot scenarios vitime outperforms timesfm by with just finetuning data vitime surpasses both leading foundation models and fullysupervised benchmarks a gap that widens with finetuning vitime also exhibits exceptional robustness effectively handling missing data and outperforming timesfm by under various data perturbations validating the power of its visual space data operation paradigm
for textbased ai systems to interact in the real world causal reasoning is an essential skill since active interventions are costly we study to what extent a system can learn causal reasoning from symbolic demonstrations of causal axioms specifically we present an axiomatic training method where the system learns from multiple demonstrations of a causal axiom or rule rather than incorporating the axiom as an inductive bias or inferring it from data values a key question is whether the system would learn to generalize from the axiom demonstrations to more complex scenarios our results based on applying axiomatic training to learn the transitivity axiom and dseparation rule indicate that such generalization is possible to avoid data contamination issues we start with a million parameter transformer model and train it from scratch on both tasks we find that a model trained on linear causal chains along with some noisy variations can generalize well to complex graphs including longer causal chains causal chains with reversed order and graphs with handle diverse text inputs the same method is extended to finetune language models finetuning llamabinstruct model on our axiomatic data leads to significant gains on causal benchmarks such as corrcause and clear in some cases providing stateoftheart performance surpassing gpt
deep learning has achieved remarkable accuracy in medical image segmentation particularly for larger structures with welldefined boundaries however its effectiveness can be challenged by factors such as irregular object shapes and edges nonsmooth surfaces small target areas etc which complicate the ability of networks to grasp the intricate and diverse nature of anatomical regions in response to these challenges we propose an adaptive focal loss afl that takes both object boundary smoothness and size into account with the goal to improve segmentation performance in intricate anatomical regions the proposed afl dynamically adjusts itself based on an objects surface smoothness size and the class balancing parameter based on the ratio of targeted area and background we evaluated the performance of the afl on the picai and brats datasets in the picai dataset the afl achieved an intersection over union iou score of and a dice ilarity coefficient dsc of outperforming the regular focal loss fl by and respectively it also surpassed the best baseline by and in the brats dataset afl achieved an iou score of and a dsc score of our ablation experiments also show that the proposed afl surpasses conventional losses this includes dice loss focal loss and their hybrid variants by large margin in iou dsc and other metrics the code is available at
large language models llms excel in coderelated tasks like code generation but benchmark evaluations often overlook task characteristics such as difficulty moreover benchmarks are
we investigate the mechanisms underlying a range of listprocessing tasks in llms and we find that llms have learned to encode a compact causal representation of a general filtering operation that mirrors the generic filter function of functional programming using causal mediation analysis on a diverse set of listprocessing tasks we find that a small number of attention heads which we dub filter heads encode a compact representation of the filtering predicate in their query states at certain tokens we demonstrate that this predicate representation is general and portable it can be extracted and reapplied to execute the same filtering operation on different collections presented in different formats languages or even in tasks however we also identify situations where transformer lms can exploit a different strategy for filtering eagerly evaluating if an item satisfies the predicate and storing this intermediate result as a flag directly in the item representations our results reveal that transformer lms can develop humaninterpretable implementations of abstract computational operations that generalize in ways that are surprisingly ilar to strategies used in traditional functional programming patterns
as increasingly capable agents are deployed a central safety question is how to retain meaningful human control without modifying the underlying system we study a minimal control interface where an agent chooses whether to act autonomously play or defer ask while a human ultaneously chooses whether to be permissive trust or to engage in oversight oversee if the agent defers the humans choice determines the outcome potentially leading to a corrective action or a system shutdown we model this interaction as a twoplayer markov game our analysis focuses on cases where this game qualifies as a markov potential game mpg a class of games where we can provide an alignment guarantee under a structural assumption on the humans value function any decision by the agent to act more autonomously that benefits itself cannot harm the humans value we also analyze extensions to this mpg framework theoretically this perspective provides conditions for a specific form of intrinsic alignment if the reward structures of the humanagent game meet these conditions we have a formal guarantee that the agent improving its own outcome will not harm the humans practically this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe while its pretrained policy and the environments reward structure remain untouched our gridworld ulation shows that through independent learning the agent and human discover their optimal oversight roles the agent learns to ask when uncertain and the human learns when to oversee leading to an emergent collaboration that avoids safety violations introduced posttraining this demonstrates a practical method for making misaligned models safer after deployment
this paper presents a comprehensive crossplatform evaluation of reasoning capabilities in contemporary foundation models establishing an infrastructureagnostic benchmark across three computational paradigms hpc supercomputing marenostrum cloud platforms nebius ai studio and university clusters a node with eight h gpus we evaluate foundation models across problems spanning eight academic domains physics mathematics chemistry economics biology statistics calculus and optimization through three experimental phases baseline establishment six models mixtralxb phi llama b gemmab mistralb olmob evaluated on problems using marenostrum establishing methodology and reference performance infrastructure validation the problem benchmark repeated on university cluster seven models including falconmamba statespace architecture and nebius ai studio nine stateoftheart models hermes bb llama bb qwen bb deepseekr gptoss bb to confirm infrastructureagnostic reproducibility extended evaluation full problem assessment on both university cluster and nebius platforms probing generalization at scale across architectural diversity the findings challenge conventional scaling assumptions establish training data quality as more critical than model size and provide actionable guidelines for model selection across educational production and research contexts the triinfrastructure methodology and problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve
multimodal large language models mllms exhibit a pronounced preference for textual inputs when processing visionlanguage data limiting their ability to reason effectively from visual evidence unlike prior studies that attribute this text bias to external factors such as data imbalance or instruction tuning we propose that the bias originates from the models internal architecture specifically we hypothesize that visual key vectors visual keys are outofdistribution ood relative to the text key space learned during languageonly pretraining consequently these visual keys receive systematically lower ilarity scores during attention computation leading to their underutilization in the context representation to validate this hypothesis we extract key vectors from llava and qwenvl and analyze their distributional structures using qualitative tsne and quantitative jensenshannon divergence methods the results provide direct evidence that visual and textual keys occupy markedly distinct subspaces within the attention space the intermodal divergence is statistically significant exceeding intramodal variation by several orders of magnitude these findings reveal that text bias arises from an intrinsic misalignment within the attention key space rather than solely from external data factors
authorizing large language model driven agents to dynamically invoke tools and access protected resources introduces significant risks since current methods for delegating authorization grant overly broad permissions and give access to tools allowing agents to operate beyond the intended task scope we introduce and assess a delegated authorization model enabling authorization servers to semantically inspect access requests to protected resources and issue access tokens constrained to the minimal set of scopes necessary for the agents assigned tasks given the unavailability of datasets centered on delegated authorization flows particularly including both semantically appropriate and inappropriate scope requests for a given task we introduce astra a dataset and data generation pipeline for benchmarking semantic matching between tasks and scopes our experiments show both the potential and current limitations of modelbased matching particularly as the number of scopes needed for task completion increases our results highlight the need for further research into semantic matching techniques enabling intentaware authorization for multiagent and toolaugmented applications including finegrained control such as taskbased access control tbac
we envision a new era of ai termed agentic organization where agents solve complex problems by working collaboratively and concurrently enabling outcomes beyond individual intelligence to realize this vision we introduce asynchronous thinking asyncthink as a new paradigm of reasoning with large language models which organizes the internal thinking process into concurrently executable structures specifically we propose a thinking protocol where an organizer dynamically assigns subqueries to workers merges intermediate knowledge and produces coherent solutions more importantly the thinking structure in this protocol can be further optimized through reinforcement learning experiments demonstrate that asyncthink achieves lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning moreover asyncthink generalizes its learned asynchronous thinking capabilities effectively tackling unseen tasks without additional training
normative reasoning is a type of reasoning that involves normative or deontic modality such as obligation and permission while large language models llms have demonstrated remarkable performance across various reasoning tasks their ability to handle normative reasoning remains underexplored in this paper we systematically evaluate llms reasoning capabilities in the normative domain from both logical and modal perspectives specifically to assess how well llms reason with normative modals we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals which share a common formal structure to this end we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains while also incorporating nonformal cognitive factors that influence human reasoning our results indicate that although llms generally adhere to valid reasoning patterns they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases ilar to those observed in psychological studies of human reasoning these findings highlight challenges in achieving logical consistency in llms normative reasoning and provide insights for enhancing their reliability all data and code are released publicly at
the electricity sector transition requires substantial increases in residential demand response capacity yet home energy management systems hems adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters while large language models have been applied to energy systems as code generators and parameter extractors no existing implementation deploys llms as autonomous coordinators managing the complete workflow from natural language input to multiappliance scheduling this paper presents an agentic ai hems where llms autonomously coordinate multiappliance scheduling from natural language requests to device control achieving optimal scheduling without example demonstrations a hierarchical architecture combining one orchestrator with three specialist agents uses the react pattern for iterative reasoning enabling dynamic coordination without hardcoded workflows while integrating google calendar for contextaware deadline extraction evaluation across three opensource models using real austrian dayahead electricity prices reveals substantial capability differences llamab successfully coordinates all appliances across all scenarios to match costoptimal benchmarks computed via mixedinteger linear programming while other models achieve perfect singleappliance performance but struggle to coordinate all appliances ultaneously progressive prompt engineering experiments demonstrate that analytical query handling without explicit guidance remains unreliable despite models general reasoning capabilities we opensource the complete system including orchestration logic agent prompts tools and web interfaces to enable reproducibility extension and future research
we present edgerunner b a finetuned version of gptossb optimized for military tasks edgerunner b was trained on m highquality records curated from military documentation and websites we also present four new tests sets a combat arms b combat medic c cyber operations and d milbenchk general military knowledge on these military test sets edgerunner b matches or exceeds gpt task performance with statistical significance except for the high reasoning setting on the combat medic test set and the low reasoning setting on the milbenchk test set versus gptossb there is no statisticallysignificant regression on generalpurpose benchmarks like arcc gpqa diamond gsmk ifeval mmlu pro or truthfulqa except for gsmk in the low reasoning setting we also present analyses on hyperparameter settings cost and throughput these findings show that small locallyhosted models are ideal solutions for datasensitive operations such as in the military domain allowing for deployment in airgapped edge devices
human feedback is critical for aligning ai systems to human values as ai capabilities improve and ai is used to tackle more challenging tasks verifying quality and safety becomes increasingly challenging this paper explores how we can leverage ai to improve the quality of human oversight we focus on an important safety problem that is already challenging for humans factverification of ai outputs we find that combining ai ratings and human ratings based on ai rater confidence is better than relying on either alone giving humans an ai factverification assistant further improves their accuracy but the type of assistance matters displaying ai explanation confidence and labels leads to overreliance but just showing search results and evidence fosters more appropriate trust these results have implications for amplified oversight the challenge of combining humans and ai to supervise ai systems even as they surpass human expert performance
karl marx once wrote that the human essence is the ensemble of social relations suggesting that individuals are not isolated entities but are fundamentally shaped by their interactions with other entities within which contexts play a constitutive and essential role with the advent of computers and artificial intelligence these contexts are no longer limited to purely humanhuman interactions humanmachine interactions are included as well then a central question emerges how can machines better understand our situations and purposes to address this challenge researchers have recently introduced the concept of context engineering although it is often regarded as a recent innovation of the agent era we argue that related practices can be traced back more than twenty years since the early s the field has evolved through distinct historical phases each shaped by the intelligence level of machines from early humancomputer interaction frameworks built around primitive computers to todays humanagent interaction paradigms driven by intelligent agents and potentially to humanlevel or superhuman intelligence in the future in this paper we situate context engineering provide a systematic definition outline its historical and conceptual landscape and examine key design considerations for practice by addressing these questions we aim to offer a conceptual foundation for context engineering and sketch its promising future this paper is a stepping stone for a broader community effort toward systematic context engineering in ai systems
human smuggling networks are complex and constantly evolving making them difficult to analyze comprehensively legal case documents offer rich factual and procedural insights into these networks but are often long unstructured and filled with ambiguous or shifting references posing significant challenges for automated knowledge graph kg construction existing methods either overlook coreference resolution or fail to scale beyond short text spans leading to fragmented graphs and inconsistent entity linking we propose linkkg a modular framework that integrates a threestage llmguided coreference resolution pipeline with downstream kg extraction at the core of our approach is a typespecific prompt cache which consistently tracks and resolves references across document chunks enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts linkkg reduces average node duplication by and noisy nodes by compared to baseline methods resulting in cleaner and more coherent graph structures these improvements establish linkkg as a strong foundation for analyzing complex criminal networks
large language models llms such as chatgpt are increasingly integrated into highstakes decisionmaking yet little is known about their susceptibility to social influence we conducted three preregistered conformity experiments with gpto in a hiring context in a baseline study gpt consistently favored the same candidate profile c reported moderate expertise m and high certainty m and rarely changed its choice in study gpt gpt faced unanimous opposition from eight ulated partners and almost always conformed reporting lower certainty and significantly elevated selfreported informational and normative conformity p in study gpt gpt interacted with a single partner and still conformed in of disagreement trials reporting less certainty and more normative conformity across studies results demonstrate that gpt does not act as an independent observer but adapts to perceived social consensus these findings highlight risks of treating llms as neutral decision aids and underline the need to elicit ai judgments prior to exposing them to human opinions
large reasoning models lrms achieve higher task performance by allocating more inferencetime compute and prior works suggest this scaled reasoning may also strengthen safety by improving refusal yet we find the opposite the same reasoning can be used to bypass safeguards we introduce chainofthought hijacking a jailbreak attack on reasoning models the attack pads harmful requests with long sequences of harmless puzzle reasoning across harmbench cot hijacking reaches a and attack success rate asr on gemini pro gpt o mini grok mini and claude sonnet respectively far exceeding prior jailbreak methods for lrms to understand the effectiveness of our attack we turn to a mechanistic analysis which shows that mid layers encode the strength of safety checking while late layers encode the verification outcome long benign cot dilutes both signals by shifting attention away from harmful tokens targeted ablations of attention heads identified by this analysis causally decrease refusal confirming their role in a safety subnetwork these results show that the most interpretable form of reasoning explicit cot can itself become a jailbreak vector when combined with finalanswer cues we release prompts outputs and judge decisions to facilitate replication
artificial intelligence in healthcare requires models that are accurate and interpretable we advance mechanistic interpretability in medical vision by applying medical sparse autoencoders medsaes to the latent space of medclip a visionlanguage model trained on chest radiographs and reports to quantify interpretability we propose an evaluation framework that combines correlation metrics entropy analyzes and automated neuron naming via the medgemma foundation model experiments on the chexpert dataset show that medsae neurons achieve higher monosemanticity and interpretability than raw medclip features our findings bridge highperforming medical ai and transparency offering a scalable step toward clinically reliable representations
the rapid growth of programming education has outpaced traditional assessment tools leaving faculty with limited means to provide meaningful scalable feedback conventional autograders while efficient act as blackbox systems that ply return passfail results offering little insight into student thinking or learning needs autograder is designed to shift autograding from a purely summative process to a formative learning experience it introduces two key capabilities automated feedback generation using a finetuned large language model and visualization of student code submissions to uncover learning patterns the model is finetuned on curated student code and expert feedback to ensure pedagogically aligned contextaware guidance in evaluation across student submissions from multiple programming tasks the system produced feedback with strong semantic alignment to instructor comments for visualization contrastively learned code embeddings trained on annotated submissions enable grouping solutions into meaningful clusters based on functionality and approach the system also supports promptpooling allowing instructors to guide feedback style through selected prompt templates by integrating aidriven feedback semantic clustering and interactive visualization autograder reduces instructor workload while supporting targeted instruction and promoting stronger learning outcomes
the emergence of agentic artificial intelligence ai is set to trigger a cambrian explosion of new kinds of personhood this paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered but as a flexible bundle of obligations rights and responsibilities that societies confer upon entities for a variety of reasons especially to solve concrete governance problems we argue that this traditional bundle can be unbundled creating bespoke solutions for different contexts this will allow for the creation of practical tools such as facilitating ai contracting by creating a target individual that can be sanctioned without needing to resolve intractable debates about an ais consciousness or rationality we explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology examining both personhood as a problem where design choices can create dark patterns that exploit human social heuristics and personhood as a solution where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict by rejecting foundationalist quests for a single essential definition of personhood this paper offers a more pragmatic and flexible way to think about integrating ai agents into our society
the prohibitive cost of evaluating large language models llms on comprehensive benchmarks necessitates the creation of small yet representative data subsets ie tiny benchmarks that enable efficient assessment while retaining predictive fidelity current methods for this task operate under a modelcentric paradigm selecting benchmarking items based on the collective performance of existing models such approaches are limited by large upfront costs an inability to immediately handle new benchmarks coldstart and the fragile assumption that future models will share the failure patterns of their predecessors in this work we challenge this paradigm and propose a itemcentric approach to benchmark subset selection arguing that selection should be based on the intrinsic properties of the task items themselves rather than on modelspecific failure patterns we instantiate this itemcentric efficient benchmarking approach via a novel method scales where data selection is based on the cognitive demands of the benchmark samples empirically we show scales reduces the upfront selection cost by over x while achieving competitive predictive fidelity on the open llm leaderboard using just a data subset we predict full benchmark scores with a mean absolute error we demonstrate that this itemcentric approach enables more efficient model evaluation without significant fidelity degradation while also providing better coldstart performance and more interpretable benchmarking
artificial intelligence ai has demonstrated impressive progress in mathematical reasoning yet its integration into the practice of mathematical research remains limited in this study we investigate how the ai mathematician aim system can operate as a research partner rather than a mere problem solver focusing on a challenging problem in homogenization theory we analyze the autonomous reasoning trajectories of aim and incorporate targeted human interventions to structure the discovery process through iterative decomposition of the problem into tractable subgoals selection of appropriate analytical methods and validation of intermediate results we reveal how human intuition and machine computation can complement one another this collaborative paradigm enhances the reliability transparency and interpretability of the resulting proofs while retaining human oversight for formal rigor and correctness the approach leads to a complete and verifiable proof and more broadly demonstrates how systematic humanai coreasoning can advance the frontier of mathematical discovery
reinforcement finetuning rft is a key technique for aligning large language models llms with human preferences and enhancing reasoning yet its effectiveness is highly sensitive to which tasks are explored during training uniform task sampling is inefficient wasting computation on tasks that are either trivial or unsolvable while existing task selection methods often suffer from high rollout costs poor adaptivity or incomplete evidence we introduce bots a unified framework for bayesian online task selection in llm reinforcement finetuning grounded in bayesian inference bots adaptively maintains posterior estimates of task difficulty as the model evolves it jointly incorporates explicit evidence from direct evaluations of selected tasks and implicit evidence inferred from these evaluations for unselected tasks with thompson sampling ensuring a principled balance between exploration and exploitation to make implicit evidence practical we instantiate it with an ultralight interpolationbased plugin that estimates difficulties of unevaluated tasks without extra rollouts adding negligible overhead empirically across diverse domains and llm scales bots consistently improves data efficiency and performance over baselines and ablations providing a practical and extensible solution for dynamic task selection in rft
one approach to enhance monte carlo tree search mcts is to improve its sample efficiency by groupingabstracting states or stateaction pairs and sharing statistics within a group though stateaction pair abstractions are mostly easy to find in algorithms such as on the go abstractions in upper confidence bounds applied to trees ogauct nearly no state abstractions are found in either noisy or large action space settings due to constraining conditions we provide theoretical and empirical evidence for this claim and we slightly alleviate this state abstraction problem by proposing a weaker state abstraction condition that trades a minor loss in accuracy for finding many more abstractions we name this technique ideal pruning abstractions in uct ipauct which outperforms ogauct and any of its derivatives across a large range of test domains and iteration budgets as experimentally validated ipauct uses a different abstraction framework from abstraction of stateaction pairs asap which is the one used by ogauct which we name ipa furthermore we show that both ipa and asap are special cases of a more general framework that we call pasap which itself is a special case of the asasap framework
compliance at web scale poses practical challenges each request may require a regulatory assessment regulatory texts eg the general data protection regulation gdpr are crossreferential and normative while runtime contexts are expressed in unstructured natural language this setting motivates us to align semantic information in unstructured text with the structured normative elements of regulations to this end we introduce graphcompliance a framework that represents regulatory texts as a policy graph and runtime contexts as a context graph and aligns them in this formulation the policy graph encodes normative structure and crossreferences whereas the context graph formalizes events as subjectactionobject sao and entityrelation triples this alignment anchors the reasoning of a judge large language model llm in structured information and helps reduce the burden of regulatory interpretation and event parsing enabling a focus on the core reasoning step in experiments on gdprderived realworld scenarios spanning five evaluation tasks graphcompliance yields percentage points pp higher microf than llmonly and rag baselines with fewer under and overpredictions resulting in higher recall and lower false positive rates ablation studies indicate contributions from each graph component suggesting that structured representations and a judge llm are complementary for normative reasoning
group based reinforcement learning rl has shown impressive results on complex reasoning and mathematical tasks yet when applied to train multiturn interactive llm agents these methods often suffer from structural blindnessthe inability to exploit the underlying connectivity of the environment this manifests in three critical challenges inefficient unguided exploration imprecise credit assignment due to overlooking pivotal states and myopic planning caused by static reward discounting we address these issues with graphenhanced policy optimization gepo which dynamically constructs a statetransition graph from agent experience and employs graphtheoretic centrality to provide three synergistic learning signals structured intrinsic rewards that guide exploration toward highimpact states a graphenhanced advantage function for topologyaware credit assignment and a dynamic discount factor adapted to each states strategic value on the alfworld webshop and a proprietary workbench benchmarks gepo demonstrates strong performance achieving absolute success rate gains of and over competitive baselines these results highlight that explicitly modeling environmental structure is a robust generalizable strategy for advancing llm agent training
with increasing urban traffic complexity traffic signal control tsc is essential for optimizing traffic flow and improving road safety large language models llms emerge as promising approaches for tsc however they are prone to hallucinations in emergencies leading to unreliable decisions that may cause substantial delays for emergency vehicles moreover diverse intersection types present substantial challenges for traffic state encoding and crossintersection training limiting generalization across heterogeneous intersections therefore this paper proposes retrieval augmented generation ragenhanced distributed llm agents with emergency response for generalizable tsc regtsc firstly this paper presents an emergencyaware reasoning framework which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel reviewerbased emergency rag rerag to distill specific knowledge and guidance from historical cases enhancing the reliability and rationality of agents emergency decisions secondly this paper designs a typeagnostic traffic representation and proposes a rewardguided reinforced refinement r for heterogeneous intersections r adaptively samples training experience from diverse intersections with environment feedbackbased priority and finetunes llm agents with a designed rewardweighted likelihood loss guiding regtsc toward highreward policies across heterogeneous intersections on three realworld road networks with to heterogeneous intersections extensive experiments show that regtsc reduces travel time by queue length by and emergency vehicle waiting time by outperforming other stateoftheart methods
millions of people take surveys every day from market polls and academic studies to medical questionnaires and customer feedback forms these datasets capture valuable insights but their scale and structure present a unique challenge for large language models llms which otherwise excel at fewshot reasoning over openended text yet their ability to process questionnaire data or lists of questions crossed with hundreds of respondent rows remains underexplored current retrieval and survey analysis tools eg qualtrics spss redcap are typically designed for humans in the workflow limiting such data integration with llm and aiempowered automation this gap leaves scientists surveyors and everyday users without evidencebased guidance on how to best represent questionnaires for llm consumption we address this by introducing qasu questionnaire analysis and structural understanding a benchmark that probes six structural skills including answer lookup respondent count and multihop inference across six serialization formats and multiple prompt strategies experiments on contemporary llms show that choosing an effective format and prompt combination can improve accuracy by up to points compared to suboptimal formats for specific tasks carefully adding a lightweight structural hint through selfaugmented prompting can yield further improvements of points on average by systematically isolating format and prompting effects our open source benchmark offers a ple yet versatile foundation for advancing both research and realworld practice in llmbased questionnaire analysis
reward models rms play a critical role in aligning large language models llms with human preferences yet in the domain of tool learning the lack of rms specifically designed for functioncalling tasks has limited progress toward more capable agentic ai we introduce toolrm a family of lightweight generative rms tailored for general tooluse scenarios to build these models we propose a novel pipeline that constructs pairwise preference data using rulebased scoring and multidimensional sampling this yields toolprefpairwisek a diverse balanced and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback to evaluate tooluse rms we also introduce trbenchbfcl a benchmark built on the agentic evaluation suite bfcl trained on our constructed data models from the qwenbb series achieve up to higher accuracy substantially outperforming frontier models such as claude and openai o in pairwise reward judgments beyond training objectives toolrm generalizes to broader critique tasks including bestofn sampling and selfcorrection experiments on acebench highlight its effectiveness and efficiency enabling inferencetime scaling and reducing output token usage by over we release data and model checkpoints to facilitate future research
large language models llms are catalyzing the development of autonomous ai research agents for scientific and engineering discovery we present fm agent a novel and generalpurpose multiagent framework that leverages a synergistic combination of llmbased reasoning and largescale evolutionary search to address complex realworld challenges the core of fm agent integrates several key innovations a coldstart initialization phase incorporating expert guidance a novel evolutionary sampling strategy for iterative optimization domainspecific evaluators that combine correctness effectiveness and llmsupervised feedback and a distributed asynchronous execution infrastructure built on ray demonstrating broad applicability our system has been evaluated across diverse domains including operations research machine learning gpu kernel optimization and classical mathematical problems fm agent reaches stateoftheart results autonomously without human interpretation or tuning on alebench on mlebench pp up to x speedups on kernelbench and establishes new stateoftheartsota results on several classical mathematical problems beyond academic benchmarks fm agent shows considerable promise for both largescale enterprise rd workflows and fundamental scientific research where it can accelerate innovation automate complex discovery processes and deliver substantial engineering and scientific advances with broader societal impact
reinforcement learning rl can elicit strong reasoning in large language models llms yet most open efforts focus on math and code we propose reasoning curriculum a ple twostage curriculum that first elicits reasoning skills in pretrainingaligned domains such as math then adapts and refines these skills across other domains via joint rl stage performs a brief cold start and then mathonly rl with verifiable rewards to develop reasoning skills stage runs joint rl on mixeddomain data to transfer and consolidate these skills the curriculum is minimal and backboneagnostic requiring no specialized reward models beyond standard verifiability checks evaluated on qwenb and llamab over a multidomain suite reasoning curriculum yields consistent gains ablations and a cognitiveskill analysis indicate that both stages are necessary and that mathfirst elicitation increases cognitive behaviors important for solving complex problems reasoning curriculum provides a compact easytoadopt recipe for general reasoning
the inference cost of large language models llms has become a critical factor in determining their commercial viability and widespread adoption this paper introduces a quantitative economics of inference framework treating the llm inference process as a computedriven intelligent production activity we analyze its marginal cost economies of scale and quality of output under various performance configurations based on empirical data from wineval we construct the first llm inference production frontier revealing three principles diminishing marginal cost diminishing returns to scale and an optimal costeffectiveness zone this paper not only provides an economic basis for model deployment decisions but also lays an empirical foundation for the future marketbased pricing and optimization of ai inference resources
large vision language models vlms have advanced graphical user interface gui task automation but still lag behind humans we hypothesize this gap stems from missing core gui knowledge which existing training schemes such as supervised fine tuning and reinforcement learning alone cannot fully address by analyzing common failure patterns in gui task execution we distill gui knowledge into three dimensions interface perception knowledge about recognizing widgets and system states interaction prediction knowledge about reasoning action state transitions and instruction understanding knowledge about planning verifying and assessing task completion progress we further introduce gui knowledge bench a benchmark with multiple choice and yesno questions across six platforms web android macos windows linux ios and applications our evaluation shows that current vlms identify widget functions but struggle with perceiving system states predicting actions and verifying task completion experiments on real world gui tasks further validate the close link between gui knowledge and task success by providing a structured framework for assessing gui knowledge our work supports the selection of vlms with greater potential prior to downstream training and provides insights for building more capable gui agents
we present leanphys a comprehensive reasoning framework for collegelevel physics problems in lean leanphys includes leanphysbench a collegelevel benchmark for formal physics reasoning in lean which contains handcrafted and peerreviewed statements derived from university textbooks and physics competition problems to establish a solid foundation for formal reasoning in physics we also introduce physlib a communitydriven repository containing fundamental unit systems and theorems essential for formal physics reasoning based on the benchmark and lean repository we composed in leanphys we report baseline results using major expert math lean provers and stateoftheart closedsource models with the best performance of deepseekprovervb achieving only and claudesonnet achieving we also conduct a detailed analysis showing that our physlib can achieve an average improvement of in model performance this demonstrates the challenging nature of our leanphysbench and the effectiveness of physlib to the best of our knowledge this is the first study to provide a physics benchmark in lean
the ai we use is powerful and its power is increasing rapidly if this powerful ai is to serve the needs of consumers voters and decision makers then it is imperative that the ai is accountable in general an agent is accountable to a forum if the forum can request information from the agent about its actions if the forum and the agent can discuss this information and if the forum can sanction the agent unfortunately in too many cases todays ai is not accountable we cannot question it enter into a discussion with it let alone sanction it in this chapter we relate the general definition of accountability to ai we illustrate what it means for ai to be accountable and unaccountable and we explore approaches that can improve our chances of living in a world where all ai is accountable to those who are affected by it
despite significant advancements in recent decades autonomous vehicles avs continue to face challenges in navigating certain traffic scenarios where human drivers excel in such situations avs often become immobilized disrupting overall traffic flow current recovery solutions such as remote intervention which is costly and inefficient and manual takeover which excludes nondrivers and limits av accessibility are inadequate this paper introduces stucksolver a novel large language model llm driven recovery framework that enables avs to resolve immobilization scenarios through selfreasoning andor passengerguided decisionmaking stucksolver is designed as a plugin addon module that operates on top of the avs existing perceptionplanningcontrol stack requiring no modification to its internal architecture instead it interfaces with standard sensor data streams to detect immobilization states interpret environmental context and generate highlevel recovery commands that can be executed by the avs native planner we evaluate stucksolver on the benchdrive benchmark and in customdesigned uncertainty scenarios results show that stucksolver achieves nearstateoftheart performance through autonomous selfreasoning alone and exhibits further improvements when passenger guidance is incorporated
the rapid growth of research literature particularly in large language models llms has made producing comprehensive and current survey papers increasingly difficult this paper introduces autosurvey a multistage pipeline that automates survey generation through retrievalaugmented synthesis and structured evaluation the system integrates parallel section generation iterative refinement and realtime retrieval of recent publications to ensure both topical completeness and factual accuracy quality is assessed using a multillm evaluation framework that measures coverage structure and relevance in alignment with expert review standards experimental results demonstrate that autosurvey consistently outperforms existing retrievalbased and automated baselines achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity by combining retrieval reasoning and automated evaluation into a unified framework autosurvey provides a scalable and reproducible solution for generating longform academic surveys and contributes a solid foundation for future research on automated scholarly writing all code and resources are available at
naturallanguagetosql nltosql systems hold promise for democratizing access to structured data allowing users to query databases without learning sql yet existing systems struggle with realistic spatiotemporal queries where success requires aligning vague user phrasing with schemaspecific categories handling temporal reasoning and choosing appropriate outputs we present an agentic pipeline that extends a naive texttosql baseline llamasqlcoderb with orchestration by a mistralbased react agent the agent can plan decompose and adapt queries through schema inspection sql generation execution and visualization tools we evaluate on naturallanguage queries over the nyc and tokyo checkin dataset covering spatial temporal and multidataset reasoning the agent achieves substantially higher accuracy than the naive baseline vs and enhances usability through maps plots and structured naturallanguage summaries crucially our design enables more natural humandatabase interaction supporting users who lack sql expertise detailed schema knowledge or prompting skill we conclude that agentic orchestration rather than stronger sql generators alone is a promising foundation for interactive geospatial assistants
peoples goaldirected behaviors are influenced by their cognitive biases and autonomous systems that interact with people should be aware of this for example peoples attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work here building on recent work in computational cognitive science we formally articulate the attentionaware inverse planning problem in which the goal is to estimate a persons attentional biases from their actions we demonstrate how attentionaware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior finally we present an approach to attentionaware inverse planning that combines deep reinforcement learning with computational cognitive modeling we use this approach to infer the attentional strategies of rl agents in reallife driving scenarios selected from the waymo open dataset demonstrating the scalability of estimating cognitive biases with attentionaware inverse planning
we introduce humansjunior a b model that matches gpto on the facts grounding public subset within a pm pp equivalence margin results on qq under identical judges gpto scores ci and humansjunior ci the paired difference is pp bootstrap ci to permutation p cohens d tost establishes equivalence at pm pp not at pm pp when purchased as managed apis humansjuniors base model phiminiinstruct is approx times less expensive than gpto on microsoft ai foundry pricing selfhosted or edge deployments can drive incremental inference cost toward zero measured vs estimated pricing sources are tabulated in appendix e method our approach combines minimal directed exoskeleton reasoning scaffolds with behavioral finetuning that teaches protocol compliance epistemic discipline rather than domain answers finetuning alone adds little combined they synergize pp p and reduce variance approx in promptonly settings on frontier models qq noncomparable directed reasoning improved gpto by pp to and geminipro by pp to baseline n see section tldr a b model achieves gptolevel facts accuracy equivalent within pm pp on qq cloud pricing shows approx times lower cost versus gpto and selfhostededge deployments can approach zero marginal cost pricing sources are listed in appendix e frontier promptonly gains qq noncomparable and optimizedprompt exploratory results under earlier judges are summarized in appendix f keywords small language models factual grounding directed reasoning finetuning model alignment costefficient ai
finops finance operations represents an operational framework and cultural practice which maximizes cloud business value through collaborative financial accountability across engineering finance and business teams finops practitioners face a fundamental challenge billing data arrives in heterogeneous formats taxonomies and metrics from multiple cloud providers and internal systems which eventually lead to synthesizing actionable insights and making timesensitive decisions to address this challenge we propose leveraging autonomous goaldriven ai agents for finops automation in this paper we built a finops agent for a typical usecase for it infrastructure and cost optimization we built a system ulating a realistic endtoend industry process starting with retrieving data from various sources to consolidating and analyzing the data to generate recommendations for optimization we defined a set of metrics to evaluate our agent using several opensource and closesource language models and it shows that the agent was able to understand plan and execute tasks as well as an actual finops practitioner
large language models llms have demonstrated transformative potential in scientific research yet their deployment in highstakes contexts raises significant trustworthiness concerns here we introduce scitrust a comprehensive framework for evaluating llm trustworthiness in scientific applications across four dimensions truthfulness adversarial robustness scientific safety and scientific ethics our framework incorporates novel openended truthfulness benchmarks developed through a verified reflectiontuning pipeline and expert validation alongside a novel ethics benchmark for scientific research contexts covering eight subcategories including dualuse research and bias we evaluated seven prominent llms including four sciencespecialized models and three generalpurpose industry models using multiple evaluation metrics including accuracy semantic ilarity measures and llmbased scoring generalpurpose industry models overall outperformed sciencespecialized models across each trustworthiness dimension with gptomini demonstrating superior performance in truthfulness assessments and adversarial robustness sciencespecialized models showed significant deficiencies in logical and ethical reasoning capabilities along with concerning vulnerabilities in safety evaluations particularly in highrisk domains such as biosecurity and chemical weapons by opensourcing our framework we provide a foundation for developing more trustworthy ai systems and advancing research on model safety and ethics in scientific contexts
aligning llmbased judges with human preferences is a significant challenge as they are difficult to calibrate and often suffer from rubric sensitivity bias and instability overcoming this challenge advances key applications such as creating reliable reward models for reinforcement learning from human feedback rlhf and building effective routing systems that select the bestsuited model for a given user query in this work we propose a framework for modeling diverse personabased preferences by learning to aggregate outputs from multiple rubricconditioned judges we investigate the performance of this approach against naive baselines and assess its robustness through case studies on both human and llmjudges biases our primary contributions include a personabased method for synthesizing preference labels at scale and two distinct implementations of our aggregator generalized additive model gam and a multilayer perceptron mlp
existing frameworks converge on the centrality of compression to intelligence but leave underspecified why this process enforces the discovery of causal structure rather than superficial statistical patterns we introduce a twolevel framework to address this gap the informationtheoretic imperative iti establishes that any system persisting in uncertain environments must minimize epistemic entropy through predictive compression this is the evolutionary why linking survival pressure to informationprocessing demands the compression efficiency principle cep specifies how efficient compression mechanically selects for generative causal models through exceptionaccumulation dynamics making reality alignment a consequence rather than a contingent achievement together iti and cep define a causal chain from survival pressure to prediction necessity compression requirement efficiency optimization generative structure discovery and ultimately reality alignment each link follows from physical informationtheoretic or evolutionary constraints implying that intelligence is the mechanically necessary outcome of persistence in structured environments this framework yields empirically testable predictions compression efficiency measured as approach to the ratedistortion frontier correlates with outofdistribution generalization exceptionaccumulation rates differentiate causal from correlational models hierarchical systems exhibit increasing efficiency across abstraction layers and biological systems demonstrate metabolic costs that track representational complexity iti and cep thereby provide a unified account of convergence across biological artificial and multiscale systems addressing the epistemic and functional dimensions of intelligence without invoking assumptions about consciousness or subjective experience
large language models llms are increasingly used as raters for evaluation tasks however their reliability is often limited for subjective tasks when human judgments involve subtle reasoning beyond annotation labels thinking traces the reasoning behind a judgment are highly informative but challenging to collect and curate we present a humanllm collaborative framework to infer thinking traces from labelonly annotations the proposed framework uses a ple and effective rejection sampling method to reconstruct these traces at scale these inferred thinking traces are applied to two complementary tasks finetuning open llm raters and synthesizing clearer annotation guidelines for proprietary llm raters across multiple datasets our methods lead to significantly improved llmhuman agreement additionally the refined annotation guidelines increase agreement among different llm models these results suggest that llms can serve as practical proxies for otherwise unrevealed human thinking traces enabling labelonly corpora to be extended into thinkingtraceaugmented resources that enhance the reliability of llm raters
large language models llms promise to transform interactive games by enabling nonplayer characters npcs to sustain unscripted dialogue yet it remains unclear whether constrained prompts actually improve player experience we investigate this question through the interview a voicebased detective game powered by gpto a withinsubjects usability study n compared highconstraint hcp and lowconstraint lcp prompts revealing no reliable experiential differences beyond sensitivity to technical breakdowns guided by these findings we redesigned the hcp into a hybrid jsonrag scaffold and conducted a synthetic evaluation with an llm judge positioned as an earlystage complement to usability testing results uncovered a novel pattern scaffolding effects were roledependent the interviewer questgiver npc gained stability while suspect npcs lost improvisational believability these findings overturn the assumption that tighter constraints inherently enhance play extending fuzzysymbolic scaffolding we introduce symbolically scaffolded play a framework in which symbolic structures are expressed as fuzzy numerical boundaries that stabilize coherence where needed while preserving improvisation where surprise sustains engagement
we present a novel framework for industry that plifies the deployment of ai models on edge devices in various industrial settings the design reduces latency and avoids external data transfer by enabling local inference and realtime processing our implementation is agentbased which means that individual agents whether human algorithmic or collaborative are responsible for welldefined tasks enabling flexibility and plifying integration moreover our framework supports modular integration and maintains low resource requirements preliminary evaluations concerning the food industry in real scenarios indicate improved deployment time and system adaptability performance the source code is publicly available at
contemporary chess engines offer precise yet opaque evaluations typically expressed as centipawn scores while effective for decisionmaking these outputs obscure the underlying contributions of individual pieces or patterns in this paper we explore adapting shap shapley additive explanations to the domain of chess analysis aiming to attribute a chess engines evaluation to specific pieces on the board by treating pieces as features and systematically ablating them we compute additive perpiece contributions that explain the engines output in a locally faithful and humaninterpretable manner this method draws inspiration from classical chess pedagogy where players assess positions by mentally removing pieces and grounds it in modern explainable ai techniques our approach opens new possibilities for visualization human training and engine comparison we release accompanying code and data to foster future research in interpretable chess ai
recent video generation models can produce highfidelity temporally coherent videos indicating that they may encode substantial world knowledge beyond realistic synthesis they also exhibit emerging behaviors indicative of visual perception modeling and manipulation yet an important question still remains are video models ready to serve as zeroshot reasoners in challenging visual reasoning scenarios in this work we conduct an empirical study to comprehensively investigate this question focusing on the leading and popular veo we evaluate its reasoning behavior across dimensions including spatial geometric physical temporal and embodied logic systematically characterizing both its strengths and failure modes to standardize this study we curate the evaluation data into mmecof a compact benchmark that enables indepth and thorough assessment of chainofframe cof reasoning our findings reveal that while current video models demonstrate promising reasoning patterns on shorthorizon spatial coherence finegrained grounding and locally consistent dynamics they remain limited in longhorizon causal reasoning strict geometric constraints and abstract logic overall they are not yet reliable as standalone zeroshot reasoners but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models project page
as coding agents are increasingly deployed in large codebases the need to automatically design challenging codebaselevel evaluation is central we propose gistify a task where a coding llm must create a single minimal selfcontained file that can reproduce a specific functionality of a codebase the coding llm is given full access to a codebase along with a specific entrypoint eg a python command and the generated file must replicate the output of the same command ran under the full codebase while containing only the essential components necessary to execute the provided command success on gistify requires both structural understanding of the codebase accurate modeling of its execution flow as well as the ability to produce potentially large code patches our findings show that current stateoftheart models struggle to reliably solve gistify tasks especially ones with long executions traces
reinforcement learning rl finetuning of large language models llms often suffers from instability due to the numerical mismatch between the training and inference policies while prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments we show that its root cause lies in the floating point precision itself the widely adopted bf despite its large dynamic range introduces large rounding errors that breaks the consistency between training and inference in this work we demonstrate that ply reverting to fp effectively eliminates this mismatch the change is ple fully supported by modern frameworks with only a few lines of code change and requires no modification to the model architecture or learning algorithm our results suggest that using fp uniformly yields more stable optimization faster convergence and stronger performance across diverse tasks algorithms and frameworks we hope these findings motivate a broader reconsideration of precision tradeoffs in rl finetuning
ais have made rapid progress on researchoriented benchmarks of knowledge and reasoning but it remains unclear how these gains translate into economic value and automation to measure this we introduce the remote labor index rli a broadly multisector benchmark comprising realworld economically valuable projects designed to evaluate endtoend agent performance in practical settings ai agents perform near the floor on rli with the highestperforming agent achieving an automation rate of these results help ground discussions of ai automation in empirical evidence setting a common basis for tracking ai impacts and enabling stakeholders to proactively navigate aidriven labor automation
a world model is an internal model that ulates how the world evolves given past observations and actions it predicts the future of both the embodied agent and its environment accurate world models are essential for enabling agents to think plan and reason effectively in complex dynamic settings despite rapid progress current world models remain brittle and degrade over long horizons we argue that a central cause is representation quality exteroceptive inputs eg images are highdimensional and lossy or entangled latents make dynamics learning unnecessarily hard we therefore ask whether improving representation learning alone can substantially improve worldmodel performance in this work we take a step toward building a truly accurate world model by addressing a fundamental yet open problem constructing a model that can fully clone and overfit to a deterministic d world we propose geometricallyregularized world models grwm which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space this approach yields significantly improved latent representations that align closely with the true topology of the environment grwm is plugandplay requires only minimal architectural modification scales with trajectory length and is compatible with diverse latent generative backbones across deterministic d settings and longhorizon prediction tasks grwm significantly increases rollout fidelity and stability analyses show that its benefits stem from learning a latent manifold with superior geometric structure these findings support a clear takeaway improving representation learning is a direct and useful path to robust world models delivering reliable longhorizon predictions without enlarging the dynamics module
we present amobench an advanced mathematical reasoning benchmark with olympiad level or even higher difficulty comprising humancrafted problems existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models llms however many existing math competitions are becoming less effective for assessing toptier llms due to performance saturation eg aime to address this amobench introduces more rigorous challenges by ensuring all problems are crossvalidated by experts to meet at least the international mathematical olympiad imo difficulty standards and entirely original problems to prevent potential performance leakages from data memorization moreover each problem in amobench requires only a final answer rather than a proof enabling automatic and robust grading for evaluation experimental results across llms on amobench show that even the bestperforming model achieves only accuracy on amobench with most llms scoring below beyond these poor performances our further analysis reveals a promising scaling trend with increasing testtime compute on amobench these results highlight the significant room for improving the mathematical reasoning in current llms we release amobench to facilitate further research into advancing the reasoning abilities of language models
as llms occupy an increasingly important role in society they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems therefore studying the alignment of llms with human values has become a crucial field of inquiry prior work however mostly focuses on evaluating the alignment of fully trained models overlooking the training dynamics by which models learn to express human values in this work we investigate how and at which stage value alignment arises during the course of a models posttraining our analysis disentangles the effects of posttraining algorithms and datasets measuring both the magnitude and time of value drifts during training experimenting with llama and qwen models of different sizes and popular supervised finetuning sft and preference optimization datasets and algorithms we find that the sft phase generally establishes a models values and subsequent preference optimization rarely realigns these values furthermore using a synthetic preference dataset that enables controlled manipulation of values we find that different preference optimization algorithms lead to different value alignment outcomes even when preference data is held constant our findings provide actionable insights into how values are learned during posttraining and help to inform data curation as well as the selection of models and algorithms for preference optimization to improve model alignment to human values
the endtoend label for llms is a misnomer in practice they depend on a nondifferentiable decoding process that requires laborious handtuning of hyperparameters like temperature and topp this paper introduces autodeco a novel architecture that enables truly endtoend generation by learning to control its own decoding strategy we augment the standard transformer with lightweight heads that at each step dynamically predict contextspecific temperature and topp values alongside the nexttoken logits this approach transforms decoding into a parametric tokenlevel process allowing the model to selfregulate its sampling strategy within a single forward pass through extensive experiments on eight benchmarks we demonstrate that autodeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracletuned baseline derived from hacking the test seta practical upper bound for any static method crucially we uncover an emergent capability for instructionbased decoding control the model learns to interpret natural language commands eg generate with low randomness and adjusts its predicted temperature and topp on a tokenbytoken basis opening a new paradigm for steerable and interactive llm decoding
we introduce kimi linear a hybrid linear attention architecture that for the first time outperforms full attention under fair comparisons across various scenarios including shortcontext longcontext and reinforcement learning rl scaling regimes at its core lies kimi delta attention kda an expressive linear attention module that extends gated deltanet with a finergrained gating mechanism enabling more effective use of limited finitestate rnn memory our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the diagonalpluslowrank dplr transition matrices which substantially reduces computation compared to the general dplr formulation while remaining more consistent with the classical delta rule we pretrain a kimi linear model with b activated parameters and b total parameters based on a layerwise hybrid of kda and multihead latent attention mla our experiments show that with an identical training recipe kimi linear outperforms full mla with a sizeable margin across all evaluated tasks while reducing kv cache usage by up to and achieving up to times decoding throughput for a m context these results demonstrate that kimi linear can be a dropin replacement for full attention architectures with superior performance and efficiency including tasks with longer input and output lengths to support further research we opensource the kda kernel and vllm implementations and release the pretrained and instructiontuned model checkpoints
large language models llms have demonstrated exceptional capabilities across multiple domains by leveraging massive pretraining and curated finetuning data however in datasensitive fields such as healthcare the lack of highquality domainspecific training corpus hinders llms adaptation for specialized applications meanwhile domain experts have distilled domain wisdom into ontology rules which formalize relationships among concepts and ensure the integrity of knowledge management repositories viewing llms as implicit repositories of human knowledge we propose evontree a novel framework that leverages a small set of highquality ontology rules to systematically extract validate and enhance domain knowledge within llms without requiring extensive external datasets specifically evontree extracts domain ontology from raw models detects inconsistencies using two core ontology rules and reinforces the refined knowledge via selfdistilled finetuning extensive experiments on medical qa benchmarks with llamabinstruct and medv demonstrate consistent outperformance over both unmodified models and leading supervised baselines achieving up to a improvement in accuracy these results confirm the effectiveness efficiency and robustness of our approach for lowresource domain adaptation of llms
recent large language model llm research has undergone an architectural shift from encoderdecoder modeling to nowadays the dominant decoderonly modeling this rapid transition however comes without a rigorous comparative analysis especially from the scaling perspective raising concerns that the potential of encoderdecoder models may have been overlooked to fill this gap we revisit encoderdecoder llm redllm enhancing it with recent recipes from decoderonly llm decllm we conduct a comprehensive comparison between redllm pretrained with prefix language modeling lm and decllm pretrained with causal lm at different model scales ranging from simm to simb using redpajama v t tokens for pretraining and flan for instruction tuning our experiments show that redllm produces compelling scaling properties and surprisingly strong performance while decllm is overall more computeoptimal during pretraining redllm demonstrates comparable scaling and context length extrapolation capabilities after instruction tuning redllm achieves comparable and even better results on various downstream tasks while enjoying substantially better inference efficiency we hope our findings could inspire more efforts on reexamining redllm unlocking its potential for developing powerful and efficient llms
multipage visual documents such as manuals brochures presentations and posters convey key information through layout colors icons and crossslide references while large language models llms offer opportunities in document understanding current systems struggle with complex multipage visual documents particularly in finegrained reasoning over elements and pages we introduce slideagent a versatile agentic framework for understanding multimodal multipage and multilayout documents especially slide decks slideagent employs specialized agents and decomposes reasoning into three specialized levelsglobal page and elementto construct a structured queryagnostic representation that captures both overarching themes and detailed visual or textual cues during inference slideagent selectively activates specialized agents for multilevel reasoning and integrates their outputs into coherent contextaware answers extensive experiments show that slideagent achieves significant improvement over both proprietary overall and opensource models overall
large language models llms face significant inference latency challenges stemming from their autoregressive design and large size to address this speculative decoding emerges as a solution enabling the ultaneous generation and validation of multiple tokens while recent approaches like eagle and eagle improve speculative decoding using dynamic tree structures they often neglect the impact of crucial system variables such as gpu devices and batch sizes therefore we introduce a new dynamic tree decoding approach called cast that takes into account inference costs including factors such as gpu configurations and batch sizes to dynamically refine the tree structure through comprehensive experimentation across six diverse tasks and utilizing six distinct llms our methodology demonstrates remarkable results achieving speeds up to times faster than conventional decoding methods moreover it generally outperforms existing stateoftheart techniques from to
reinforcement learning with verifiable rewards rlvr is a promising approach for enhancing agentic deep search however its application is often hindered by low reward density in deep search scenarios where agents expend significant exploratory costs for infrequent and often null final rewards in this paper we formalize this challenge as the reward density optimization problem which aims to improve the reward obtained per unit of exploration cost this paper introduce infoflow a systematic framework that tackles this problem from three aspects subproblem decomposition breaking down longrange tasks to assign process rewards thereby providing denser learning signals failureguided hints injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes dualagent refinement employing a dualagent architecture to offload the cognitive burden of deep exploration a refiner agent synthesizes the search history which effectively compresses the researchers perceived trajectory thereby reducing exploration cost and increasing the overall reward density we evaluate infoflow on multiple agentic search benchmarks where it significantly outperforms strong baselines enabling lightweight llms to achieve performance comparable to advanced proprietary llms
this paper investigates the structure of linear operators introduced in hernandez et al that decode specific relational facts in transformer language models we extend their singlerelation findings to a collection of relations and systematically chart their organization we show that such collections of relation decoders can be highly compressed by ple order tensor networks without significant loss in decoding accuracy to explain this surprising redundancy we develop a crossevaluation protocol in which we apply each linear decoder operator to the subjects of every other relation our results reveal that these linear maps do not encode distinct relations but extract recurring coarsegrained semantic properties eg country of capital city and country of food are both in the countryofx property this propertycentric structure clarifies both the operators compressibility and highlights why they generalize only to new relations that are semantically close our findings thus interpret linear relational decoding in transformer language models as primarily propertybased rather than relationspecific
diacritics restoration in hebrew is a fundamental task for ensuring accurate word pronunciation and disambiguating textual meaning despite the languages high degree of ambiguity when unvocalized recent machine learning approaches have significantly advanced performance on this task in this work we present divrit a novel system for hebrew diacritization that frames the task as a zeroshot classification problem our approach operates at the word level selecting the most appropriate diacritization pattern for each undiacritized word from a dynamically generated candidate set conditioned on the surrounding textual context a key innovation of divrit is its use of a hebrew visual language model which processes undiacritized text as an image allowing diacritic information to be embedded directly within the inputs vector representation through a comprehensive evaluation across various configurations we demonstrate that the system effectively performs diacritization without relying on complex explicit linguistic analysis notably in an oracle setting where the correct diacritized form is guaranteed to be among the provided candidates divrit achieves a high level of accuracy furthermore strategic architectural enhancements and optimized training methodologies yield significant improvements in the systems overall generalization capabilities these findings highlight the promising potential of visual representations for accurate and automated hebrew diacritization
human smuggling networks are increasingly adaptive and difficult to analyze legal case documents offer critical insights but are often unstructured lexically dense and filled with ambiguous or shifting references which pose significant challenges for automated knowledge graph kg construction while recent llmbased approaches improve over static templates they still generate noisy fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution the recently proposed corekg framework addresses these limitations by integrating a typeaware coreference module and domainguided structured prompts significantly reducing node duplication and legal noise in this work we present a systematic ablation study of corekg to quantify the individual contributions of its two key components our results show that removing coreference resolution results in a increase in node duplication and a increase in noisy nodes while removing structured prompts leads to a increase in node duplication and a increase in noisy nodes these findings offer empirical insights for designing robust llmbased pipelines for extracting structured representations from complex legal texts
purpose the purpose of this study was to determine if an ensemble of multiple llm agents could be used collectively to provide a more reliable assessment of a pixelbased ai triage tool than a single llm methods noncontrast ct head exams from fourteen hospitals were processed by a commercial intracranial hemorrhage ich ai detection tool radiology reports were analyzed by an ensemble of eight opensource llm models and a hipaa compliant internal version of gpto using a single multishot prompt that assessed for presence of ich examples were manually reviewed performance characteristics of the eight opensource models and consensus were compared to gpto three ideal consensus llm ensembles were tested for rating the performance of the triage tool results the cohort consisted of head cts examreport pairs the highest auc performance was achieved with llamab and gpto auc the average precision was highest for llamab and gpto ap llamab had the highest f score and recall greater precision specificity and mcc using mcc ci the ideal combination of llms were full ensemble top ensemble consensus and gpto no statistically significant differences were observed between top full and consensus p conclusion an ensemble of medium to large sized opensource llms provides a more consistent and reliable method to derive a ground truth retrospective evaluation of a clinical ai triage tool over a single llm alone
large language models llms continue to advance with an increasing number of domainspecific variants tailored for specialised tasks however these models often lack transparency and explainability can be costly to finetune require substantial prompt engineering yield inconsistent results across domains and impose significant adverse environmental impact due to their high computational demands to address these challenges we propose the bayesian network llm fusion bnlf framework which integrates predictions from three llms including finbert roberta and bertweet through a probabilistic mechanism for sentiment analysis bnlf performs late fusion by modelling the sentiment predictions from multiple llms as probabilistic nodes within a bayesian network evaluated across three humanannotated financial corpora with distinct linguistic and contextual characteristics bnlf demonstrates consistent gains of about six percent in accuracy over the baseline llms underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion for interpretable sentiment classification
large language models llms have demonstrated remarkable proficiency in language comprehension and generation however their widespread adoption is constrained by substantial bandwidth and computational demands while pruning and lowrank imation have each demonstrated promising performance individually their synergy for llms remains underexplored we introduce underlinesynergistic underlinesparse and underlinelowrank underlinecompression sslc methods for llms which leverages the strengths of both techniques lowrank imation compresses the model by retaining its essential structure with minimal information loss whereas sparse optimization eliminates nonessential weights preserving those crucial for generalization based on theoretical analysis we first formulate the lowrank imation and sparse optimization as a unified problem and solve it by iterative optimization algorithm experiments on llama and qwen models bb show that sslc without any additional training steps consistently surpasses standalone methods achieving stateofthearts results notably sslc compresses qwen by with no performance drop and achieves at least times speedup offering a practical solution for efficient llm deployment
with the rapid development of large language models llms various llmbased works have been widely applied in educational fields however most existing llms and their benchmarks focus primarily on the knowledge dimension largely neglecting the evaluation of cultivation capabilities that are essential for realworld educational scenarios additionally current benchmarks are often limited to a single subject or question type lacking sufficient diversity this issue is particularly prominent within the chinese context to address this gap we introduce omniedubench a comprehensive chinese educational benchmark omniedubench consists of k highquality questionanswer pairs the data is meticulously divided into two core dimensions the knowledge dimension and the cultivation dimension which contain k and k entries respectively each dimension is further subdivided into finegrained categories covering a total of different subjects in the knowledge and in the cultivation furthermore the dataset features a rich variety of question formats including common exam question types providing a solid foundation for comprehensively evaluating llms capabilities in education extensive experiments on mainstream opensource and closedsource llms reveal a clear performance gap in the knowledge dimension only gemini pro surpassed accuracy while in the cultivation dimension the bestperforming model qwq still trailed human intelligence by nearly these results highlight the substantial room for improvement and underscore the challenges of applying llms in education
with the increasing use of generative artificial intelligence ai methods to support science workflows we are interested in the use of discourselevel information to find supporting evidence for ai generated scientific claims a first step towards this objective is to examine the task of inferring discourse structure in scientific writing in this work we present a preliminary investigation of pretrained language model plm and large language model llm approaches for discourse relation classification drc focusing on scientific publications an understudied genre for this task we examine how context can help with the drc task with our experiments showing that context as defined by discourse structure is generally helpful we also present an analysis of which scientific discourse relation types might benefit most from context
while a multiagent approach based on large language models llms represents a promising strategy to surpass the capabilities of single models its success is critically dependent on synergistic team composition however forming optimal teams is a significant challenge as the inherent opacity of most models obscures the internal characteristics necessary for effective collaboration in this paper we propose an interactioncentric framework for automatic team composition that does not require any prior knowledge including their internal architectures training data or task performances our method constructs a language model graph that maps relationships between models from the semantic coherence of pairwise conversations and then applies community detection to identify synergistic model clusters our experiments with diverse llms demonstrate that the proposed method discovers functionally coherent groups that reflect their latent specializations priming conversations with specific topics identified synergistic teams which outperform random baselines on downstream benchmarks and achieve comparable accuracy to that of manuallycurated teams based on known model specializations our findings provide a new basis for the automated design of collaborative multiagent llm teams
healthrelated misinformation is very prevalent and potentially harmful it is difficult to identify especially when claims distort or misinterpret scientific findings we investigate the impact of synthetic data generation and lightweight finetuning techniques on the ability of large language models llms to recognize fallacious arguments using the missci dataset and framework in this work we propose missynth a pipeline that applies retrievalaugmented generation rag to produce synthetic fallacy samples which are then used to finetune an llm model our results show substantial accuracy gains with finetuned models compared to vanilla baselines for instance the llama b finetuned model achieved an over fscore absolute improvement on the missci test split over its vanilla baseline we demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zeroshot llm classification performance on realworld scientific misinformation tasks even with limited computational resources the code and synthetic dataset are available on
large language models llms excel at general tasks but underperform in specialized domains like economics and psychology which require deep principled understanding to address this we introduce acer automated curriculumenhanced regimen that transforms generalist models into domain experts without sacrificing their broad capabilities acer first synthesizes a comprehensive textbookstyle curriculum by generating a table of contents for a subject and then creating questionanswer qa pairs guided by blooms taxonomy this ensures systematic topic coverage and progressively increasing difficulty the resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule aligning learning across both content and cognitive dimensions experiments with llama b and b show significant gains in specialized mmlu subsets in challenging domains like microeconomics where baselines struggle acer boosts accuracy by percentage points across all target domains we observe a consistent macroaverage improvement of percentage points notably acer not only prevents catastrophic forgetting but also facilitates positive crossdomain knowledge transfer improving performance on nontarget domains by points beyond mmlu acer enhances performance on knowledgeintensive benchmarks like arc and gpqa by over absolute points while maintaining stable performance on general reasoning tasks our results demonstrate that acer offers a scalable and effective recipe for closing critical domain gaps in llms
language models can be used to provide interactive personalized student feedback in educational settings however realworld deployment faces three key challenges privacy concerns limited computational resources and the need for pedagogically valid responses these constraints require small opensource models that can run locally and reliably ground their outputs in correct information we introduce scribe a framework for multihop toolaugmented reasoning designed to generate valid responses to student questions about feedback reports scribe combines domainspecific tools with a selfreflective inference pipeline that supports iterative reasoning tool use and error recovery we distil these capabilities into b and b models via twostage lora finetuning on synthetic gptogenerated data evaluation with a humanaligned gptjudge and a user study with students shows that bscribe models achieve comparable or superior quality to much larger models in key dimensions such as relevance and actionability while being perceived on par with gpto and llama b by students these findings demonstrate the viability of scribe for lowresource privacysensitive educational applications
openais chatgpt atlas introduces new capabilities for web interaction enabling the model to analyze webpages process user intents and execute cursor and keyboard inputs directly within the browser while its capacity for information retrieval tasks has been demonstrated its performance in dynamic interactive environments remains less explored in this study we conduct an early evaluation of atlass web interaction capabilities using browserbased games as test scenarios including googles trex runner sudoku flappy bird and we employ ingame performance scores as quantitative metrics to assess performance across different task types our results show that atlas performs strongly in logical reasoning tasks like sudoku completing puzzles significantly faster than human baselines but struggles substantially in realtime games requiring precise timing and motor control often failing to progress beyond initial obstacles these findings suggest that while atlas demonstrates capable analytical processing there remain notable limitations in dynamic web environments requiring realtime interaction the website of our project can be found at
recent work has shown that different large language models llms converge to ilar and accurate input embedding representations for numbers these findings conflict with the documented propensity of llms to produce erroneous outputs when dealing with numeric information in this work we aim to explain this conflict by exploring how language models manipulate numbers and quantify the lower bounds of accuracy of these mechanisms we find that despite surfacing errors different language models learn interchangeable representations of numbers that are systematic highly accurate and universal across their hidden states and the types of input contexts this allows us to create universal probes for each llm and to trace information including the causes of output errors to specific layers our results lay a fundamental understanding of how pretrained llms manipulate numbers and outline the potential of more accurate probing techniques in addressed refinements of llms architectures
large language models llms commonly boost reasoning via sampleevaluateensemble decoders achieving label free gains without ground truth however prevailing strategies score candidates using only external outputs such as token probabilities entropies or self evaluations and these signals can be poorly calibrated after post training we instead analyze internal behavior based on neuron activations and uncover three findings external signals are low dimensional projections of richer internal dynamics correct responses activate substantially fewer unique neurons than incorrect ones throughout generation and activations from correct responses exhibit stronger cross sample agreement whereas incorrect ones diverge motivated by these observations we propose neuron agreement decoding nad an unsupervised bestofn method that selects candidates using activation sparsity and cross sample neuron agreement operating solely on internal signals and without requiring comparable textual outputs nad enables early correctness prediction within the first generated tokens and supports aggressive early stopping across math and science benchmarks with verifiable answers nad matches majority voting on open ended coding benchmarks where majority voting is inapplicable nad consistently outperforms avg by pruning unpromising trajectories early nad reduces token usage by with minimal loss in generation quality showing that internal signals provide reliable scalable and efficient guidance for label free ensemble decoding
visionlanguage models vlms exhibit uneven performance across languages a problem that is often exacerbated when the model size is reduced while knowledge distillation kd demonstrates promising results in transferring knowledge from larger to smaller vlms applying kd in multilingualism is an underexplored area this paper presents a controlled empirical study of kd behavior across five distillation approaches isolating their effects on crosslingual representation consistency and downstream performance stability under model compression we study five distillation formulations across clip and siglip and evaluate them on indomain retrieval and outofdomain visual qa we find that some configurations preserve or even improve multilingual retrieval robustness despite halving model size but others fail to maintain crosstask stability exposing designsensitive tradeoffs that aggregate accuracy alone does not reveal
throughout language history words are borrowed from one language to another and gradually become integrated into the recipients lexicon speakers can often differentiate these loanwords from native vocabulary particularly in bilingual communities where a dominant language continuously imposes lexical items on a minority language this paper investigates whether pretrained language models including large language models possess ilar capabilities for loanword identification we evaluate multiple models across languages despite explicit instructions and contextual information our results show that models perform poorly in distinguishing loanwords from native ones these findings corroborate previous evidence that modern nlp systems exhibit a bias toward loanwords rather than native equivalents our work has implications for developing nlp tools for minority languages and supporting language preservation in communities under lexical pressure from dominant languages
the ability to accurately interpret implied meanings plays a crucial role in human communication and language use and language models are also expected to possess this capability this study demonstrates that providing language models with pragmatic theories as prompts is an effective incontext learning approach for tasks to understand implied meanings specifically we propose an approach in which an overview of pragmatic theories such as gricean pragmatics and relevance theory is presented as a prompt to the language model guiding it through a stepbystep reasoning process to derive a final interpretation experimental results showed that compared to the baseline which prompts intermediate reasoning without presenting pragmatic theories shot chainofthought our methods enabled language models to achieve up to higher scores on pragmatic reasoning tasks furthermore we show that even without explaining the details of pragmatic theories merely mentioning their names in the prompt leads to a certain performance improvement around in larger models compared to the baseline
retrievalaugmented generation rag has emerged as a leading approach to reducing hallucinations in large language models llms current rag evaluation benchmarks primarily focus on what we call local rag retrieving relevant chunks from a small subset of documents to answer queries that require only localized understanding within specific text chunks however many realworld applications require a fundamentally different capability global rag which involves aggregating and analyzing information across entire document collections to derive corpuslevel insights for example what are the top most cited papers in in this paper we introduce globalqa the first benchmark specifically designed to evaluate global rag capabilities covering four core task types counting extremum queries sorting and topk extraction through systematic evaluation across different models and baselines we find that existing rag methods perform poorly on global tasks with the strongest baseline achieving only f score to address these challenges we propose globalrag a multitool collaborative framework that preserves structural coherence through chunklevel retrieval incorporates llmdriven intelligent filters to eliminate noisy documents and integrates aggregation modules for precise symbolic computation on the qwenb model globalrag achieves f compared to the strongest baselines f validating the effectiveness of our method
human feedback can alter language models in unpredictable and undesirable ways as practitioners lack a clear understanding of what feedback data encodes while prior work studies preferences over certain attributes eg length or sycophancy automatically extracting relevant features without prespecifying hypotheses remains challenging we introduce whats in my human feedback wimhf a method to explain feedback data using sparse autoencoders wimhf characterizes both the preferences a dataset is capable of measuring and the preferences that the annotators actually express across datasets wimhf identifies a small number of humaninterpretable features that account for the majority of the preference prediction signal achieved by blackbox models these features reveal a wide diversity in what humans prefer and the role of datasetlevel context for example users on reddit prefer informality and jokes while annotators in hhrlhf and prism disprefer them wimhf also surfaces potentially unsafe preferences such as that lmarena users tend to vote against refusals often in favor of toxic content the learned features enable effective data curation relabeling the harmful examples in arena yields large safety gains with no cost to general performance they also allow finegrained personalization on the community alignment dataset we learn annotatorspecific weights over subjective features that improve preference prediction wimhf provides a humancentered analysis method for practitioners to better understand and use preference data
while diffusion language models dlms enable finegrained refinement their practical controllability remains fragile we identify and formally characterize a central failure mode called update forgetting in which uniform and context agnostic updates induce token level fluctuations across timesteps erasing earlier semantic edits and disrupting the cumulative refinement process thereby degrading fluency and coherence as this failure originates in uniform and context agnostic updates effective control demands explicit token ordering we propose token timestep allocation tta which realizes soft and semantic token ordering via per token timestep schedules critical tokens are frozen early while uncertain tokens receive continued refinement this timestep based ordering can be instantiated as either a fixed policy or an adaptive policy driven by task signals thereby supporting a broad spectrum of refinement strategies because it operates purely at inference time it applies uniformly across various dlms and naturally extends to diverse supervision sources empirically tta improves controllability and fluency on sentiment control it yields more than percent higher accuracy and nearly halves perplexity using less than one fifth the steps in detoxification it lowers maximum toxicity versus and perplexity versus together these results demonstrate that softened ordering via timestep allocation is the critical lever for mitigating update forgetting and achieving stable and controllable diffusion text generation
current llm evaluations often rely on a single instruction template overlooking models sensitivity to instruction stylea critical aspect for realworld deployments we present rcscore a multidimensional framework quantifying how instruction formulation affects model responses by systematically transforming benchmark problems into multiple instruction styles rcscore reveals performance variations undetected by conventional metrics our experiments across ten llms on four reasoning benchmarks demonstrate that instruction style can shift accuracy by up to points we introduce crossresponse ilarity crs a method applying rcscore metrics to measure stylistic selfconsistency and establish its strong correlation with task accuracy suggesting consistency as a valuable proxy for model reliability additional findings show that deterministic decoding produces more stylistically stable outputs and model scale correlates positively with crossstyle consistency rcscore offers a principled approach to assess instruction robustness
we introduce ilaritydistancemagnitude sdm language models lms which are sequence prediction models finetuned to maximize the proportion of generations in the wellcalibrated highprobability region partitioned by a finallayer sdm activation layer used for binary classification of instructionfollowing we demonstrate that existing pretrained decoderonly transformer lms can be readily converted into sdm lms via supervised finetuning using the finallayer sdm activation layer during training to estimate a changeofbase for a supervised nexttoken loss over a contrastive input encoding scheme with additional hard negative examples generated online during training this results in reduced abstentions ie improved statistical efficiency compared to strong supervised baselines
large language models llms have significantly advanced generative applications in natural language processing nlp recent trends in model architectures revolve around efficient variants of transformers or statespacegatedrecurrent models ssms grms however prevailing ssmgrmbased methods often emulate only a single attention head potentially limiting their expressiveness in this work we propose mossnet a novel mixtureofstatespaceexperts architecture that emulates a linear multihead attention mha mossnet leverages a mixtureofexperts moe implementation not only in channelmixing multilayered perceptron mlp blocks but also in the timemixing ssm kernels to realize multiple attention heads extensive experiments on language modeling and downstream evaluations show that mossnet outperforms both transformer and ssmbased architectures of ilar model size and data budgets larger variants of mossnet trained on trillions of tokens further confirm its scalability and superior performance in addition realdevice profiling on a samsung galaxy s ultra and an nvidia a gpu demonstrate favorable runtime speed and resource usage compared to ilarly sized baselines our results suggest that mossnet is a compelling new direction for efficient highperforming recurrent llm architectures
this paper investigates the relationship between persuasion techniques pts and discourse relations drs by leveraging large language models llms and prompt engineering since no dataset annotated with both pts and drs exists we took the semeval task dataset labelled with pts as a starting point and developed llmbased classifiers to label each instance of the dataset with one of the pdtb level drs in total four llms were evaluated using different prompts resulting in unique dr classifiers ensemble models using different majoritypooling strategies were used to create silver datasets of instances labelled with both persuasion techniques and level pdtb senses the silver dataset sizes vary from instances to instances depending on the majority pooling technique used statistical analysis of these silver datasets shows that six discourse relations namely cause purpose contrast causebelief concession and condition play a crucial role in persuasive texts especially in the use of loaded language exaggerationminimisation repetition and to cast doubt this insight can contribute to detecting online propaganda and misinformation as well as to our general understanding of effective communication
while testtime scaling tts has proven effective in improving the reasoning ability of large language models llms low diversity in model outputs often becomes a bottleneck this is partly caused by the common one problem one solution ps training practice which provides a single canonical answer and can push models toward a narrow set of reasoning paths to address this we propose a one problem multiple solutions pns training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity a core challenge for pns is reliably measuring semantic differences between multistep chains of thought so we introduce reasoning path divergence rpd a steplevel metric that aligns and scores long chainofthought solutions to capture differences in intermediate reasoning using rpd we curate maximally diverse solution sets per problem and finetune qwenbbase experiments show that rpdselected training yields more varied outputs and higher passk with an average gain in pass over a strong ps baseline and a gain on aime demonstrating that pns further amplifies the effectiveness of tts our code is available at
large language models llms have increasingly been applied to automatic programming code generation this task can be viewed as a language generation task that bridges natural language human knowledge and programming logic however it remains underexplored in domains that require interaction with hardware devices such as quantum programming where human coders write python code that is executed on a quantum computer to address this gap we introduce qcoder benchmark an evaluation framework that assesses llms on quantum programming with feedback from ulated hardware devices our benchmark offers two key features first it supports evaluation using a quantum ulator environment beyond conventional python execution allowing feedback of domainspecific metrics such as circuit depth execution time and error classification which can be used to guide better generation second it incorporates humanwritten code submissions collected from real programming contests enabling both quantitative comparisons and qualitative analyses of llm outputs against humanwritten codes our experiments reveal that even advanced models like gpto achieve only around accuracy highlighting the difficulty of the benchmark in contrast reasoningbased models such as o reach up to accuracy outperforming averaged success rates of humanwritten codes we release the qcoder benchmark dataset and public evaluation api to support further research
importance incidental thyroid findings itfs are increasingly detected on imaging performed for nonthyroid indications their prevalence features and clinical consequences remain undefined objective to develop validate and deploy a natural language processing nlp pipeline to identify itfs in radiology reports and assess their prevalence features and clinical outcomes design setting and participants retrospective cohort of adults without prior thyroid disease undergoing thyroidcapturing imaging at mayo clinic sites from july to september a transformerbased nlp pipeline identified itfs and extracted nodule characteristics from image reports from multiple modalities and body regions main outcomes and measures prevalence of itfs downstream thyroid ultrasound biopsy thyroidectomy and thyroid cancer diagnosis logistic regression identified demographic and imagingrelated factors results among patients mean age sd years women had an itf of which were nodules itfs were more likely in women older adults those with higher bmi and when imaging was ordered by oncology or internal medicine compared with chest ct itfs were more likely via neck ct pet and nuclear medicine scans nodule characteristics were poorly documented with size reported in and other features in fewer than eg calcifications compared with patients without itfs those with itfs had higher odds of thyroid nodule diagnosis biopsy thyroidectomy and thyroid cancer diagnosis most cancers were papillary and larger when detected after itfs vs no itf conclusions itfs were common and strongly associated with cascades leading to the detection of small lowrisk cancers these findings underscore the role of itfs in thyroid cancer overdiagnosis and the need for standardized reporting and more selective followup
crosslingual alignment cla aims to align multilingual representations enabling large language models llms to seamlessly transfer knowledge across languages while intuitive we hypothesize this pursuit of representational convergence can inadvertently cause cultural erasure the functional loss of providing culturallysituated responses that should diverge based on the query language in this work we systematically analyze this tradeoff by introducing a holistic evaluation framework the transferlocalization plane which quantifies both desirable knowledge transfer and undesirable cultural erasure using this framework we reevaluate recent cla approaches and find that they consistently improve factual transfer at the direct cost of cultural localization across all six languages studied our investigation into the internal representations of these models reveals a key insight universal factual transfer and culturallyspecific knowledge are optimally steerable at different model layers based on this finding we propose surgical steering a novel inferencetime method that disentangles these two objectives by applying targeted activation steering to distinct layers our approach achieves a better balance between the two competing dimensions effectively overcoming the limitations of current alignment techniques
current tooluse large language models llms are trained on static datasets enabling them to interact with external tools and perform multistep toolintegrated reasoning which produces toolcall trajectories however these models imitate how a query is resolved in a generic toolcall routine thereby failing to explore possible solutions and demonstrating limited performance in an evolved dynamic toolcall environment in this work we propose portool a reinforcement learning rl method that encourages a tooluse llm to explore various trajectories yielding the correct answer specifically this method starts with generating multiple rollouts for a given query and some of them share the first few toolcall steps thereby forming a treelike structure next we assign rewards to each step based on its ability to produce a correct answer and make successful tool calls a shared step across different trajectories receives the same reward while different steps under the same fork receive different rewards finally these stepwise rewards are used to calculate forkrelative advantages blended with trajectoryrelative advantages to train the llm for tool use the experiments utilize tools to address user queries covering both timesensitive and timeinvariant topics we conduct ablation studies to systematically justify the necessity and the design robustness of stepwise rewards furthermore we compare the proposed portool with other training approaches and demonstrate significant improvements in final accuracy and the number of toolcall steps
large language models llms often struggle with problems that require multistep reasoning for smallscale opensource models reinforcement learning with verifiable rewards rlvr fails when correct solutions are rarely sampled even after many attempts while supervised finetuning sft tends to overfit long demonstrations through rigid tokenbytoken imitation to address this gap we propose supervised reinforcement learning srl a framework that reformulates problem solving as generating a sequence of logical actions srl trains the model to generate an internal reasoning monologue before committing to each action it provides smoother rewards based on the ilarity between the models actions and expert actions extracted from the sft dataset in a stepwise manner this supervision offers richer learning signals even when all rollouts are incorrect while encouraging flexible reasoning guided by expert demonstrations as a result srl enables small models to learn challenging problems previously unlearnable by sft or rlvr moreover initializing training with srl before refining with rlvr yields the strongest overall performance beyond reasoning benchmarks srl generalizes effectively to agentic software engineering tasks establishing it as a robust and versatile training framework for reasoningoriented llms
large language models llms are widely used in generative applications such as chatting code generation and reasoning however many realworld workloads such as classification question answering recommendation and text embedding rely solely on the prefill stage of inference where the model encodes input sequences without performing autoregressive decoding in these prefill only scenarios the selfattention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length in this paper we observe that semantically different sentences often produce ilar attention maps across layers and heads building on this insight we propose attncache a framework that accelerates the prefill stage of llm inference by retrieving and reusing ilar attention maps based on an attention map memorization database attncache employs efficient caching and ilarity search techniques to identify and reuse precached attention maps during inference thereby reducing the computational overhead of selfattention experimental results show that attncache achieves an average of x endtoend and x attention speedup on cpu and x endtoend and x attention speedup on gpu with negligible accuracy degradation
ai accelerators customized to ai workloads provide costeffective and highperformance solutions for training and inference trainium an ai accelerator recently developed by amazon web services aws provides an attractive option for llm training and inference through its heterogeneous architecture however leveraging trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout in this paper we design highperformance matrix multiplication matmul a critical compute kernel for llm inference on trainium we introduce a series of techniques customized to trainium based on kernel fusion and novel caching strategies to reduce data movement across the softwaremanaged memory hierarchy maximize sram bandwidth and avoid expensive matrix transpose evaluating with nine datasets and four recent llms we show that our system largely outperforms the stateoftheart matmul implemented by aws on trainium at the level of matmul kernel it achieves an average x speedup up to x which translates to an average x speedup up to x for endtoend llm inference
large language models llms often struggle with complex mathematical reasoning where prosebased generation leads to unverified and arithmetically unsound solutions current prompting strategies like chain of thought still operate within this unreliable medium lacking a mechanism for deterministic verification to address these limitations we introduce symcode a neurosymbolic framework that reframes mathematical problemsolving as a task of verifiable code generation using the sympy library we evaluate symcode on challenging benchmarks including math and olympiadbench demonstrating significant accuracy improvements of up to percentage points over baselines our analysis shows that symcode is not only more tokenefficient but also fundamentally shifts model failures from opaque logical fallacies towards transparent programmatic errors by grounding llm reasoning in a deterministic symbolic engine symcode represents a key step towards more accurate and trustworthy ai in formal domains
machine translation mt is widely employed to address resource scarcity in lowresource languages by generating synthetic data from highresource counterparts while sentiment preservation in translation has long been studied a critical but underexplored factor is the role of cultural alignment between source and target languages in this paper we hypothesize that semantic labels are drifted or altered during mt due to cultural divergence through a series of experiments across culturally sensitive and neutral domains we establish three key findings mt systems including modern large language models llms induce label drift during translation particularly in culturally sensitive domains unlike earlier statistical mt tools llms encode cultural knowledge and leveraging this knowledge can amplify label drift and cultural ilarity or dissimilarity between source and target languages is a crucial determinant of label preservation our findings highlight that neglecting cultural factors in mt not only undermines label fidelity but also risks misinterpretation and cultural conflict in downstream applications
the impact of different multilingual data mixtures in pretraining large language models llms has been a topic of ongoing debate often raising concerns about potential tradeoffs between language coverage and model performance ie the curse of multilinguality in this work we investigate these assumptions by training b and b parameter llms on diverse multilingual corpora varying the number of languages from to our study challenges common beliefs surrounding multilingual training first we find that combining english and multilingual data does not necessarily degrade the inlanguage performance of either group provided that languages have a sufficient number of tokens included in the pretraining corpus second we observe that using english as a pivot language ie a highresource language that serves as a catalyst for multilingual generalization yields benefits across language families and contrary to expectations selecting a pivot language from within a specific family does not consistently improve performance for languages within that family lastly we do not observe a significant curse of multilinguality as the number of training languages increases in models at this scale our findings suggest that multilingual data when balanced appropriately can enhance language model capabilities without compromising performance even in lowresource settings
if we cannot inspect the training data of a large language model llm how can we ever know what it has seen we believe the most compelling evidence arises when the model itself freely reproduces the target content as such we propose recap an agentic pipeline designed to elicit and verify memorized training data from llm outputs at the heart of recap is a feedbackdriven loop where an initial extraction attempt is evaluated by a secondary language model which compares the output against a reference passage and identifies discrepancies these are then translated into minimal correction hints which are fed back into the target model to guide subsequent generations in addition to address alignmentinduced refusals recap includes a jailbreaking module that detects and overcomes such barriers we evaluate recap on echotrace a new benchmark spanning over full books and the results show that recap leads to substantial gains over singleiteration approaches for instance with gpt the average rougel score for the copyrighted text extraction improved from to a nearly increase
the use of llmbased applications as a means to accelerate andor substitute human labor in the creation of language resources and dataset is a reality nonetheless despite the potential of such tools for linguistic research comprehensive evaluation of their performance and impact on the creation of annotated datasets especially under a perspectivized approach to nlp is still missing this paper contributes to reduction of this gap by reporting on an extensive evaluation of the semiautomatization of framenetlike semantic annotation by the use of an llmbased semantic role labeler the methodology employed compares annotation time coverage and diversity in three experimental settings manual automatic and semiautomatic annotation results show that the hybrid semiautomatic annotation setting leads to increased frame diversity and ilar annotation coverage when compared to the humanonly setting while the automatic setting performs considerably worse in all metrics except for annotation time
posttraining of large language models llms is crucial for unlocking their task generalization potential and domainspecific capabilities however the current llm posttraining paradigm faces significant data challenges including the high costs of manual annotation and diminishing marginal returns on data scales therefore achieving dataefficient posttraining has become a key research question in this paper we present the first systematic survey of dataefficient llm posttraining from a datacentric perspective we propose a taxonomy of dataefficient llm posttraining methods covering data selection data quality enhancement synthetic data generation data distillation and compression and selfevolving data ecosystems we summarize representative approaches in each category and outline future research directions by examining the challenges in dataefficient llm posttraining we highlight open problems and propose potential research avenues we hope our work inspires further exploration into maximizing the potential of data utilization in largescale model training paper list
electronic health records ehr store clinical documentation as base encoded attachments in fhir documentreference resources which makes semantic question answering difficult traditional vector database methods often miss nuanced clinical relationships the clinical entity augmented retrieval clear method introduced by lopez et al uses entity aware retrieval and achieved improved performance with an f score of versus for embedding based retrieval while using over percent fewer tokens we developed a clinical notes qa evaluation platform to validate clear against zero shot large context inference and traditional chunk based retrieval augmented generation the platform was tested on clinical notes ranging from to tokens representing realistic ehr content clear achieved a percent win rate an average semantic ilarity of and used percent fewer tokens than wide context processing the largest performance gains occurred on long notes with a percent win rate for documents exceeding tokens these findings confirm that entity aware retrieval improves both efficiency and accuracy in clinical natural language processing the evaluation framework provides a reusable and transparent benchmark for assessing clinical question answering systems where semantic precision and computational efficiency are critical
there are two prevalent ways to constructing d scenes procedural generation and d lifting among them panoramabased d lifting has emerged as a promising technique leveraging powerful d generative priors to produce immersive realistic and diverse d environments in this work we advance this technique to generate graphicsready d scenes suitable for physically based rendering pbr relighting and ulation our key insight is to repurpose d generative models for panoramic perception of geometry textures and pbr materials unlike existing d lifting approaches that asize appearance generation and ignore the perception of intrinsic properties we present omnix a versatile and unified framework based on a lightweight and efficient crossmodal adapter structure omnix reuses d generative priors for a broad range of panoramic vision tasks including panoramic perception generation and completion furthermore we construct a largescale synthetic panorama dataset containing highquality multimodal panoramas from diverse indoor and outdoor scenes extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphicsready d scene generation opening new possibilities for immersive and physically realistic virtual world generation
we learn visual features by captioning images with an imageconditioned masked diffusion language model a formulation we call masked diffusion captioning mdc during training text tokens in each imagecaption pair are masked at a randomly chosen ratio and a decoder conditioned on visual features is trained to reconstruct the original text after training the learned visual features can be applied to downstream vision tasks unlike autoregressive captioning the strength of the visual learning signal in mdc does not depend on each tokens position in the sequence reducing the need for auxiliary objectives linear probing experiments across a variety of academicscale models and datasets show that the learned visual features are competitive with those produced by autoregressive and contrastive approaches
immersive applications call for synthesizing spatiotemporal d content from casual videos without costly d supervision existing videotod methods typically rely on manually annotated camera poses which are laborintensive and brittle for inthewild footage recent warptheninpaint approaches mitigate the need for pose labels by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions thereby depicting the d scene from diverse viewpoints however this trajectorytotrajectory formulation often entangles camera motion with scene dynamics and complicates both modeling and inference we introduce seed a posefree trajectorytocamera framework that replaces explicit trajectory prediction with rendering to a bank of fixed virtual cameras thereby separating camera control from scene modeling a viewconditional video inpainting model is trained to learn a robust geometry prior by denoising realistically synthesized warped images and to inpaint occluded or missing regions across virtual viewpoints eliminating the need for explicit d annotations building on this inpainting core we design a spatiotemporal autoregressive inference pipeline that traverses virtualcamera splines and extends videos with overlapping windows enabling coherent generation at bounded perstep complexity we validate seed on crossview video generation and sparse reconstruction benchmarks across quantitative metrics and qualitative assessments our method achieves superior generalization and improved performance relative to pose or trajectoryconditioned baselines advancing practical d world modeling from casual videos
determining the precise geographic location of an image at a global scale remains an unsolved challenge standard image retrieval techniques are inefficient due to the sheer volume of images m and fail when coverage is insufficient scalable solutions however involve a tradeoff global classification typically yields coarse results kilometers while crossview retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions this paper introduces a hybrid approach that achieves finegrained geolocalization across a large geographic expanse the size of a continent we leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information we combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of groundlevel data this enables direct finegrained retrieval over areas spanning multiple countries our extensive evaluation demonstrates that our approach can localize within m more than of queries of a dataset covering a large part of europe the code is publicly available at
despite recent advances in d human motion generation mogen on standard benchmarks existing models still face a fundamental bottleneck in their generalization capability in contrast adjacent generative fields most notably video generation vigen have demonstrated remarkable generalization in modeling human behaviors highlighting transferable insights that mogen can leverage motivated by this observation we present a comprehensive framework that systematically transfers knowledge from vigen to mogen across three key pillars data modeling and evaluation first we introduce vimogenk a largescale dataset comprising highquality motion samples that integrates highfidelity optical mocap data with semantically annotated motions from web videos and synthesized samples generated by stateoftheart vigen models the dataset includes both textmotion pairs and textvideomotion triplets substantially expanding semantic diversity second we propose vimogen a flowmatchingbased diffusion transformer that unifies priors from mocap data and vigen models through gated multimodal conditioning to enhance efficiency we further develop vimogenlight a distilled variant that eliminates video generation dependencies while preserving strong generalization finally we present mbench a hierarchical benchmark designed for finegrained evaluation across motion quality prompt fidelity and generalization ability extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations the code data and benchmark will be made publicly available
hierarchical structures of motion exist across research fields including computer vision graphics and robotics where complex dynamics typically arise from coordinated interactions among pler motion components existing methods to model such dynamics typically rely on manuallydefined or heuristic hierarchies with fixed motion primitives limiting their generalizability across different tasks in this work we propose a general hierarchical motion modeling method that learns structured interpretable motion relationships directly from data our method represents observed motions using graphbased hierarchies explicitly decomposing global absolute motions into parentinherited patterns and local motion residuals we formulate hierarchy inference as a differentiable graph learning problem where vertices represent elemental motions and directed edges capture learned parentchild dependencies through graph neural networks we evaluate our hierarchical reconstruction approach on three examples d translational motion d rotational motion and dynamic d scene deformation via gaussian splatting experimental results show that our method reconstructs the intrinsic motion hierarchy in d and d cases and produces more realistic and interpretable deformations compared to the baseline on dynamic d gaussian splatting scenes by providing an adaptable datadriven hierarchical modeling paradigm our method offers a formulation applicable to a broad range of motioncentric tasks project page
charts play an important role in visualization reasoning data analysis and the exchange of ideas among humans however existing visionlanguage models vlms still lack accurate perception of details and struggle to extract finegrained structures from charts such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them in this paper we introduce a novel chartalign benchmark chartab to provide a comprehensive evaluation of vlms in chart grounding tasks ie extracting tabular data localizing visualization elements and recognizing various attributes from charts of diverse types and complexities we design a json template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task by incorporating a novel twostage inference workflow the benchmark can further evaluate vlms capability to align and compare elementsattributes across two charts our analysis of evaluations on several recent vlms reveals new insights into their perception biases weaknesses robustness and hallucinations in chart understanding these findings highlight the finegrained discrepancies among vlms in chart understanding tasks and point to specific skills that need to be strengthened in current models
agerelated macular degeneration amd is one of the leading causes of irreversible vision impairment in people over the age of this research focuses on semantic segmentation for amd lesion detection in rgb fundus images a noninvasive and costeffective imaging technique the results of the adam challenge the most comprehensive amd detection from rgb fundus images research competition and open dataset to date serve as a benchmark for our evaluation taking the unet connectivity as a base of our framework we evaluate and compare several approaches to improve the segmentation models architecture and training pipeline including preprocessing techniques encoder backbone deep network types of varying complexity and specialized loss functions to mitigate class imbalances on image and pixel levels the main outcome of this research is the final configuration of the amd detection framework which outperforms all the prior adam challenge submissions on the multiclass segmentation of different amd lesion types in noninvasive rgb fundus images the source code used to conduct the experiments presented in this paper is made freely available
this work introduces steervlm a lightweight steering module designed to guide visionlanguage models vlms towards outputs that better adhere to desired instructions our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context this allows for finegrained inferencetime control over complex output semantics without modifying model weights while preserving performance on offtarget tasks our steering module requires learning parameters equal to of the original vlms size our steering module gains model control through dimensionwise activation modulation and adaptive steering across layers without requiring preextracted static vectors or manual tuning of intervention points furthermore we introduce vnia visual narrative intent alignment a multimodal dataset specifically created to facilitate the development and evaluation of vlm steering techniques our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for vlms and proposes a robust solution for multimodal model control through activation engineering
since its introduction d gaussian splatting dgs has rapidly transformed the landscape of d scene representations inspiring an extensive body of associated research followup work includes analyses and contributions that enhance the efficiency scalability and realworld applicability of dgs in this summary we present an overview of several key directions that have emerged in the wake of dgs we highlight advances enabling resourceefficient training and rendering the evolution toward dynamic or fourdimensional dgs representations and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process furthermore we examine efforts to bring dgs to mobile and virtual reality platforms its extension to massivescale environments and recent progress toward nearinstant radiance field reconstruction via feedforward or distributed computation collectively these developments illustrate how dgs has evolved from a breakthrough representation into a versatile and foundational tool for d vision and graphics
we present a longterm deployment study of a machine visionbased anomaly detection system for failure prediction in a steel rolling mill the system integrates industrial cameras to monitor equipment operation alignment and hot bar motion in real time along the process line live video streams are processed on a centralized video server using deep learning models enabling early prediction of equipment failures and process interruptions thereby reducing unplanned breakdown costs serverbased inference minimizes the computational load on industrial process control systems plcs supporting scalable deployment across production lines with minimal additional resources by jointly analyzing sensor data from data acquisition systems and visual inputs the system identifies the location and probable root causes of failures providing actionable insights for proactive maintenance this integrated approach enhances operational reliability productivity and profitability in industrial manufacturing environments
the presence of occlusions has provided substantial challenges to typicallypowerful object recognition algorithms additional sources of information can be extremely valuable to reduce errors caused by occlusions scene context is known to aid in object recognition in biological vision in this work we attempt to add robustness into existing region proposal networkdeep convolutional neural network rpndcnn object detection networks through two distinct scenebased information fusion techniques we present one algorithm under each methodology the first operates prior to prediction selecting a custom object network to use based on the identified background scene and the second operates after detection fusing scene knowledge into initial object scores output by the rpn we demonstrate our algorithms on challenging datasets featuring partial occlusions which show overall improvement in both recall and precision against baseline methods in addition our experiments contrast multiple training methodologies for occlusion handling finding that training on a combination of both occluded and unoccluded images demonstrates an improvement over the others our method is interpretable and can easily be adapted to other datasets offering many future directions for research and practical applications
accurate estimation of sea ice drift is critical for arctic navigation climate research and operational forecasting while optical flow a computer vision technique for estimating pixel wise motion between consecutive images has advanced rapidly in computer vision its applicability to geophysical problems and to satellite sar imagery remains underexplored classical optical flow methods rely on mathematical models and strong assumptions about motion which limit their accuracy in complex scenarios recent deep learning based approaches have substantially improved performance and are now the standard in computer vision motivating their application to sea ice drift estimation we present the first large scale benchmark of deep learning optical flow models on radarsat scansar sea ice imagery evaluated with endpoint error epe and fl all metrics against gnss tracked buoys several models achieve sub kilometer accuracy epe to pixels to m a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the arctic our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods which have substantially improved motion estimation accuracy compared to classical methods can be effectively transferred to polar remote sensing optical flow produces spatially continuous drift fields providing motion estimates for every image pixel rather than at sparse buoy locations offering new opportunities for navigation and climate modeling
autonomous vehicles avs are transforming the future of transportation through advances in intelligent perception decisionmaking and control systems however their success is tied to one core capability reliable object detection in complex and multimodal environments while recent breakthroughs in computer vision cv and artificial intelligence ai have driven remarkable progress the field still faces a critical challenge as knowledge remains fragmented across multimodal perception contextual reasoning and cooperative intelligence this survey bridges that gap by delivering a forwardlooking analysis of object detection in avs asizing emerging paradigms such as visionlanguage models vlms large language models llms and generative ai rather than reexamining outdated techniques we begin by systematically reviewing the fundamental spectrum of av sensors camera ultrasonic lidar and radar and their fusion strategies highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in llmvlmdriven perception frameworks next we introduce a structured categorization of av datasets that moves beyond ple collections positioning egovehicle infrastructurebased and cooperative datasets eg vv vi vx ii followed by a crossanalysis of data structures and characteristics ultimately we analyze cuttingedge detection methodologies ranging from d and d pipelines to hybrid sensor fusion with particular attention to emerging transformerdriven approaches powered by vision transformers vits large and small language models slms and vlms by synthesizing these perspectives our survey delivers a clear roadmap of current capabilities open challenges and future opportunities
to address the challenges in uav object detection such as complex backgrounds severe occlusion dense small objects and varying lighting conditionsthis paper proposes ptdetr based on rtdetr a novel detection algorithm specifically designed for small objects in uav imagery in the backbone network we introduce the partiallyaware detail focus padf module to enhance feature extraction for small objects additionallywe design the medianfrequency feature fusion mfff modulewhich effectively improves the models ability to capture smallobject details and contextual information furthermorewe incorporate focalersiou to strengthen the models bounding box matching capability and increase its sensitivity to smallobject features thereby further enhancing detection accuracy and robustness compared with rtdetr our ptdetr achieves map improvements of and on the visdrone dataset with lower computational complexity and fewer parameters demonstrating its robustness and feasibility for smallobject detection tasks
we propose tokenization of events and present a tokenizer spiking patches specifically designed for event cameras given a stream of asynchronous and spatially sparse events our goal is to discover an event representation that preserves these properties prior works have represented events as frames or as voxels however while these representations yield high accuracy both frames and voxels are synchronous and decrease the spatial sparsity spiking patches gives the means to preserve the unique properties of event cameras and we show in our experiments that this comes without sacrificing accuracy we evaluate our tokenizer using a gnn pcn and a transformer on gesture recognition and object detection tokens from spiking patches yield inference times that are up to x faster than voxelbased tokens and up to x faster than frames we achieve this while matching their accuracy and even surpassing in some cases with absolute improvements up to for gesture recognition and up to for object detection thus tokenization constitutes a novel direction in eventbased vision and marks a step towards methods that preserve the properties of event cameras
accurate and timely crop yield prediction is crucial for global food security and modern agricultural management traditional methods often lack the scalability and granularity required for precision farming this paper introduces cypress crop yield prediction via regression on prithvis encoder for satellite sensing a deep learning model designed for highresolution intrafield canola yield prediction cypress leverages a pretrained largescale geospatial foundation model prithvieom and adapts it for a continuous regression task transforming multitemporal satellite imagery into dense pixellevel yield maps evaluated on a comprehensive dataset from the canadian prairies cypress demonstrates superior performance over existing deep learningbased yield prediction models highlighting the effectiveness of finetuning foundation models for specialized agricultural applications by providing a continuous highresolution output cypress offers a more actionable tool for precision agriculture than conventional classification or countylevel aggregation methods this work validates a novel approach that bridges the gap between largescale earth observation and onfarm decisionmaking offering a scalable solution for detailed agricultural monitoring
computational superresolution csr in fluorescence microscopy has despite being an illposed problem a long history at its very core csr is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the imagegenerating microscope it stands to reason that with the advent of better datadriven machine learning techniques stronger prior can be learned and hence csr can lead to better results here we present resmatching a novel csr method that uses guided conditional flow matching to learn such improved datapriors we evaluate resmatching on diverse biological structures from the biosr dataset and compare its results against baselines resmatching consistently achieves competitive results demonstrating in all cases the best tradeoff between data fidelity and perceptual realism we observe that csr using resmatching is particularly effective in cases where a strong prior is hard to learn eg when the given lowresolution images contain a lot of noise additionally we show that resmatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested usecases enabling our method to deliver a pixelwise datauncertainty term that can guide future users to reject uncertain predictions
we introduce emu a largescale multimodal world model that natively predicts the next state across vision and language emu is pretrained endtoend with a unified nexttoken prediction objective on a corpus of visionlanguage interleaved data containing over trillion tokens primarily derived from sequential frames and transcripts of internet videos the model naturally accepts interleaved visionlanguage inputs and generates interleaved visionlanguage outputs emu is further posttrained with largescale reinforcement learning to enhance multimodal reasoning and generation to improve inference efficiency we propose discrete diffusion adaptation dida which converts tokenbytoken decoding into bidirectional parallel prediction accelerating perimage inference by about x without sacrificing performance emu exhibits strong native multimodal capabilities including longhorizon visionlanguage generation anytoimage xi generation and complex textrich image generation it also exhibits generalizable worldmodeling abilities enabling spatiotemporally consistent world exploration and openworld embodied manipulation across diverse scenarios and tasks for comparison emu achieves performance comparable to gemini flash image nano banana on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks we opensource emu at to support community research
recent advances in visual question answering vqa have demonstrated impressive performance in natural image domains with models like llava leveraging large language models llms for openended reasoning however their generalization degrades significantly when transferred to outofdomain scenarios such as remote sensing medical imaging or math diagrams due to large distributional shifts and the lack of effective domain adaptation mechanisms existing approaches typically rely on perdomain finetuning or bespoke pipelines which are costly inflexible and not scalable across diverse tasks in this paper we propose catch a plugandplay framework for crossdomain adaptation that improves the generalization of vqa models while requiring minimal changes to their core architecture our key idea is to decouple visual and linguistic adaptation by introducing two lightweight modules a domain classifier to identify the input image type and a dual adapter mechanism comprising a prompt adapter for language modulation and a visual adapter for vision feature adjustment both modules are dynamically injected via a unified hook interface requiring no retraining of the backbone model experimental results across four domainspecific vqa benchmarks demonstrate that our framework achieves consistent performance gains without retraining the backbone model including bleu on mathvqa vqa on medvqarad and rouge on chartqa these results highlight that catch provides a scalable and extensible approach to multidomain vqa enabling practical deployment across diverse application domains
in realworld environments ai systems often face unfamiliar scenarios without labeled data creating a major challenge for conventional scene understanding models the inability to generalize across unseen contexts limits the deployment of visionbased applications in dynamic unstructured settings this work introduces a dynamic contextaware scene reasoning framework that leverages visionlanguage alignment to address zeroshot realworld scenarios the goal is to enable intelligent systems to infer and adapt to new environments without prior taskspecific training the proposed approach integrates pretrained vision transformers and large language models to align visual semantics with natural language descriptions enhancing contextual comprehension a dynamic reasoning module refines predictions by combining global scene cues and objectlevel interactions guided by linguistic priors extensive experiments on zeroshot benchmarks such as coco visual genome and open images demonstrate up to improvement in scene understanding accuracy over baseline models in complex and unseen environments results also show robust performance in ambiguous or cluttered scenes due to the synergistic fusion of vision and language this framework offers a scalable and interpretable approach for contextaware reasoning advancing zeroshot generalization in dynamic realworld settings
advertisers commonly need multiple versions of the same advertisement ad at varying durations for a single campaign the traditional approach involves manually selecting and reediting shots from longer video ads to create shorter versions which is laborintensive and timeconsuming in this paper we introduce a framework for automated video ad clipping using video summarization techniques we are the first to frame video clipping as a shot selection problem tailored specifically for advertising unlike existing general video summarization methods that primarily focus on visual content our approach asizes the critical role of audio in advertising to achieve this we develop a twostream audiovisual fusion model that predicts the importance of video frames where importance is defined as the likelihood of a frame being selected in the firmproduced short ad to address the lack of adspecific datasets we present adsum a novel dataset comprising pairs of second and second ads from real advertising campaigns extensive experiments demonstrate that our model outperforms stateoftheart methods across various metrics including average precision area under curve spearman and kendall
spine segmentation based on ultrasound volume projection imaging vpi plays a vital role for intelligent scoliosis diagnosis in clinical applications however this task faces several significant challenges firstly the global contextual knowledge of spines may not be welllearned if we neglect the high spatial correlation of different bone features secondly the spine bones contain rich structural knowledge regarding their shapes and positions which deserves to be encoded into the segmentation process to address these challenges we propose a novel scaleadaptive structureaware network sanet for effective spine segmentation first we propose a scaleadaptive complementary strategy to learn the crossdimensional longdistance correlation features for spinal images second motivated by the consistency between multihead selfattention in transformers and semantic level affinity we propose structureaffinity transformation to transform semantic features with classspecific affinity and combine it with a transformer decoder for structureaware reasoning in addition we adopt a feature mixing loss aggregation method to enhance model training this method improves the robustness and accuracy of the segmentation process the experimental results demonstrate that our sanet achieves superior segmentation performance compared to other stateoftheart methods moreover the adaptability of sanet to various backbones enhances its potential as a promising tool for advanced scoliosis diagnosis using intelligent spinal image analysis the code and experimental demo are available at
the edge detection task is essential in image processing aiming to extract relevant information from an image one recurring problem in this task is the weaknesses found in some detectors such as the difficulty in detecting loose edges and the lack of context to extract relevant information from specific problems to address these weaknesses and adapt the detector to the properties of an image an adaptable detector described by twodimensional cellular automaton and optimized by metaheuristic combined with transfer learning techniques was developed this study aims to analyze the impact of expanding the search space of the optimization phase and the robustness of the adaptability of the detector in identifying edges of a set of natural images and specialized subsets extracted from the same image set the results obtained prove that expanding the search space of the optimization phase was not effective for the chosen image set the study also analyzed the adaptability of the model through a series of experiments and validation techniques and found that regardless of the validation the model was able to adapt to the input and the transfer learning techniques applied to the model showed no significant improvements
selfimprovement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large visionlanguage models lvlms where models explore and learn from successful trajectories iteratively however we identify a critical issue during this process the model excels at generating highquality trajectories for ple queries ie head data but struggles with more complex ones ie tail data this leads to an imbalanced optimization that drives the model to prioritize ple reasoning skills while hindering its ability to tackle more complex reasoning tasks over iterations this imbalance becomes increasingly pronounceda dynamic we term the matthew effectwhich ultimately hinders further model improvement and leads to performance bottlenecks to counteract this challenge we introduce four efficient strategies from two perspectives distributionreshaping and trajectoryresampling to achieve headtail rebalancing during the explorationandlearning selfimprovement process extensive experiments on qwenvlbinstruct and internvlb models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities outperforming vanilla selfimprovement by points on average
objectcontext shortcuts remain a persistent challenge in visionlanguage models undermining zeroshot reliability when testtime scenes differ from familiar training cooccurrences we recast this issue as a causal inference problem and ask would the prediction remain if the object appeared in a different environment to answer this at inference time we estimate object and background expectations within clips representation space and synthesize counterfactual embeddings by recombining object features with diverse alternative contexts sampled from external datasets batch neighbors or textderived descriptions by estimating the total direct effect and ulating intervention we further subtract backgroundonly activation preserving beneficial objectcontext interactions while mitigating hallucinated scores without retraining or prompt design our method substantially improves both worstgroup and average accuracy on contextsensitive benchmarks establishing a new zeroshot state of the art beyond performance our framework provides a lightweight representationlevel counterfactual approach offering a practical causal avenue for debiased and reliable multimodal reasoning
fewshot anomaly detection fsad methods identify anomalous regions with few known normal samples most existing methods rely on the generalization ability of pretrained visionlanguage models vlms to recognize potentially anomalous regions through feature ilarity between text descriptions and images however due to the lack of detailed textual descriptions these methods can only predefine imagelevel descriptions to match each visual patch token to identify potential anomalous regions which leads to the semantic misalignment between image descriptions and patchlevel visual anomalies achieving suboptimal localization performance to address the above issues we propose the multilevel finegrained semantic caption mfsc to provide multilevel and finegrained textual descriptions for existing anomaly detection datasets with automatic construction pipeline based on the mfsc we propose a novel framework named finegrainedad to improve anomaly localization performance which consists of two components multilevel learnable prompt mllp and multilevel semantic alignment mlsa mllp introduces finegrained semantics into multilevel learnable prompts through automatic replacement and concatenation mechanism while mlsa designs region aggregation strategy and multilevel alignment training to facilitate learnable prompts better align with corresponding visual regions experiments demonstrate that the proposed finegrainedad achieves superior overall performance in fewshot settings on mvtecad and visa datasets
recent advances in foundational d reconstruction models such as dustr and mastr have shown great potential in d and d correspondence in static scenes in this paper we propose to adapt them for the task of point tracking through d grounded correspondence we first demonstrate that these models are competitive point trackers when focusing on static points present in current point tracking benchmarks on egopoints vs cotracker we propose to combine the reconstruction loss with training for dynamic correspondence along with a visibility head and finetuning mastr for point tracking using a relatively small amount of synthetic data importantly we only train and evaluate on pairs of frames where one contains the query point effectively removing any temporal context using a mix of dynamic and static point correspondences we achieve competitive or superior point tracking results on four datasets eg competitive on tapviddavis deltaavg occlusion acc for pointstr compared to for cotracker and significantly outperform cotracker on egopoints vs and rgbs vs we also present results on d point tracking along with several ablations on training datasets and percentage of dynamic correspondences
testtime prompt tuning tpt has emerged as a promising technique for adapting large visionlanguage models vlms to unseen tasks without relying on labeled data however the lack of dispersion between textual features can hurt calibration performance which raises concerns about vlms reliability trustworthiness and safety current tpt approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation however these methods may not always have optimal angular separation between classwise textual features which implies overlooking the critical role of angular diversity to address this we propose atpt a novel tpt framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts this uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere we show that our approach consistently surpasses stateoftheart tpt methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets notably our approach exhibits superior zeroshot calibration performance on natural distribution shifts and generalizes well to medical datasets we provide extensive analyses including theoretical aspects to establish the grounding of atpt these results highlight the potency of promoting angular diversity to achieve welldispersed textual features significantly improving vlm calibration during testtime adaptation our code will be made publicly available
recently texttovideo generation has made impressive progress in producing short highquality clips but evaluating longform outputs remains a major challenge especially when processing complex prompts existing benchmarks mostly rely on plified prompts and focus on lowlevel metrics overlooking finegrained alignment with prompts and abstract dimensions such as narrative coherence and thematic expression to address these gaps we propose locotvbench a benchmark specifically designed for long video generation lvg under complex input conditions based on various realworld videos locotvbench introduces a suite of realistic and complex prompts incorporating elements like scene transitions and event dynamics moreover it constructs a multidimensional evaluation framework that includes our newly proposed metrics such as eventlevel alignment finegrained temporal consistency content clarity and the human expectation realization degree herd that focuses on more abstract attributes like narrative flow emotional response and character development using this framework we conduct a comprehensive evaluation of nine representative lvg models finding that while current methods perform well on basic visual and temporal aspects they struggle with interevent consistency finegrained alignment and highlevel thematic adherence etc overall locotvbench provides a comprehensive and reliable platform for evaluating longform complex texttovideo generation and highlights critical directions for future method improvement
existing eegdriven image reconstruction methods often overlook spatial attention mechanisms limiting fidelity and semantic coherence to address this we propose a dualconditioning framework that combines eeg embeddings with spatial saliency maps to enhance image generation our approach leverages the adaptive thinking mapper atm for eeg feature extraction and finetunes stable diffusion via lowrank adaptation lora to align neural signals with visual semantics while a controlnet branch conditions generation on saliency maps for spatial control evaluated on thingseeg our method achieves a significant improvement in the quality of low and highlevel image features over existing approaches ultaneously strongly aligning with human visual attention the results demonstrate that attentional priors resolve eeg ambiguities enabling highfidelity reconstructions with applications in medical diagnostics and neuroadaptive interfaces advancing neural decoding through efficient adaptation of pretrained diffusion models
image superresolutionsr is fundamental to many vision systemfrom surveillance and autonomy to document analysis and retail analyticsbecause recovering highfrequency details especially scenetext enables reliable downstream perception scenetext ie text embedded in natural images such as signs product labels and storefronts often carries the most actionable information when characters are blurred or hallucinated optical character recognitionocr and subsequent decisions fail even if the rest of the image appears sharp yet previous sr research has often been tuned to distortion psnrssim or learned perceptual metrics lipis maniqa clipiqa musiq that are largely insensitive to characterlevel errors furthermore studies that do address text sr often focus on plified benchmarks with isolated characters overlooking the challenges of text within complex natural scenes as a result scenetext is effectively treated as generic texture for sr to be effective in practical deployments it is therefore essential to explicitly optimize for both text legibility and perceptual quality we present glyphsr a visionlanguageguided diffusion framework that aims to achieve both objectives jointly glyphsr utilizes a textsr fusion controlnettscontrolnet guided by ocr data and a pingpong scheduler that alternates between text and scenecentric guidance to enable targeted text restoration we train these components on a synthetic corpus while keeping the main sr branch frozen across svt scutctw and cute at x and x glyphsr improves ocr f by up to percentage points over diffusiongan baseline svt x openocr while maintaining competitive maniqa clipiqa and musiq glyphsr is designed to satisfy both objectives ultaneouslyhigh readability and high visual realismdelivering sr that looks right and reds right
diabetic retinopathy dr is a leading cause of vision loss among middleaged and elderly people which significantly impacts their daily lives and mental health to improve the efficiency of clinical screening and enable the early detection of dr a variety of automated dr diagnosis systems have been recently established based on convolutional neural network cnn or vision transformer vit however due to the own shortages of cnn vit the performance of existing methods using singletype backbone has reached a bottleneck one potential way for the further improvements is integrating different kinds of backbones which can fully leverage the respective strengths of them ie the local feature extraction capability of cnn and the global feature capturing ability of vit to this end we propose a novel paradigm to effectively fuse the features extracted by different backbones based on the theory of evidence specifically the proposed evidential fusion paradigm transforms the features from different backbones into supporting evidences via a set of deep evidential networks with the supporting evidences the aggregated opinion can be accordingly formed which can be used to adaptively tune the fusion pattern between different backbones and accordingly boost the performance of our hybrid model we evaluated our method on two publicly available dr grading datasets the experimental results demonstrate that our hybrid model not only improves the accuracy of dr grading compared to the stateoftheart frameworks but also provides the excellent interpretability for feature fusion and decisionmaking
the subject of this work is to check how different types of music affect human emotions while listening to music a subjective survey and brain activity measurements were carried out using an eeg helmet the aim is to demonstrate the impact of different music genres on emotions the research involved a diverse group of participants of different gender and musical preferences this had the effect of capturing a wide range of emotional responses to music after the experiment a relationship analysis of the respondents questionnaires with eeg signals was performed the analysis revealed connections between emotions and observed brain activity
agile earth observation satellites aeoss constellations offer unprecedented flexibility for monitoring the earths surface but their scheduling remains challenging under largescale scenarios dynamic environments and stringent constraints existing methods often plify these complexities limiting their realworld performance we address this gap with a unified framework integrating a standardized benchmark suite and a novel scheduling model our benchmark suite aeosbench contains finely tuned satellite assets and scenarios each scenario features to satellites and to imaging tasks these scenarios are generated via a highfidelity ulation platform ensuring realistic satellite behavior such as orbital dynamics and resource constraints ground truth scheduling annotations are provided for each scenario to our knowledge aeosbench is the first largescale benchmark suite tailored for realistic constellation scheduling building upon this benchmark we introduce aeosformer a transformerbased scheduling model that incorporates a constraintaware attention mechanism a dedicated internal constraint module explicitly models the physical and operational limits of each satellite through ulationbased iterative learning aeosformer adapts to diverse scenarios offering a robust solution for aeos constellation scheduling experimental results demonstrate that aeosformer outperforms baseline models in task completion and energy efficiency with ablation studies highlighting the contribution of each component code and data are provided in
we focus on ocular biometrics specifically the periocular region the area around the eye which offers high discrimination and minimal acquisition constraints we evaluate three convolutional neural network architectures of varying depth and complexity to assess their effectiveness for periocular recognition the networks are trained on ocular crops extracted from the largescale vggface database this significantly contrasts with existing works which typically rely on smallscale periocular datasets for training having only a few thousand images experiments are conducted with ocular images from vggfacepose a subset of vggface containing inthewild face images and the ufprperiocular database which consists of selfies captured via mobile devices with user guidance on the screen due to the uncontrolled conditions of vggface the equal error rates eers obtained with ocular crops range from noticeably higher than the eers achieved using fullface images in contrast ufprperiocular yields significantly better performance eers of thanks to higher image quality and more consistent acquisition protocols to the best of our knowledge these are the lowest reported eers on the ufpr dataset to date
planning is a critical component of endtoend autonomous driving however prevailing imitation learning methods often suffer from mode collapse failing to produce diverse trajectory hypotheses meanwhile existing generative approaches struggle to incorporate crucial safety and physical constraints directly into the generative process necessitating an additional optimization stage to refine their outputs to address these limitations we propose catg a novel planning framework that leverages constrained flow matching concretely catg explicitly models the flow matching process which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals our primary contribution is the novel imposition of explicit constraints directly within the flow matching process ensuring that the generated trajectories adhere to vital safety and kinematic rules secondly catg parameterizes driving aggressiveness as a control signal during generation enabling precise manipulation of trajectory style notably on the navsim v challenge catg achieved nd place with an epdms score of and was honored with the innovation award
we study the complementarity of different cnns for periocular verification at different distances on the ubipr database we train three architectures of increasing complexity squeezenet mobilenetv and resnet on a large set of eye crops from vggface we analyse performance with cosine and chi metrics compare different network initialisations and apply scorelevel fusion via logistic regression in addition we use lime heatmaps and jensenshannon divergence to compare attention patterns of the cnns while resnet consistently performs best individually the fusion provides substantial gains especially when combining all three networks heatmaps show that networks usually focus on distinct regions of a given image which explains their complementarity our method significantly outperforms previous works on ubipr achieving a new stateoftheart
existing infrared and visible image fusion methods often face the dilemma of balancing modal information generative fusion methods reconstruct fused images by learning from data distributions but their generative capabilities remain limited moreover the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios this manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method termed hclfuse first hclfuse investigates the quantification theory of information mapping in unsupervised fusion networks which leads to the design of a multiscale maskregulated variational bottleneck encoder this encoder applies posterior probability modeling and information decomposition to extract accurate and concise lowlevel modal information thereby supporting the generation of highfidelity structural details furthermore the probabilistic generative capability of the diffusion model is integrated with physical laws forming a timevarying physical guidance mechanism that adaptively regulates the generation process at different stages thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality experimental results show that the proposed method achieves stateoftheart fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics this fully demonstrates the advantages of this generative image fusion method drawing inspiration from human cognition in enhancing structural consistency and detail quality
modern visionlanguage models vlms excel at many multimodal tasks yet their grasp of temporal information in video remains weak and crucially underevaluated we probe this gap with a deceptively ple but revealing challenge judging the arrow of time aotwhether a short clip is played forward or backward we introduce aotpsyphybench a psychophysically validated benchmark that tests whether vlms can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans our comprehensive evaluation of openweight and proprietary reasoning and nonreasoning vlms reveals that most models perform near chance and even the best lag far behind human accuracy on physically irreversible processes eg free fall diffusionexplosion and causal manual actions divisionaddition that humans recognize almost instantly these results highlight a fundamental gap in current multimodal systems while they capture rich visualsemantic correlations they lack the inductive biases required for temporal continuity and causal understanding we release the code and data for aotpsyphybench to encourage further progress in the physical and temporal reasoning capabilities of vlms
document ai has advanced rapidly and is attracting increasing attention yet while most efforts have focused on document layout analysis dla its generative counterpart document layout generation remains underexplored a major obstacle lies in the scarcity of diverse layouts academic papers with manhattanstyle structures dominate existing studies while openworld genres such as newspapers and magazines remain severely underrepresented to address this gap we curate omnilayoutm the first millionscale dataset of diverse document layouts covering six common document types and comprising contemporary layouts collected from multiple sources moreover since existing methods struggle in complex domains and often fail to arrange long sequences coherently we introduce omnilayoutllm a b model with designed twostage coarsetofine learning paradigm learning universal layout principles from omnilayoutm with coarse category definitions and transferring the knowledge to a specific domain with finegrained annotations extensive experiments demonstrate that our approach achieves strong performance on multiple domains in mdoc dataset substantially surpassing both existing layout generation experts and several latest generalpurpose llms our code models and dataset will be publicly released
the sustainability of supply chain plays a key role in achieving optimal performance in controlling the supply chain the management of risks that occur in a supply chain is a fundamental problem for the purpose of developing the sustainability of the network and elevating the performance efficiency of the supply chain the correct classification of products is another essential element in a sustainable supply chain acknowledging recent breakthroughs in the context of deep networks several architectural options have been deployed to analyze supply chain datasets a novel geometric deep network is used to propose an ensemble deep network the proposed chebyshev ensemble geometric network chegn is a hybrid convolutional and geometric deep learning this network is proposed to leverage the information dependencies in supply chain to derive invisible states of samples in the database the functionality of the proposed deep network is assessed on the two different databases the supplygraph dataset and dataco are considered in this research the prediction of delivery status of dataco supply chain is done for risk administration the product classification and edge classification are performed using the supplygraph database to enhance the sustainability of the supply network an average accuracy of is obtained for the ensemble network for risk management the average accuracy of and are obtained for sustainable supply chain in terms of product group classification and product relation classification respectively the average accuracy of is attained for company relation classification the results confirm an average improvement and efficiency of the proposed method compared to the stateoftheart approaches
d human pose estimation from sketches has broad applications in computer animation and film production unlike traditional human pose estimation this task presents unique challenges due to the abstract and disproportionate nature of sketches previous sketchtopose methods constrained by the lack of largescale sketchd pose annotations primarily relied on optimization with heuristic rulesan approach that is both timeconsuming and limited in generalizability to address these challenges we propose a novel approach leveraging a learn from synthesis strategy first a diffusion model is trained to synthesize sketch images from d poses projected from d human poses mimicking disproportionate human structures in sketches this process enables the creation of a synthetic dataset skepk consisting of k accurate sketchd pose annotation pairs across various sketch styles building on this synthetic dataset we introduce an endtoend datadriven framework for estimating human poses and shapes from diverse sketch styles our framework combines existing d pose detectors and generative diffusion priors for sketch feature extraction with a feedforward neural network for efficient d pose estimation multiple heuristic loss functions are incorporated to guarantee geometric coherence between the derived d poses and the detected d poses while preserving accurate selfcontacts qualitative quantitative and subjective evaluations collectively show that our model substantially surpasses previous ones in both estimation accuracy and speed for sketchtopose tasks
dataset bias where data points are skewed to certain concepts is ubiquitous in machine learning datasets yet systematically identifying these biases is challenging without costly finegrained attribute annotations we present conceptscope a scalable and automated framework for analyzing visual datasets by discovering and quantifying humaninterpretable concepts using sparse autoencoders trained on representations from vision foundation models conceptscope categorizes concepts into target context and bias types based on their semantic relevance and statistical correlation to class labels enabling classlevel dataset characterization bias identification and robustness evaluation through conceptbased subgrouping we validate that conceptscope captures a wide range of visual concepts including objects textures backgrounds facial attributes emotions and actions through comparisons with annotated datasets furthermore we show that concept activations produce spatial attributions that align with semantically meaningful image regions conceptscope reliably detects known biases eg background bias in waterbirds and uncovers previously unannotated ones eg cooccurring objects in imagenet offering a practical tool for dataset auditing and model diagnostics
accurate estimation of motion information is crucial in diverse computational imaging and computer vision applications researchers have investigated various methods to extract motion information from a single blurred image including blur kernels and optical flow however existing motion representations are often of low quality ie coarsegrained and inaccurate in this paper we propose the first highresolution hr motion trajectory estimation framework using diffusion models motdiff different from existing motion representations we aim to estimate an hr motion trajectory with highquality from a single motionblurred image the proposed motdiff consists of two key components a new conditional diffusion framework that uses multiscale feature maps extracted from a single blurred image as a condition and a new training method that can promote precise identification of a finegrained motion trajectory consistent estimation of overall shape and position of a motion path and pixel connectivity along a motion trajectory our experiments demonstrate that the proposed motdiff can outperform stateoftheart methods in both blind image deblurring and coded exposure photography applications
wearable devices such as smart glasses are transforming the way people interact with their surroundings enabling users to seek information regarding entities in their view multimodal retrievalaugmented generation mmrag plays a key role in supporting such questions yet there is still no comprehensive benchmark for this task especially regarding wearables scenarios to fill this gap we present cragmm a comprehensive rag benchmark for multimodal multiturn conversations cragmm contains a diverse set of k image question answer triplets and k visualbased multiturn conversations across domains including k egocentric images designed to mimic captures from wearable devices we carefully constructed the questions to reflect realworld scenarios and challenges including five types of imagequality issues six question types varying entity popularity differing information dynamism and different conversation turns we design three tasks singlesource augmentation multisource augmentation and multiturn conversations each paired with an associated retrieval corpus and apis for both imagekg retrieval and webpage retrieval our evaluation shows that straightforward rag approaches achieve only and truthfulness on cragmm single and multiturn qa respectively whereas stateoftheart industry solutions have ilar quality underscoring ample room for improvement the benchmark has hosted kdd cup attracting about k participants and k submissions with winning solutions improving baseline performance by highlighting its early impact on advancing the field
modes of transportation vary across countries depending on geographical location and cultural context in south asian countries rickshaws are among the most common means of local transport based on their mode of operation rickshaws in cities across bangladesh can be broadly classified into nonauto pedalpowered and autorickshaws motorized monitoring the movement of autorickshaws is necessary as traffic rules often restrict autorickshaws from accessing certain routes however existing surveillance systems make it quite difficult to monitor them due to their ilarity to other vehicles especially nonauto rickshaws whereas manual video analysis is too timeconsuming this paper presents a machine learningbased approach to automatically detect autorickshaws in traffic images in this system we used realtime object detection using the yolov model for training purposes we prepared a set of annotated images that were captured under various traffic conditions the results show that our proposed model performs well in realtime autorickshaw detection and offers an map of and binary precision and recall values above demonstrating its effectiveness in handling both dense and sparse traffic scenarios the dataset has been publicly released for further research
large annotated datasets are essential for training robust computeraided diagnosis cad models for breast cancer detection or risk prediction however acquiring such datasets with finedetailed annotation is both costly and timeconsuming visionlanguage models vlms such as clip which are pretrained on large imagetext pairs offer a promising solution by enhancing robustness and data efficiency in medical imaging tasks this paper introduces a novel multiview mammography and language model for breast cancer classification and risk prediction trained on a dataset of paired mammogram images and synthetic radiology reports our mvmlm leverages multiview supervision to learn rich representations from extensive radiology data by employing crossmodal selfsupervision across imagetext pairs this includes multiple views and the corresponding pseudoradiology reports we propose a novel joint visualtextual learning strategy to enhance generalization and accuracy performance over different data types and tasks to distinguish breast tissues or cancer characteristicscalcification mass and utilize these patterns to understand mammography images and predict cancer risk we evaluated our method on both private and publicly available datasets demonstrating that the proposed model achieves stateoftheart performance in three classification tasks malignancy classification subtype classification and imagebased cancer risk prediction furthermore the model exhibits strong data efficiency outperforming existing fully supervised or vlm baselines while trained on synthetic text reports and without the need for actual radiology reports
arbitraryscale video superresolution avsr aims to enhance the resolution of video frames potentially at various scaling factors which presents several challenges regarding spatial detail reproduction temporal consistency and computational complexity in this paper we propose a strong baseline basicavsr for avsr by integrating four key components adaptive multiscale frequency priors generated from image laplacian pyramids a flowguided propagation unit to aggregate spatiotemporal information from adjacent frames a secondorder motion compensation unit for more accurate spatial alignment of adjacent frames and a hyperupsampling unit to generate scaleaware and contentindependent upsampling kernels to meet diverse application demands we instantiate three propagation variants i a unidirectional rnn unit for strictly online inference ii a unidirectional rnn unit empowered with a limited lookahead that tolerates a small output delay and iii a bidirectional rnn unit designed for offline tasks where computational resources are less constrained experimental results demonstrate the effectiveness and adaptability of our model across these different scenarios through extensive experiments we show that basicavsr significantly outperforms existing methods in terms of superresolution quality generalization ability and inference speed our work not only advances the stateoftheart in avsr but also extends its core components to multiple frameworks for diverse scenarios the code is available at
large language models llms in psychological counseling have attracted increasing attention however existing approaches often lack emotional understanding adaptive strategies and the use of therapeutic methods across multiple sessions with longterm memory leaving them far from real clinical practice to address these critical gaps we introduce theramind a strategic and adaptive agent for longitudinal psychological counseling the cornerstone of theramind is a novel dualloop architecture that decouples the complex counseling process into an intrasession loop for tactical dialogue management and a crosssession loop for strategic therapeutic planning the intrasession loop perceives the patients emotional state to dynamically select response strategies while leveraging crosssession memory to ensure continuity crucially the crosssession loop empowers the agent with longterm adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions we validate our approach in a highfidelity ulation environment grounded in real clinical cases extensive evaluations show that theramind outperforms other methods especially on multisession metrics like coherence flexibility and therapeutic attunement validating the effectiveness of its dualloop design in emulating strategic adaptive and longitudinal therapeutic behavior the code is publicly available at
retrievalaugmented generation allows llms to access external knowledge reducing hallucinations and ageingdata issues however it treats retrieved chunks independently and struggles with multihop or relational reasoning especially across documents knowledge graphs enhance this by capturing the relationships between entities using triplets enabling structured multichunk reasoning however these tend to miss information that fails to conform to the triplet structure we introduce bambookg a knowledge graph with frequencybased weights on nontriplet edges which reflect link strength drawing on the hebbian principle of fire together wire together this decreases information loss and results in improved performance on single and multihop reasoning outperforming the existing solutions
unmanned aerial vehicles uavs are increasingly populating urban areas for delivery and surveillance purposes in this work we develop an optimal navigation strategy based on deep reinforcement learning the environment is represented by a threedimensional highfidelity ulation of an urban flow characterized by turbulence and recirculation zones the algorithm presented here is a flowaware proximal policy optimization ppo combined with a gated transformer extra large gtrxl architecture giving the agent richer information about the turbulent flow field in which it navigates the results are compared with a ppogtrxl without the secondary prediction tasks a ppo combined with long short term memory lstm cells and a traditional navigation algorithm the obtained results show a significant increase in the success rate sr and a lower crash rate cr compared to a ppolstm ppogtrxl and the classical zermelos navigation algorithm paving the way to a completely reimagined uav landscape in complex urban environments
visionlanguage models vlms excel at interpreting textrich images but struggle with long visually complex documents that demand analysis and integration of information spread across multiple pages existing approaches typically rely on fixed reasoning templates or rigid pipelines which force vlms into a passive role and hinder both efficiency and generalization we present active longdocument navigation alden a multiturn reinforcement learning framework that finetunes vlms as interactive agents capable of actively navigating long visually rich documents alden introduces a novel fetch action that directly accesses the page by index complementing the classic search action and better exploiting document structure for dense process supervision and efficient training we propose a rulebased crosslevel reward that provides both turn and tokenlevel signals to address the empirically observed training instability caused by numerous visual tokens from long documents we further propose a visualsemantic anchoring mechanism that applies a dualpath kldivergence constraint to stabilize visual and textual representations separately during training trained on a corpus constructed from three opensource datasets alden achieves stateoftheart performance on five longdocument benchmarks overall alden marks a step beyond passive document reading toward agents that autonomously navigate and reason across long visually rich documents offering a robust path to more accurate and efficient longdocument understanding
an agentic ai workflow aaw also known as an llmbased multiagent system is an autonomous system that assembles several llmbased agents to work collaboratively towards a shared goal the high autonomy widespread adoption and growing interest in such aaws highlight the need for a deeper understanding of their operations from both quality and security aspects to this day there are no existing methods to assess the influence of each agent on the aaws final output adopting techniques from related fields is not feasible since existing methods perform only static structural analysis which is unsuitable for inference time execution we present counterfactualbased agent influence ranker cair the first method for assessing the influence level of each agent on the aaws output and determining which agents are the most influential by performing counterfactual analysis cair provides a taskagnostic analysis that can be used both offline and at inference time we evaluate cair using an aaws dataset of our creation containing different use cases with different functionalities our evaluation showed that cair produces consistent rankings outperforms baseline methods and can easily enhance the effectiveness and relevancy of downstream tasks
the diagnosis of most mental disorders including psychiatric evaluations primarily depends on dialogues between psychiatrists and patients this subjective process can lead to variability in diagnoses across clinicians and patients resulting in inconsistencies and challenges in achieving reliable outcomes to address these issues and standardize psychiatric diagnoses we propose a finetuned large language model llm consortium and openaigptoss reasoning llmenabled decision support system for the clinical diagnosis of mental disorders our approach leverages finetuned llms trained on conversational datasets involving psychiatristpatient interactions focused on mental health conditions eg depression the diagnostic predictions from individual models are aggregated through a consensusbased decisionmaking process refined by the openaigptoss reasoning llm we propose a novel method for deploying llm agents that orchestrate communication between the llm consortium and the reasoning llm ensuring transparency reliability and responsible ai across the entire diagnostic workflow experimental results demonstrate the transformative potential of combining finetuned llms with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment a prototype of the proposed platform integrating three finetuned llms with the openaigptoss reasoning llm was developed in collaboration with the us army medical research team in norfolk virginia usa to the best of our knowledge this work represents the first application of a finetuned llm consortium integrated with a reasoning llm for clinical mental health diagnosis paving the way for nextgeneration aipowered ehealth systems aimed at standardizing psychiatric diagnoses
exploration is fundamental to reinforcement learning rl as it determines how effectively an agent discovers and exploits the underlying structure of its environment to achieve optimal performance existing exploration methods generally fall into two categories active exploration and passive exploration the former introduces stochasticity into the policy but struggles in highdimensional environments while the latter adaptively prioritizes transitions in the replay buffer to enhance exploration yet remains constrained by limited sample diversity to address the limitation in passive exploration we propose modelic generative exploration moge which augments exploration through the generation of underexplored critical states and synthesis of dynamicsconsistent experiences through transition models moge is composed of two components a diffusionbased generator that synthesizes critical states under the guidance of a utility function evaluating each states potential influence on policy exploration and a onestep imagination world model for constructing critical transitions based on the critical states for agent learning our method adopts a modular formulation that aligns with the principles of offpolicy learning allowing seamless integration with existing algorithms to improve exploration without altering their core structures empirical results on openai gym and deepmind control suite reveal that moge effectively bridges exploration and policy learning leading to remarkable gains in both sample efficiency and performance across complex control tasks
zero reinforcement learning zerorl has proven to be an effective approach for enhancing the reasoning capabilities of large language models llms by directly applying reinforcement learning with verifiable rewards on pretrained models without the need for a supervised finetuning phase however current research on zerorl primarily focuses on domains with easily verifiable reward signals such as mathematics programming and other reasoning tasks the challenge of eliciting reasoning abilities in more diverse scenarios where verification is not straightforward remains underexplored to address this gap we propose a novel zerorl paradigm designed to improve a models reasoning ability across both verifiable and nonverifiable domains by combining verifiable rewards with a generative reward model we conduct multitask zerorl training across both domains facilitating the transfer of reasoning capabilities between them furthermore to mitigate reward hacking in the generative reward model we design a smooth length penalty that encourages the generation of more comprehensive thinking tokens in general domains experimental results on qwenbbase and qwenbbase demonstrate that our approach achieves superior reasoning performance not only on tasks requiring extensive reasoning but also on more general tasks
retrievalaugmented generation rag systems often face limitations in specialized domains such as fintech where domainspecific ontologies dense terminology and acronyms complicate effective retrieval and synthesis this paper introduces an agentic rag architecture designed to address these challenges through a modular pipeline of specialized agents the proposed system supports intelligent query reformulation iterative subquery decomposition guided by keyphrase extraction contextual acronym resolution and crossencoderbased context reranking we evaluate our approach against a standard rag baseline using a curated dataset of questionanswerreference triples derived from an enterprise fintech knowledge base experimental results demonstrate that the agentic rag system outperforms the baseline in retrieval precision and relevance albeit with increased latency these findings suggest that structured multiagent methodologies offer a promising direction for enhancing retrieval robustness in complex domainspecific settings
in this paper we address the problem of giving names to predicates in logic rules using large language models llms in the context of inductive logic programming various rule generation methods produce rules containing unnamed predicates with predicate invention being a key example this hinders the readability interpretability and reusability of the logic theory leveraging recent advancements in llms development we explore their ability to process natural language and code to provide semantically meaningful suggestions for giving a name to unnamed predicates the evaluation of our approach on some handcrafted logic rules indicates that llms hold potential for this task
as large language models llms are increasingly used in texttosql tasks reinforcement learning rl has become a common method for improving performance existing methods primarily rely on static execution feedback which restricts realtime error correction however integrating multiturn tool invocation along with dynamic feedback could significantly improve adaptability and robustness ultimately enhancing model performance to address these issues we propose mtirsql an innovative multiturn toolintegrated reasoning reinforcement learning framework for texttosql our approach introduces an executionaware multiturn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step enabling contextsensitive query generation and progressive refinement throughout the reasoning process the framework extends the grpo algorithm to accommodate complex multiturn interaction scenarios considering the training instability characteristics of mtir and the potential for significant deviation of model distribution from the initial model we enhance the grpo algorithm by adding a trajectory filtering mechanism and removing kl loss constraints experimental results demonstrate that mtirsql with b parameters achieves textbf accuracy in the bird dev and execution accuracy in the spider dev significantly outperforming existing approaches
multiobjective search mos has emerged as a unifying framework for planning and decisionmaking problems where multiple often conflicting criteria must be balanced while the problem has been studied for decades recent years have seen renewed interest in the topic across ai applications such as robotics transportation and operations research reflecting the reality that realworld systems rarely optimize a single measure this paper surveys developments in mos while highlighting crossdisciplinary opportunities and outlines open challenges that define the emerging frontier of mos
in artificial intelligence ai alignment research instrumental goals also called instrumental subgoals or instrumental convergent goals are widely associated with advanced ai systems these goals which include tendencies such as powerseeking and selfpreservation become problematic when they conflict with human aims conventional alignment theory treats instrumental goals as sources of risk that become problematic through failure modes such as reward hacking or goal misgeneralization and attempts to limit the symptoms of instrumental goals notably resource acquisition and selfpreservation this article proposes an alternative framing that a philosophical argument can be constructed according to which instrumental goals may be understood as features to be accepted and managed rather than failures to be limited drawing on aristotles ontology and its modern interpretations an ontology of concrete goaldirected entities it argues that advanced ai systems can be seen as artifacts whose formal and material constitution gives rise to effects distinct from their designers intentions in this view the instrumental tendencies of such systems correspond to per se outcomes of their constitution rather than accidental malfunctions the implication is that efforts should focus less on eliminating instrumental goals and more on understanding managing and directing them toward humanaligned ends
agentic ai represents a transformative shift in artificial intelligence but its rapid advancement has led to a fragmented understanding often conflating modern neural systems with outdated symbolic models a practice known as conceptual retrofitting this survey cuts through this confusion by introducing a novel dualparadigm framework that categorizes agentic systems into two distinct lineages the symbolicclassical relying on algorithmic planning and persistent state and the neuralgenerative leveraging stochastic generation and promptdriven orchestration through a systematic prismabased review of studies we provide a comprehensive analysis structured around this framework across three dimensions the theoretical foundations and architectural principles defining each paradigm domainspecific implementations in healthcare finance and robotics demonstrating how application constraints dictate paradigm selection and paradigmspecific ethical and governance challenges revealing divergent risks and mitigation strategies our analysis reveals that the choice of paradigm is strategic symbolic systems dominate safetycritical domains eg healthcare while neural systems prevail in adaptive datarich environments eg finance furthermore we identify critical research gaps including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neurosymbolic architectures the findings culminate in a strategic roadmap arguing that the future of agentic ai lies not in the dominance of one paradigm but in their intentional integration to create systems that are both adaptable and reliable this work provides the essential conceptual toolkit to guide future research development and policy toward robust and trustworthy hybrid intelligent systems
a core challenge of monte carlo tree search mcts is its sample efficiency which can be improved by grouping stateaction pairs and using their aggregate statistics instead of singlenode statistics on the go abstractions in upper confidence bounds applied to trees ogauct is the stateoftheart mcts abstraction algorithm for deterministic environments that builds its abstraction using the abstractions of stateaction pairs asap framework which aims to detect states and stateaction pairs with the same value under optimal play by analysing the search graph asap however requires two stateaction pairs to have the same immediate reward which is a rigid condition that limits the number of abstractions that can be found and thereby the sample efficiency in this paper we break with the paradigm of grouping valueequivalent states or stateaction pairs and instead group states and stateaction pairs with possibly different values as long as the difference between their values can be inferred we call this abstraction framework known value difference abstractions kvda which infers the value differences by analysis of the immediate rewards and modifies ogauct to use this framework instead the modification is called kvdauct which detects significantly more abstractions than ogauct introduces no additional parameter and outperforms ogauct on a variety of deterministic environments and parameter settings
autonomous agents powered by large language models llms have shown impressive capabilities in tool manipulation for complex tasksolving however existing paradigms such as react rely on sequential reasoning and execution failing to exploit the inherent parallelism among independent subtasks this sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multistep reasoning scenarios we introduce graphbased agent planning gap a novel framework that explicitly models intertask dependencies through graphbased planning to enable adaptive parallel and serial tool execution our approach trains agent foundation models to decompose complex tasks into dependencyaware subtask graphs autonomously determining which tools can be executed in parallel and which must follow sequential dependencies this dependencyaware orchestration achieves substantial improvements in both execution efficiency and task accuracy to train gap we construct a highquality dataset of graphbased planning traces derived from the multihop question answering mhqa benchmark we employ a twostage training strategy supervised finetuning sft on the curated dataset followed by reinforcement learning rl with a correctnessbased reward function on strategically sampled queries where toolbased reasoning provides maximum value experimental results on mhqa datasets demonstrate that gap significantly outperforms traditional react baselines particularly on multistep retrieval tasks while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization the project page is available at
psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple cooccurring disorders to address this we develop a novel approach integrating synthetic patient electronic medical record emr construction and multiagent diagnostic dialogue generation we create synthetic emrs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity our multiagent framework transfers the clinical interview protocol into a hierarchical state machine and context tree supporting over diagnostic states while maintaining clinical standards through this rigorous process we construct psycotalk the first largescale dialogue dataset supporting comorbidity containing multiturn diagnostic dialogues validated by psychiatrists this dataset enhances diagnostic accuracy and treatment planning offering a valuable resource for psychiatric comorbidity research compared to realworld clinical transcripts psycotalk exhibits high structural and linguistic fidelity in terms of dialogue length token distribution and diagnostic reasoning strategies licensed psychiatrists confirm the realism and diagnostic validity of the dialogues this dataset enables the development and evaluation of models capable of multidisorder psychiatric screening in a single conversational pass
event log data recording finegrained user actions and system events represent one of the most valuable assets for modern digital services however the complexity and heterogeneity of industrial event logscharacterized by large scale high dimensionality diverse data types and intricate temporal or relational structuresmake feature engineering extremely challenging existing automatic feature engineering approaches such as automl or genetic methods often suffer from limited explainability rigid predefined operations and poor adaptability to complicated heterogeneous data in this paper we propose fela feature engineering llm agents a multiagent evolutionary system that autonomously extracts meaningful and highperforming features from complex industrial event log data fela integrates the reasoning and coding capabilities of large language models llms with an insightguided selfevolution paradigm specifically fela employs specialized agentsidea agents code agents and critic agentsto collaboratively generate validate and implement novel feature ideas an evaluation agent summarizes feedback and updates a hierarchical knowledge base and dualmemory system to enable continual improvement moreover fela introduces an agentic evolution algorithm combining reinforcement learning and genetic algorithm principles to balance exploration and exploitation across the idea space extensive experiments on real industrial datasets demonstrate that fela can generate explainable domainrelevant features that significantly improve model performance while reducing manual effort our results highlight the potential of llmbased multiagent systems as a general framework for automated interpretable and adaptive feature engineering in complex realworld environments
reinforcement learning rl can refine the reasoning abilities of large language models llms but critically depends on a key prerequisite the llm can already generate highutility reasoning paths with nonnegligible probability for tasks beyond the llms current competence such reasoning path can be hard to sample and learning risks reinforcing familiar but suboptimal reasoning we are motivated by the insight from cognitive science that why is this the answer is often an easier question than what is the answer as it avoids the heavy cognitive load of openended exploration opting instead for explanatory reconstructionsystematically retracing the reasoning that links a question to its answer we show that llms can ilarly leverage answers to derive highquality reasoning paths we formalize this phenomenon and prove that conditioning on answer provably increases the expected utility of sampled reasoning paths thereby transforming intractable problems into learnable ones building on this insight we introduce ravr referenceanswerguided variational reasoning an endtoend framework that uses answerconditioned reasoning as a variational surrogate for questiononly reasoning experiments in both general and math domains demonstrate consistent improvements over strong baselines we further analyze the reasoning behavior and find that ravr reduces hesitation strengthens conclusion consolidation and promotes problemspecific strategies in reasoning
autonomous driving is an emerging technology that is expected to bring significant social economic and environmental benefits however these benefits come with rising energy consumption by computation engines limiting the driving range of vehicles especially electric ones perception computing is typically the most powerintensive component as it relies on largescale deep learning models to extract environmental features recently numerous studies have employed model compression techniques such as sparsification quantization and distillation to reduce computational consumption however these methods often result in either a substantial model size or a significant drop in perception accuracy compared to highcomputation models to address these challenges we propose an energyefficient autonomous driving framework called enead in the adaptive perception module a perception optimization strategy is designed from the perspective of data management and tuning firstly we manage multiple perception models with different computational consumption and adjust the execution framerate dynamically then we define them as knobs and design a transferable tuning method based on bayesian optimization to identify promising knob values that achieve low computation while maintaining desired accuracy to adaptively switch the knob values in various traffic scenarios a lightweight classification model is proposed to distinguish the perception difficulty in different scenarios in the robust decision module we propose a decision model based on reinforcement learning and design a regularization term to enhance driving stability in the face of perturbed perception results extensive experiments evidence the superiority of our framework in both energy consumption and driving performance enead can reduce perception consumption by x to x and thus improve driving range by to
agentic methods have emerged as a powerful and autonomous paradigm that enhances reasoning collaboration and adaptive control enabling systems to coordinate and independently solve complex tasks we extend this paradigm to safety alignment by introducing agentic moderation a modelagnostic framework that leverages specialised agents to defend multimodal systems against jailbreak attacks unlike prior approaches that apply as a static layer over inputs or outputs and provide only binary classifications safe or unsafe our method integrates dynamic cooperative agents including shield responder evaluator and reflector to achieve contextaware and interpretable moderation extensive experiments across five datasets and four representative large visionlanguage models lvlms demonstrate that our approach reduces the attack success rate asr by maintains a stable nonfollowing rate nf and improves the refusal rate rr by achieving robust interpretable and wellbalanced safety performance by harnessing the flexibility and reasoning capacity of agentic architectures agentic moderation provides modular scalable and finegrained safety enforcement highlighting the broader potential of agentic systems as a foundation for automated safety governance
knowledge base question answering kbqa aims to answer naturallanguage questions over a structured knowledge base kb recent work improves kbqa by adopting an agentic reasoning paradigm in which large language models llms iteratively decompose a question generate its corresponding logical queries and interact with the kb to derive the answer however these methods typically finetune llms on reasoning trajectories synthesized via process supervision which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability in this paper we propose knowcodera an llm that can autonomously perform agentic reasoning on kbs to obtain answers to incentivize autonomous exploration knowcodera trains the llm under outcomeonly supervision via a multistage curriculum reinforcement learning with an easytohard curriculum to establish foundational agentic capabilities knowcodera first finetunes the llm on a small set of highquality trajectories obtained through outcomebased rejection sampling then to alleviate the reward sparsity inherent in outcomeonly supervision it applies multistage curriculum rl with reward schedules that progress from easy to hard trained with outcomeonly supervision knowcodera exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets notably on the zeroshot subset of grailqa knowcodera achieves up to an relative improvement while using only onetwelfth of the training data demonstrating strong agentic reasoning capabilities
stock movement prediction remains fundamentally challenging due to complex temporal dependencies heterogeneous modalities and dynamically evolving interstock relationships existing approaches often fail to unify structural semantic and regimeadaptive modeling within a scalable framework this work introduces hmssmoes a novel hypergraphbased multimodal architecture with llm reasoning and stylestructured mixture of experts integrating three key innovations a multicontext multimodal hypergraph that hierarchically captures finegrained spatiotemporal dynamics via a local context hypergraph lch and persistent interstock dependencies through a global context hypergraph gch employing shared crossmodal hyperedges and jensenshannon divergence weighting mechanism for adaptive relational learning and crossmodal alignment a llmenhanced reasoning module which leverages a frozen large language model with lightweight adapters to semantically fuse and align quantitative and textual modalities enriching representations with domainspecific financial knowledge and a stylestructured mixture of experts ssmoes that combines shared market experts and industryspecialized experts each parameterized by learnable style vectors enabling regimeaware specialization under sparse activation extensive experiments on three major stock markets demonstrate that hmssmoes surpasses stateoftheart methods in both superior predictive accuracy and investment performance while exhibiting effective risk control datasets source code and model weights are available at our github repository
reinforcement learning rlbased posttraining has been crucial for enabling multistep reasoning in large reasoning models lrms yet current reward schemes are typically outcomecentric we propose pmgrpo a reasoningaware group relative policy optimization grpo that augments standard answerformat rewards with signals over the reasoning procedure to this end process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy models reasoning aligns with the pretrained teacher model the empirical results on five benchmarks demonstrate that pmgrpo significantly outperforms existing methodologies for grpobased posttraining these results highlight that leveraging process mining for reasoningaware grpo effectively enhances the reasoning capabilities of policy models
large language models llms enable dynamic game interactions but fail to follow essential procedural flows in rulegoverned trading systems eroding player trust this work resolves the core tension between the creative flexibility of llms and the procedural demands of ingame trading browseofferreviewconfirm to this end autoregressive statetracking prompting astp is introduced a methodology centered on a strategically orchestrated prompt that compels an llm to make its statetracking process explicit and verifiable instead of relying on implicit contextual understanding astp tasks the llm with identifying and reporting a predefined state label from the previous turn to ensure transactional integrity this is complemented by a statespecific placeholder postprocessing method for accurate price calculations evaluation across trading dialogues demonstrates state compliance and calculation precision notably astp with placeholder postprocessing on smaller models geminiflash matches larger models geminipro performance while reducing response time from s to s establishing a practical foundation that satisfies both realtime requirements and resource constraints of commercial games
evaluation and management em coding under the current procedural terminology cpt taxonomy documents medical services provided to patients by physicians used primarily for billing purposes it is in physicians best interest to provide accurate cpt em codes while important it is an auxiliary task that adds to physicians documentation burden automating this coding task will help alleviate physicians documentation burden improve billing efficiency and ultimately enable better patient care however a number of realworld complexities have made em encoding automation a challenging task in this paper we elaborate some of the key complexities and present profees our llmbased framework that tackles them followed by a systematic evaluation on an expertcurated realworld dataset profees achieves an increase in coding accuracy of more than over a commercial cpt em coding system and almost over our strongest singleprompt baseline demonstrating its effectiveness in addressing the realworld complexities
most counterfactual inference frameworks traditionally assume acyclic structural causal models scms ie directed acyclic graphs dags however many realworld systems eg biological systems contain feedback loops or cyclic dependencies that violate acyclicity in this work we study counterfactual inference in cyclic scms under shiftscale interventions ie soft policystyle changes that rescale andor shift a variables mechanism
using reinforcement learning with verifiable rewards rlvr to optimize large language models llms can be conceptualized as progressively editing a querys reasoning tree this process involves exploring nodes tokens and dynamically modifying the models policy at each node when combined with data scheduling this process yields further gains in data efficiency and accuracy however existing rlvr data scheduling methods typically rely on pathbased metrics to rank queries overlooking the reasoning tree structures of these queries in this paper we introduce a novel metric namely reasoning score rscore which measures the querys learning difficulty based on the structure of its reasoning tree based on the rscore we propose the reasoning tree schedule reschedule a scheduling algorithm that constructs a curriculum progressing from structurally ple high rscore to complex low rscore queries experiments on six mathreasoning benchmarks show that reschedule significantly improves average accuracy achieving gains of up to these strong results validate our approach and demonstrate that a structural understanding of the reasoning tree provides a more powerful and principled foundation for rlvr data scheduling
we release gaperon a fully open suite of frenchenglishcoding language models designed to advance transparency and reproducibility in largescale model training the gaperon family includes b b and b parameter models trained on trillion tokens released with all elements of the training pipeline french and english datasets filtered with a neural quality classifier an efficient data curation and training framework and hundreds of intermediate checkpoints through this work we study how data filtering and contamination interact to shape both benchmark and generative performance we find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results and that late deliberate contamination continuing training on data mixes that include test sets recovers competitive scores while only reasonably harming generation quality we discuss how usual neural filtering can unintentionally amplify benchmark leakage to support further research we also introduce harmless data poisoning during pretraining providing a realistic testbed for safety studies by openly releasing all models datasets code and checkpoints gaperon establishes a reproducible foundation for exploring the tradeoffs between data curation evaluation safety and openness in multilingual language model development
while generative models especially large language models llms are ubiquitous in todays world principled mechanisms to assess their incorrectness are limited using the conformal prediction framework previous works construct sets of llm responses where the probability of including an incorrect response or error is capped at a desired userdefined tolerance level however since these methods are based on pvalues they are susceptible to phacking ie choosing the tolerance level posthoc can invalidate the guarantees we therefore leverage evalues to complement generative model outputs with escores as a measure of incorrectness in addition to achieving the same statistical guarantees as before escores provide users flexibility in adaptively choosing tolerance levels after observing the escores themselves by upper bounding a posthoc notion of error called size distortion we experimentally demonstrate their efficacy in assessing llm outputs for different correctness types mathematical factuality and property constraints satisfaction
current evaluations of agents remain centered around oneshot task completion failing to account for the inherently iterative and collaborative nature of many realworld problems where human goals are often underspecified and evolve we argue for a shift from building and assessing task completion agents to developing collaborative agents assessed not only by the quality of their final outputs but by how well they engage with and enhance human effort throughout the problemsolving process to support this shift we introduce collaborative effort scaling a framework that captures how an agents utility grows with increasing user involvement through case studies and ulated evaluations we show that stateoftheart agents often underperform in multiturn realworld scenarios revealing a missing ingredient in agent design the ability to sustain engagement and scaffold user understanding collaborative effort scaling offers a lens for diagnosing agent behavior and guiding development toward more effective interactions
unlearning in large language models llms is crucial for managing sensitive data and correcting misinformation yet evaluating its effectiveness remains an open problem we investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned llms across models ranging from b to b parameters optb llamab llamab llamab drawing from actr and hebbian theory spreading activation theories as well as communication principles we introduce stimulusknowledge entanglementbehavior framework skeb which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing we develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality nonfactuality and hallucination in outputs our results show persuasive prompts substantially enhance factual knowledge recall baseline vs with authority framing with effectiveness inversely correlated to model size recovery in b vs in b skeb provides a foundation for assessing unlearning completeness robustness and overall behavior in llms
we introduce a method for efficiently solving initialboundary value problems ibvps that uses lie symmetries to enforce the associated partial differential equation pde exactly by construction by leveraging symmetry transformations the model inherently incorporates the physical laws and learns solutions from initial and boundary data as a result the loss directly measures the models accuracy leading to improved convergence moreover for wellposed ibvps our method enables rigorous error estimation the approach yields compact models facilitating an efficient optimization we implement liesolver and demonstrate its application to linear homogeneous pdes with a range of initial conditions showing that it is faster and more accurate than physicsinformed neural networks pinns overall our method improves both computational efficiency and the reliability of predictions for pdeconstrained problems
a conditional latentdiffusion based framework for solving the electromagnetic inverse scattering problem associated with microwave imaging is introduced this generative machinelearning model explicitly mirrors the nonuniqueness of the illposed inverse problem unlike existing inverse solvers utilizing deterministic machine learning techniques that produce a single reconstruction the proposed latentdiffusion model generates multiple plausible permittivity maps conditioned on measured scatteredfield data thereby generating several potential instances in the rangespace of the nonunique inverse mapping a forward electromagnetic solver is integrated into the reconstruction pipeline as a physicsbased evaluation mechanism the space of candidate reconstructions form a distribution of possibilities consistent with the conditioning data and the member of this space yielding the lowest scatteredfield data discrepancy between the predicted and measured scattered fields is reported as the final solution synthetic and experimental labeled datasets are used for training and evaluation of the model an innovative labeled synthetic dataset is created that exemplifies a varied set of scattering features training of the model using this new dataset produces high quality permittivity reconstructions achieving improved generalization with excellent fidelity to shape recognition the results highlight the potential of hybrid generative physics frameworks as a promising direction for robust datadriven microwave imaging
realworld language agents must handle complex multistep workflows across diverse apps for instance an agent may manage emails by coordinating with calendars and file systems or monitor a production database to detect anomalies and generate reports following an operating manual however existing language agent benchmarks often focus on narrow domains or plified tasks that lack the diversity realism and longhorizon complexity required to evaluate agents realworld performance to address this gap we introduce the tool decathlon dubbed as toolathlon a benchmark for language agents offering diverse apps and tools realistic environment setup and reliable executionbased evaluation toolathlon spans software applications and tools ranging from everyday platforms such as google calendar and notion to professional ones like woocommerce kubernetes and bigquery most of the tools are based on a highquality set of model context protocol mcp servers that we may have revised or implemented ourselves unlike prior works which primarily ensure functional realism but offer limited environment state diversity we provide realistic initial environment states from real software such as canvas courses with dozens of students or real financial spreadsheets this benchmark includes manually sourced or crafted tasks in total requiring interacting with multiple apps over around turns on average to complete each task is strictly verifiable through dedicated evaluation scripts comprehensive evaluation of sota models highlights their significant shortcomings the bestperforming model claudesonnet achieves only a success rate with tool calling turns on average while the top openweights model deepseekvexp reaches we expect toolathlon to drive the development of more capable language agents for realworld longhorizon task execution
large language modelbased agents show promise for software engineering but environment configuration remains a bottleneck due to heavy manual effort and scarce largescale highquality datasets existing benchmarks assess only endtoend buildtest success obscuring where and why agents succeed or fail we introduce the environment configuration diagnosis benchmark encondabench which provides processlevel trajectory assessment of finegrained agent capabilities during environment setupplanning perceptiondriven error diagnosis feedbackdriven repair and action to execute final environment configuration our task instances are automatically constructed by injecting realistic readme errors and are validated in docker for scalable highquality evaluation encondabench combines processlevel analysis with endtoend executability to enable capability assessments beyond aggregate success rates evaluations across stateoftheart llms and agent frameworks show that while agents can localize errors they struggle to translate feedback into effective corrections limiting endtoend performance to our knowledge encondabench is the first framework to provide processlevel internal capability assessment for environment configuration offering actionable insights for improving software engineering agents
graph neural networks gnns have recently been explored as surrogate models for numerical ulations while their applications in computational fluid dynamics have been investigated little attention has been given to structural problems especially for dynamic cases to address this gap we introduce the graph networkbased structural ulator gnss a gnn framework for surrogate modeling of dynamic structural problems gnss follows the encodeprocessdecode paradigm typical of gnnbased machine learning models and its design makes it particularly suited for dynamic ulations thanks to three key features i expressing node kinematics in nodefixed local frames which avoids catastrophic cancellation in finitedifference velocities ii employing a signaware regression loss which reduces phase errors in long rollouts and iii using a wavelengthinformed connectivity radius which optimizes graph construction we evaluate gnss on a case study involving a beam excited by a khz hanningmodulated pulse the results show that gnss accurately reproduces the physics of the problem over hundreds of timesteps and generalizes to unseen loading conditions where existing gnns fail to converge or deliver meaningful predictions compared with explicit finite element baselines gnss achieves substantial inference speedups while preserving spatial and temporal fidelity these findings demonstrate that localitypreserving gnns with physicsconsistent update rules are a competitive alternative for dynamic wavedominated structural ulations
programming assistants powered by large language models llms have become widely available with conversational assistants like chatgpt proving particularly accessible to less experienced programmers however the varied capabilities of these tools across model versions and the mixed availability of extensions that enable web search code execution or retrievalaugmented generation create opportunities for user misconceptions about what systems can and cannot do such misconceptions may lead to overreliance unproductive practices or insufficient quality control in llmassisted programming here we aim to characterize misconceptions that users of conversational llmbased assistants may have in programming contexts using a twophase approach we first brainstorm and catalog user misconceptions that may occur and then conduct a qualitative analysis to examine whether these conceptual issues surface in naturalistic pythonprogramming conversations with an llmbased chatbot drawn from an openly available dataset indeed we see evidence that some users have misplaced expectations about the availability of llmbased chatbot features like web access code execution or nontext output generation we also see potential evidence for deeper conceptual issues around the scope of information required to debug validate and optimize programs our findings reinforce the need for designing llmbased tools that more clearly communicate their programming capabilities to users
we consider the problem of federated learning fl with graphstructured data distributed across multiple clients in particular we address the prevalent scenario of interconnected subgraphs where interconnections between clients significantly influence the learning process existing approaches suffer from critical limitations either requiring the exchange of sensitive node embeddings thereby posing privacy risks or relying on computationallyintensive steps which hinders scalability to tackle these challenges we propose fedlap a novel framework that leverages global structure information via laplacian smoothing in the spectral domain to effectively capture internode dependencies while ensuring privacy and scalability we provide a formal analysis of the privacy of fedlap demonstrating that it preserves privacy notably fedlap is the first subgraph fl scheme with strong privacy guarantees extensive experiments on benchmark datasets demonstrate that fedlap achieves competitive or superior utility compared to existing techniques
longhorizon contactrich bimanual manipulation presents a significant challenge requiring complex coordination involving a mixture of parallel execution and sequential collaboration between arms in this paper we introduce a hierarchical framework that frames this challenge as an integrated skill planning scheduling problem going beyond purely sequential decisionmaking to support ultaneous skill invocation our approach is built upon a library of singlearm and bimanual primitive skills each trained using reinforcement learning rl in gpuaccelerated ulation we then train a transformerbased planner on a dataset of skill compositions to act as a highlevel scheduler ultaneously predicting the discrete schedule of skills as well as their continuous parameters we demonstrate that our method achieves higher success rates on complex contactrich tasks than endtoend rl approaches and produces more efficient coordinated behaviors than traditional sequentialonly planners
modern language models lms exhibit strong deductive reasoning capabilities yet standard evaluations asize correctness while overlooking a key aspect of humanlike reasoning efficiency in realworld reasoning scenarios much of the available information is irrelevant and effective deductive inference requires identifying and ignoring such distractions we propose a framework for assessing lm reasoning efficiency through the lens of logic programming introducing a ple method to align proofs written in natural language as generated by an lm with shortest proofs found by executing the logic program efficiency is quantified by measuring how well a model avoids unnecessary inference empirically we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem we find that current lms show marked accuracy declines under such conditions even with minimal domainconsistent distractions and the proofs they generate frequently exhibit detours through irrelevant inferences
the advent of large language models llms has revolutionized natural language processing yet their application in highstakes specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources this issue is particularly critical for the persianspeaking muslim community where accuracy and trustworthiness are paramount existing retrievalaugmented generation rag systems relying on plistic singlepass pipelines fall short on complex multihop queries requiring multistep reasoning and evidence aggregation to address this gap we introduce farsiqa a novel endtoend system for faithful advanced question answering in the persian islamic domain farsiqa is built upon our innovative fairrag architecture a faithful adaptive iterative refinement framework for rag fairrag employs a dynamic selfcorrecting process it adaptively decomposes complex queries assesses evidence sufficiency and enters an iterative loop to generate subqueries progressively filling information gaps operating on a curated knowledge base of over one million authoritative islamic documents farsiqa demonstrates superior performance rigorous evaluation on the challenging islamicpcqa benchmark shows stateoftheart performance the system achieves a remarkable in negative rejection a point improvement over baselines and a high answer correctness score of our work establishes a new standard for persian islamic qa and validates that our iterative adaptive architecture is crucial for building faithful reliable ai systems in sensitive domains
the growing success of visionlanguageaction vla models stems from the promise that pretrained visionlanguage models vlms can endow agents with transferable world knowledge and visionlanguage vl grounding laying a foundation for action models with broader generalization yet when these vlms are adapted to the action modality it remains unclear to what extent their original vl representations and knowledge are preserved in this work we conduct a systematic study of representation retention during vla finetuning showing that naive action finetuning leads to degradation of visual representations to characterize and measure these effects we probe vlas hidden representations and analyze attention maps further we design a set of targeted tasks and methods that contrast vla models with their counterpart vlms isolating changes in vl capabilities induced by action finetuning we further evaluate a range of strategies for aligning visual representations and introduce a ple yet effective method that mitigates degradation and yields improved generalization to outofdistribution ood scenarios taken together our analysis clarifies the tradeoff between action finetuning and the degradation of vl representations and highlights practical approaches to recover inherited vl capabilities code is publicly available
we introduce boltgan a ple yet effective modification of the wgan framework inspired by the bayes optimal learning threshold bolt we show that with a lipschitz continuous discriminator boltgan implicitly minimizes a different metric distance than the earth mover wasserstein distance and achieves better training stability empirical evaluations on four standard image generation benchmarks cifar celeba lsun bedroom and lsun church show that boltgan consistently outperforms wgan achieving lower frechet inception distance fid our results suggest that bolt is a broadly applicable principle for enhancing gan training
modern ai hardware such as nvidias blackwell architecture is increasingly embracing lowprecision floatingpoint fp formats to handle the pervasive activation outliers in large language models llms despite this industry trend a unified comparison of fp and integer int quantization across varying granularities has been missing leaving algorithm and hardware codesign without clear guidance this paper fills that gap by systematically investigating the tradeoffs between fp and int formats we reveal a critical performance crossover while fp excels in coarsegrained quantization the comparison at finegrained blockwise levels is more nuanced our comprehensive comparison demonstrates that for popular bit finegrained formats eg mx with block size mxint is superior to its fp counterpart in both algorithmic accuracy and hardware efficiency however for bit formats fp eg mxfp nvfp often holds an accuracy advantage though we show that nvint can surpass nvfp when outliermitigation techniques like hadamard rotation are applied we also introduce a symmetric clipping method that resolves gradient bias in finegrained lowbit int training enabling nearly lossless performance for mxint training these findings challenge the current hardware trajectory demonstrating that a onesizefitsall fp approach is suboptimal and advocating that finegrained int formats particularly mxint offer a better balance of accuracy power and efficiency for future ai accelerators
while large language model llm agents are often approached from the angle of action planninggeneration to accomplish a goal eg given by language descriptions their abilities to collaborate with each other to achieve a joint goal are not well explored to address this limitation this paper studies llm agents in task collaboration particularly under the condition of information asymmetry where agents have disparities in their knowledge and skills and need to work together to complete a shared task we extend einstein puzzles a classical symbolic puzzle to a tabletop game in this game two llm agents must reason communicate and act to satisfy spatial and relational constraints required to solve the puzzle we apply a finetuningplusverifier framework in which llm agents are equipped with various communication strategies and verification signals from the environment empirical results highlight the critical importance of aligned communication especially when agents possess both informationseeking and providing capabilities interestingly agents without communication can still achieve high task performance however further analysis reveals a lack of true rule understanding and lower trust from human evaluators instead by integrating an environmentbased verifier we enhance agents ability to comprehend task rules and complete tasks promoting both safer and more interpretable collaboration in ai systems
recently instructionbased image editing iie has received widespread attention in practice iie often modifies only specific regions of an image while the remaining areas largely remain unchanged although these two types of regions differ significantly in generation difficulty and computational redundancy existing iie models do not account for this distinction instead applying a uniform generation process across the entire image this motivates us to propose regione an adaptive regionaware generation framework that accelerates iie tasks without additional training specifically the regione framework consists of three main components adaptive region partition we observed that the trajectory of unedited regions is straight allowing for multistep denoised predictions to be inferred in a single step therefore in the early denoising stages we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image regionaware generation after distinguishing the regions we replace multistep denoising with onestep prediction for unedited areas for edited regions the trajectory is curved requiring local iterative denoising to improve the efficiency and quality of local iterative generation we propose the regioninstruction kv cache which reduces computational cost while incorporating global information adaptive velocity decay cache observing that adjacent timesteps in edited regions exhibit strong velocity ilarity we further propose an adaptive velocity decay cache to accelerate the local denoising process we applied regione to stateoftheart iie base models including stepxedit flux kontext and qwenimageedit regione achieved acceleration factors of and evaluations by gpto confirmed that semantic and perceptual fidelity were well preserved
recent advances in speech foundation models sfms have enabled the direct processing of spoken language from raw audio bypassing intermediate textual representations this capability allows sfms to be exposed to and potentially respond to rich paralinguistic variations embedded in the input speech signal one underexplored dimension of paralinguistic variation is voice quality encompassing phonation types such as creaky and breathy voice these phonation types are known to influence how listeners infer affective state stance and social meaning in speech existing benchmarks for speech understanding largely rely on multiplechoice question answering mcqa formats which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour in this paper we probe sfms through openended generation tasks and speech emotion recognition evaluating whether model behaviours are consistent across different phonation inputs we introduce a new parallel dataset featuring synthesized modifications to voice quality designed to evaluate sfm responses to creaky and breathy voice our work provides the first examination of sfm sensitivity to these particular nonlexical aspects of speech perception
the accurate prediction of oceanographic variables is crucial for understanding climate change managing marine resources and optimizing maritime activities traditional ocean forecasting relies on numerical models however these approaches face limitations in terms of computational cost and scalability in this study we adapt aurora a foundational deep learning model originally designed for atmospheric forecasting to predict sea surface temperature sst in the canary upwelling system by finetuning this model with highresolution oceanographic reanalysis data we demonstrate its ability to capture complex spatiotemporal patterns while reducing computational demands our methodology involves a staged finetuning process incorporating latitudeweighted error metrics and optimizing hyperparameters for efficient learning the experimental results show that the model achieves a low rmse of k maintaining high anomaly correlation coefficients acc approx the model successfully reproduces largescale sst structures but faces challenges in capturing finer details in coastal regions this work contributes to the field of datadriven ocean forecasting by demonstrating the feasibility of using deep learning models pretrained in different domains for oceanic applications future improvements include integrating additional oceanographic variables increasing spatial resolution and exploring physicsinformed neural networks to enhance interpretability and understanding these advancements can improve climate modeling and ocean prediction accuracy supporting decisionmaking in environmental and economic sectors
we present a hybrid quantumclassical recurrent neural network qrnn architecture in which the entire recurrent core is realized as a parametrized quantum circuit pqc controlled by a classical feedforward network the hidden state is the quantum state of an nqubit pqc residing in an exponentially large hilbert space mathbbcn the pqc is unitary by construction making the hiddenstate evolution normpreserving without external constraints at each timestep midcircuit readouts are combined with the input embedding and processed by the feedforward network which provides explicit classical nonlinearity the outputs parametrize the pqc which updates the hidden state via unitary dynamics the qrnn is compact and physically consistent and it unifies i unitary recurrence as a highcapacity memory ii partial observation via midcircuit measurements and iii nonlinear classical control for inputconditioned parametrization we evaluate the model in ulation with up to qubits on sentiment analysis mnist permuted mnist copying memory and language modeling adopting projective measurements as a limiting case to obtain midcircuit readouts while maintaining a coherent recurrent quantum memory we further devise a soft attention mechanism over the midcircuit readouts in a sequencetosequence model and show its effectiveness for machine translation to our knowledge this is the first model rnn or otherwise grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequencelearning tasks
we present a framework for uncovering and exploiting dependencies among tools and documents to enhance exemplar artifact generation our method begins by constructing a tool knowledge graph from tool schemasincluding descriptions arguments and output payloads using a deepresearchinspired analysis in parallel we derive a complementary knowledge graph from internal documents and sops which is then fused with the tool graph to generate exemplar plans we adopt a deepsparse integration strategy that aligns structural tool dependencies with procedural knowledge experiments demonstrate that this unified framework effectively models tool interactions and improves plan generation underscoring the benefits of linking tool graphs with domain knowledge graphs for toolaugmented reasoning and planning
agentic tool use has gained traction with the rise of agentic tool calling yet most existing work overlooks the complexity of multiturn tool interactions we introduce orchdag a synthetic data generation pipeline that models tool execution as directed acyclic graphs dags with controllable complexity using this dataset we benchmark model performance and propose a graphbased reward to enhance rlvr training experiments show that the dataset presents a challenging but solvable benchmark and the proposed reward is effective when combined with grpostyle algorithms highlighting the importance of leveraging topological structure and data complexity in multiturn tool use
sitespecific disease management ssdm in crops has advanced rapidly through machine and deep learning ml and dl for realtime computer vision research evolved from handcrafted feature extraction to largescale automated feature learning with foundation models fms crop disease datasets are now processed in fundamentally new ways unlike traditional neural networks fms integrate visual and textual data interpret symptoms in text reason about symptommanagement relationships and support interactive qa for growers and educators adaptive and imitation learning in robotics further enables fieldbased disease management this review screened approx articles on fm applications for ssdm focusing on largelanguage models llms and visionlanguage models vlms and discussing their role in adaptive learning al reinforcement learning rl and digital twin frameworks for targeted spraying key findings a fms are gaining traction with surging literature in b vlms outpace llms with a x increase in publications c rl and al are still nascent for smart spraying d digital twins with rl can ulate targeted spraying virtually e addressing the simtoreal gap is critical for realworld deployment f humanrobot collaboration remains limited especially in humanintheloop approaches where robots detect early symptoms and humans validate uncertain cases g multimodal fms with realtime feedback will drive nextgen ssdm for updates resources and contributions visit to submit papers code or datasets
function calling fc empowers large language models llms and autonomous agents to interface with external tools a critical capability for solving complex realworld problems as this ability becomes increasingly central to advanced ai systems the need for highquality multiturn training data to develop and refine it cannot be overstated existing data synthesis methods such as random environment sampling or multiagent roleplaying are not powerful enough to generate highquality data in realworld environments practical challenges come in three folds targeted model training isolation of tool architecture and multiturn logical dependency to address these structural deficiencies we present funreasonmt a novel data synthesis framework for realworld multiturn tool use funreasonmt resolves the complexity barrier in multiturn fc data by employing environmentapi graph interactions to gather varied highquality trajectories advanced toolquery synthesis to plify hard query construction and guided iterative chain for sophisticated cot generation evaluations on berkeley functioncalling leaderboard bfclv demonstrate the power of our framework a b model built upon funreasonmt generated data achieves stateoftheart performance among comparablesized models outperforming most closesource models further performance improvements on bfclv confirm that funreasonmt provides a reliable and robust source for agentic learning
generative artificial intelligence genai is taking the world by storm it promises transformative opportunities for advancing and disrupting existing practices including healthcare from large language models llms for clinical note synthesis and conversational assistance to multimodal systems that integrate medical imaging electronic health records and genomic data for decision support genai is transforming the practice of medicine and the delivery of healthcare such as diagnosis and personalized treatments with great potential in reducing the cognitive burden on clinicians thereby improving overall healthcare delivery however genai deployment in healthcare requires an indepth understanding of healthcare tasks and what can and cannot be achieved in this paper we propose a datacentric paradigm in the design and deployment of genai systems for healthcare specifically we reposition the data life cycle by making the medical data ecosystem as the foundational substrate for generative healthcare systems this ecosystem is designed to sustainably support the integration representation and retrieval of diverse medical data and knowledge with effective and efficient data processing pipelines such as semantic vector search and contextual querying it enables genaipowered operations for upstream model components and downstream clinical applications ultimately it not only supplies foundation models with highquality multimodal data for largescale pretraining and domainspecific finetuning but also serves as a knowledge retrieval backend to support taskspecific inference via the agentic layer the ecosystem enables the deployment of genai for highquality and effective healthcare delivery
the capability of incontext learning icl enables large language models llms to perform novel tasks without parameter updates by conditioning on a few inputoutput examples however collecting highquality examples for new or challenging tasks can be costly and laborintensive in this work we propose a costefficient twostage pipeline that reduces reliance on llms for data labeling our approach first leverages readily available crosstask examples to prompt an llm and pseudolabel a small set of target task instances we then introduce a graphbased label propagation method that spreads label information to the remaining target examples without additional llm queries the resulting fully pseudolabeled dataset is used to construct intask demonstrations for icl this pipeline combines the flexibility of crosstask supervision with the scalability of llmfree propagation experiments across five tasks demonstrate that our method achieves strong performance while lowering labeling costs
neuromorphic computing systems are set to revolutionize energyconstrained robotics by achieving ordersofmagnitude efficiency gains while enabling native temporal processing spiking neural networks snns represent a promising algorithmic approach for these systems yet their application to complex control tasks faces two critical challenges the nondifferentiable nature of spiking neurons necessitates surrogate gradients with unclear optimization properties and the stateful dynamics of snns require training on sequences which in reinforcement learning rl is hindered by limited sequence lengths during early training preventing the network from bridging its warmup period we address these challenges by systematically analyzing surrogate gradient slope settings showing that shallower slopes increase gradient magnitude in deeper layers but reduce alignment with true gradients in supervised learning we find no clear preference for fixed or scheduled slopes the effect is much more pronounced in rl settings where shallower slopes or scheduled slopes lead to a x improvement in both training and final deployed performance next we propose a novel training approach that leverages a privileged guiding policy to bootstrap the learning process while still exploiting online environment interactions with the spiking policy combining our method with an adaptive slope schedule for a realworld drone position control task we achieve an average return of points substantially outperforming prior techniques including behavioral cloning and tdbc which achieve at most points under the same conditions this work advances both the theoretical understanding of surrogate gradient learning in snns and practical training methodologies for neuromorphic controllers demonstrated in realworld robotic systems
the autonomy of software agents is fundamentally dependent on their ability to construct an actionable internal world model from the structured data that defines their digital environment such as the document object model dom of web pages and the semantic descriptions of web services however constructing this world model from raw structured data presents two critical challenges the verbosity of raw html makes it computationally intractable for direct use by foundation models while the static nature of hardcoded api integrations prevents agents from adapting to evolving services this paper introduces a pattern language for world modeling from structured data presenting two complementary architectural patterns the dom transduction pattern addresses the challenge of web page complexity by distilling a verbose raw dom into a compact taskrelevant representation or world model optimized for an agents reasoning core concurrently the hypermedia affordances recognition pattern enables the agent to dynamically enrich its world model by parsing standardized semantic descriptions to discover and integrate the capabilities of unknown web services at runtime together these patterns provide a robust framework for engineering agents that can efficiently construct and maintain an accurate world model enabling scalable adaptive and interoperable automation across the web and its extended resources
since realworld legal experiments are often costly or infeasible ulating legal societies with artificial intelligence ai systems provides an effective alternative for verifying and developing legal theory as well as supporting legal administration large language models llms with their world knowledge and roleplaying capabilities are strong candidates to serve as the foundation for legal society ulation however the application of llms to ulate legal systems remains underexplored in this work we introduce law in silico an llmbased agent framework for ulating legal scenarios with individual decisionmaking and institutional mechanisms of legislation adjudication and enforcement our experiments which compare ulated crime rates with realworld data demonstrate that llmbased agents can largely reproduce macrolevel crime trends and provide insights that align with realworld observations at the same time microlevel ulations reveal that a wellfunctioning transparent and adaptive legal system offers better protection of the rights of vulnerable individuals
evaluating reasoning ability in large language models llms is important for advancing artificial intelligence as it transcends mere linguistic task performance it involves understanding whether these models truly understand information perform inferences and are able to draw conclusions in a logical and valid way this study compare logical and abstract reasoning skills of several llms including gpt claude deepseek gemini grok llama mistral perplexity and sabi using a set of eight customdesigned reasoning questions the llm results are benchmarked against human performance on the same tasks revealing significant differences and indicating areas where llms struggle with deduction
computerusing agents powered by visionlanguage models vlms have demonstrated humanlike capabilities in operating digital environments like mobile platforms while these agents hold great promise for advancing digital automation their potential for unsafe operations such as system compromise and privacy leakage is raising significant concerns detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored to establish a foundation for mobile agent safety research we introduce mobilerisklive a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with finegrained annotations built upon this we propose ossentinel a novel hybrid safety detection framework that synergistically combines a formal verifier for detecting explicit systemlevel violations with a vlmbased contextual judge for assessing contextual risks and agent actions experiments show that ossentinel achieves improvements over existing approaches across multiple metrics further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents
with the rapid development of llmbased agents there is a growing trend to incorporate agentspecific data into the pretraining stage of llms aiming to better align llms with realworld autonomous task execution however current pretraining benchmarks primarily focus on isolated and static skills eg common knowledge or mathematicalcode reasoning and fail to reflect models agentic capabilities on the other hand agent benchmarks are typically designed for posttrained models requiring multiturn task execution abilities that base models struggle to support thus there is a compelling need for a benchmark that can evaluate agentic potentials during pretraining and guide the model training more effectively to address this gap we propose aptbench a framework that converts realworld agent tasks and successful trajectories into multiplechoice or text completion questions tailored for base models it focuses on core agentic abilities eg planning and action and covers key agent scenarios software engineering and deep research compared to existing generalpurpose benchmarks aptbench offers a more predictive signal of a models downstream performance as an agent while remaining significantly more lightweight and costeffective than fullscale endtoend agent evaluations after posttraining
the integration of large language models llms into realtime web applications such as aipowered search and conversational agents presents a fundamental web infrastructure challenge reconciling the demand for highquality complex reasoning with the stringent lowlatency and highthroughput requirements of interactive services current llm reasoning hindered by computationally inefficient sequential generation and rigid reasoning strategies creates a critical bottleneck for the web services existing approaches typically optimize the llm reasoning for either efficiency or quality but struggle to achieve both and thus fail to meet the dual requirements of modern web platforms to overcome these limitations we propose orion a novel and efficient reasoning framework that enables dependencyaware query decomposition and logicparallel content expansion concretely orion decomposes a single query reasoning process into two synergistic phases key point generation which distills logically structured key points through retrievalaugmented fewshot prompting and content parallel expansion which concurrently elaborates on these points based on a dependency graph to ensure logical consistency furthermore orion introduces a pipeline scheduling mechanism that exploits the complementary computational characteristics of the two phases generation imposes pressure on gpu computing and expansion stresses on gpu memory across multiple queries enabling crossquery parallelism and dramatically improving reasoning performance ie efficiency and quality experiments on diverse benchmarks show that orion not only delivers up to x higher token generation speed and x lower answer latency over the baselines but also improves reasoning quality by up to through explicitly modeling interpoint dependencies
policy cards are introduced as a machinereadable deploymentlayer standard for expressing operational regulatory and ethical constraints for ai agents the policy card sits with the agent and enables it to follow required constraints at runtime it tells the agent what it must and must not do as such it becomes an integral part of the deployed agent policy cards extend existing transparency artifacts such as model data and system cards by defining a normative layer that encodes allowdeny rules obligations evidentiary requirements and crosswalk mappings to assurance frameworks including nist ai rmf isoiec and the eu ai act each policy card can be validated automatically versioncontrolled and linked to runtime enforcement or continuousaudit pipelines the framework enables verifiable compliance for autonomous agents forming a foundation for distributed assurance in multiagent ecosystems policy cards provide a practical mechanism for integrating highlevel governance with handson engineering practice and enabling accountable autonomy at scale
artificial intelligence in medicine is built to serve the average patient by minimizing error across large datasets most systems deliver strong aggregate accuracy yet falter at the margins patients with rare variants multimorbidity or underrepresented demographics this average patient fallacy erodes both equity and trust we propose a different design a multiagent ecosystem for nof decision support in this environment agents clustered by organ systems patient populations and analytic modalities draw on a shared library of models and evidence synthesis tools their results converge in a coordination layer that weighs reliability uncertainty and data density before presenting the clinician with a decisionsupport packet risk estimates bounded by confidence ranges outlier flags and linked evidence validation shifts from population averages to individual reliability measured by error in lowdensity regions calibration in the small and riskcoverage tradeoffs anticipated challenges include computational demands automation bias and regulatory fit addressed through caching strategies consensus checks and adaptive trial frameworks by moving from monolithic models to orchestrated intelligence this approach seeks to align medical ai with the first principle of medicine care that is transparent equitable and centered on the individual
for decades neuroscientists and computer scientists have pursued a shared ambition to understand intelligence and build it modern artificial neural networks now rival humans in language perception and reasoning yet it is still largely unknown whether these artificial systems organize information as the brain does existing brainai alignment studies have shown the striking correspondence between the two systems but such comparisons remain bound to specific inputs and tasks offering no common ground for comparing how ai models with different kinds of modalitiesvision language or multimodalare intrinsically organized here we introduce a groundbreaking concept of brainlike space a unified geometric space in which every ai model can be precisely situated and compared by mapping its intrinsic spatial attention topological organization onto canonical human functional brain networks regardless of input modality task or sensory domain our extensive analysis of transformerbased models spanning stateoftheart large vision models large language models and large multimodal models uncovers a continuous arcshaped geometry within this space reflecting a gradual increase of brainlikeness different models exhibit distinct distribution patterns within this geometry associated with different degrees of brainlikeness shaped not merely by their modality but by whether the pretraining paradigm asizes global semantic abstraction and whether the positional encoding scheme facilitates deep fusion across different modalities moreover the degree of brainlikeness for a model and its downstream task performance are not identical twins the brainlike space provides the first unified framework for situating quantifying and comparing intelligence across domains revealing the deep organizational principles that bridge machines and the brain
large language models llms become increasingly integrated into data science workflows for automated system design however these llmdriven data science systems rely solely on the internal reasoning of llms lacking guidance from scientific and theoretical principles this limits their trustworthiness and robustness especially when dealing with noisy and complex realworld datasets this paper provides vdsagents a multiagent system grounded in the predictabilitycomputabilitystability pcs principles proposed in the veridical data science vds framework guided by pcs principles the system implements a modular workflow for data cleaning feature engineering modeling and evaluation each phase is handled by an elegant agent incorporating perturbation analysis unit testing and model validation to ensure both functionality and scientific auditability we evaluate vdsagents on nine datasets with diverse characteristics comparing it with stateoftheart endtoend data science systems such as autokaggle and datainterpreter using deepseekv and gpto as backends vdsagents consistently outperforms the results of autokaggle and datainterpreter which validates the feasibility of embedding pcs principles into llmdriven data science automation
generative large language models gllms such as chatgpt are increasingly being used in communication research for content analysis studies show that gllms can outperform both crowd workers and trained coders such as research assistants on various coding tasks relevant to communication science often at a fraction of the time and cost additionally gllms can decode implicit meanings and contextual information be instructed using natural language deployed with only basic programming skills and require little to no annotated data beyond a validation dataset constituting a paradigm shift in automated content analysis despite their potential the integration of gllms into the methodological toolkit of communication research remains underdeveloped in gllmassisted quantitative content analysis researchers must address at least seven critical challenges that impact result quality codebook development prompt engineering model selection parameter tuning iterative refinement validation of the models reliability and optionally performance enhancement this paper synthesizes emerging research on gllmassisted quantitative content analysis and proposes a comprehensive bestpractice guide to navigate these challenges our goal is to make gllmbased content analysis more accessible to a broader range of communication researchers and ensure adherence to established disciplinary quality standards of validity reliability reproducibility and research ethics
judgmental forecasting is the task of making predictions about future events based on human judgment this task can be seen as a form of claim verification where the claim corresponds to a future event and the task is to assess the plausibility of that event in this paper we propose a novel multiagent framework for claim verification whereby different agents may disagree on claim veracity and bring specific evidence for and against the claims represented as quantitative bipolar argumentation frameworks qbafs we then instantiate the framework for supporting claim verification with a variety of agents realised with large language models llms argllm agents an existing approach for claim verification that generates and evaluates qbafs rbam agents whereby llmempowered relationbased argument mining rbam from external sources is used to generate qbafs ragargllm agents extending argllm agents with a form of retrievalaugmented generation rag of arguments from external sources finally we conduct experiments with two standard judgmental forecasting datasets with instances of our framework with two or three agents empowered by six different base llms we observe that combining evidence from agents can improve forecasting accuracy especially in the case of three agents while providing an explainable combination of evidence for claim verification
despite the strong reasoning ability of large language modelsllms they are prone to errors and hallucinations as a result how to check their outputs effectively and efficiently has become a critical problem in their applications existing checking methods heavily rely on external resources such as trained verifiers eg processoutcome reward models or elaborate prompts which lead to high computational overhead and are only applicable to specific domains in this paper we investigate whether the internal behaviors of llms have already implied the credibility of their reasoning paths specifically we find that the rank of the correlation matrix between the input problem and the output reasoning path is a robust indicator of reasoning correctness different from other correctness indicators for llms the calculation of the correlation matrix only relies on the llm itself which avoids the hassle of training a separate model or designing complicated prompts based on it we design a ple plugandplay selfindicator method to reweight candidate reasoning paths which achieves significant performance improvements than other voting and verification methods with very few computational overhead our experiments across multiple llms of varying scales and model families have further shown the effectiveness of selfindicator it achieves over accuracy in distinguishing correct reasoning paths from incorrect ones and in turn improves the accuracies on three reasoning benchmarks by more than
one weakness of monte carlo tree search mcts is its sample efficiency which can be addressed by building and using state andor action abstractions in parallel to the tree search such that information can be shared among nodes of the same layer the primary usage of abstractions for mcts is to enhance the upper confidence bound ucb value during the tree policy by aggregating visits and returns of an abstract node however this direct usage of abstractions does not take the case into account where multiple actions with the same parent might be in the same abstract node as these would then all have the same ucb value thus requiring a tiebreak rule in stateoftheart abstraction algorithms such as pruned on the go abstractions pruned oga this case has not been noticed and a random tiebreak rule was implicitly chosen in this paper we propose and empirically evaluate several alternative intraabstraction policies several of which outperform the random policy across a majority of environments and parameter settings
large language models llms increasingly rely on external tools to perform complex realistic tasks yet their ability to utilize the rapidly expanding model contextual protocol mcp ecosystem remains limited existing mcp research covers few servers depends on costly manual curation and lacks training support hindering progress toward realworld deployment to overcome these limitations we introduce mcpflow an automated webagentdriven pipeline for largescale server discovery data synthesis and model training mcpflow collects and filters data from servers and tools producing highquality instructionfunction call pairs and trajectories far exceeding prior work in scale and diversity extensive experiments demonstrate mcpflows effectiveness in driving superior mcp tool selection functioncall generation and enhanced agentic task performance mcpflow thus provides a scalable foundation for advancing llm agents proficiency in realworld mcp environments mcpflow is publicly available at href
the rapid progress of large language models llms and their multimodal extensions mllms has enabled agentic systems capable of perceiving and acting across diverse environments a challenging yet impactful frontier is the development of gui agents which must navigate complex desktop and web interfaces while maintaining robustness and generalization existing paradigms typically model tasks as longchain executions concatenating historical trajectories into the context while approaches such as mirage and gta refine planning or introduce multibranch action selection they remain constrained by two persistent issues dependence on historical trajectories which amplifies error propagation and local exploration bias where decisionfirst observationlater mechanisms overlook critical interface cues we introduce the memorydriven gui agent mga which reframes gui interaction around the principle of observe first then decide mga models each step as an independent contextrich environment state represented by a triad current screenshot taskagnostic spatial information and a dynamically updated structured memory experiments on osworld benchmarks real desktop applications chrome vscode vlc and crosstask transfer demonstrate that mga achieves substantial gains in robustness generalization and efficiency compared to stateoftheart baselines the code is publicly available at
motion planning is a critical component of autonomous vehicle decisionmaking systems directly determining trajectory safety and driving efficiency while deep learning approaches have advanced planning capabilities existing methods remain confined to singledataset training limiting their robustness in planning through systematic analysis we discover that vehicular trajectory distributions and historyfuture correlations demonstrate remarkable consistency across different datasets based on these findings we propose uniplanner the first planning framework designed for multidataset integration in autonomous vehicle decisionmaking uniplanner achieves unified crossdataset learning through three synergistic innovations first the historyfuture trajectory dictionary network hftdn aggregates historyfuture trajectory pairs from multiple datasets using historical trajectory ilarity to retrieve relevant futures and generate crossdataset planning guidance second the gradientfree trajectory mapper gftm learns robust historyfuture correlations from multiple datasets transforming historical trajectories into universal planning priors its gradientfree design ensures the introduction of valuable priors while preventing shortcut learning making the planning knowledge safely transferable third the sparsetodense sd paradigm implements adaptive dropout to selectively suppress planning priors during training for robust learning while enabling full prior utilization during inference to maximize planning performance
multimodal large language models mllms have advanced visionlanguage reasoning and are increasingly deployed in embodied agents however significant limitations remain mllms generalize poorly across digitalphysical spaces and embodiments visionlanguageaction models vlas produce lowlevel actions yet lack robust highlevel embodied reasoning and most embodied large language models ellms are constrained to digitalspace with poor generalization to the physical world thus unified models that operate seamlessly across digital and physical spaces while generalizing across embodiments and tasks remain absent we introduce the boundless large model blm a multimodal spatial foundation model that preserves instruction following and reasoning incorporates embodied knowledge and supports robust crossembodiment control blm integrates three key capabilities crossspace transfer crosstask learning and crossembodiment generalization via a twostage training paradigm stage i injects embodied knowledge into the mllm through curated digital corpora while maintaining language competence stage ii trains a policy module through an intentbridging interface that extracts highlevel semantics from the mllm to guide control without finetuning the mllm backbone this process is supported by a selfcollected crossembodiment demonstration suite spanning four robot embodiments and six progressively challenging tasks evaluations across digital and physical benchmarks show that a single blm instance outperforms four model families mllms ellms vlas and gmlms achieving simtextbf gains in digital tasks and simtextbf in physical tasks
building trainingready multihop question answering qa datasets that truly stress a models retrieval and reasoning abilities remains highly challenging recently while there have been a few recent evaluation datasets that capture the characteristics of hardtosearch but easytoverify problems requiring the integration of ambiguous indirect and crossdomain cues these data resources remain scarce and are mostly designed for evaluation making them unsuitable for supervised finetuning sft or reinforcement learning rl meanwhile manually curating nontrivially retrievable questions where answers cannot be found through a single direct query but instead require multihop reasoning over oblique and loosely connected evidence incurs prohibitive human costs and fails to scale creating a critical data bottleneck for training highcapability retrievalandreasoning agents to address this we present an automated framework for generating highdifficulty trainingready multihop questions from semistructured knowledge sources the system i grows diverse logically labeled evidence clusters through natural language inference nlibased relation typing and diversityaware expansion ii applies reverse question construction to compose oblique cues so that isolated signals are underinformative but their combination uniquely identifies the target entity and iii enforces quality with a twostep evaluation pipeline that combines multimodel consensus filtering with structured constraint decomposition and evidencebased matching the result is a scalable process that yields complex retrievalresistant yet verifiable questions suitable for sftrl training as well as challenging evaluation substantially reducing human curation effort while preserving the difficulty profile of strong evaluation benchmarks
incident management im is central to the reliability of largescale cloud systems yet manual im where oncall engineers examine metrics logs and traces is laborintensive and errorprone in the face of massive and heterogeneous observability data existing automated im approaches often struggle to generalize across systems provide limited interpretability and incur high deployment costs which hinders adoption in practice in this paper we present opsagent a lightweight selfevolving multiagent system for im that employs a trainingfree data processor to convert heterogeneous observability data into structured textual descriptions along with a multiagent collaboration framework that makes diagnostic inference transparent and auditable to support continual capability growth opsagent also introduces a dual selfevolution mechanism that integrates internal model updates with external experience accumulation thereby closing the deployment loop comprehensive experiments on the openrca benchmark demonstrate stateoftheart performance and show that opsagent is generalizable interpretable costefficient and selfevolving making it a practically deployable and sustainable solution for longterm operation in realworld cloud systems
for doctors to truly trust artificial intelligence it cant be a black box they need to understand its reasoning almost as if they were consulting a colleague we created histolens to be that transparent collaborative partner it allows a pathologist to ply ask a question in plain english about a tissue slidejust as they would ask a trainee our system intelligently translates this question into a precise query for its ai engine which then provides a clear structured report but it doesnt stop there if a doctor ever asks why histolens can instantly provide a visual proof for any findinga heatmap that points to the exact cells and regions the ai used for its analysis weve also ensured the ai focuses only on the patients tissue just like a trained pathologist would by teaching it to ignore distracting background noise the result is a workflow where the pathologist remains the expert in charge using a trustworthy ai assistant to verify their insights and make faster more confident diagnoses
the increasing adoption of electric vehicles evs necessitates an understanding of their driving behavior to enhance traffic safety and develop smart driving systems this study compares classical and machine learning models for ev car following behavior classical models include the intelligent driver model idm optimum velocity model ovm optimal velocity relative velocity ovrv and a plified cacc model while the machine learning approach employs a random forest regressor using a real world dataset of an ev following an internal combustion engine ice vehicle under varied driving conditions we calibrated classical model parameters by minimizing the rmse between predictions and real data the random forest model predicts acceleration using spacing speed and gap type as inputs results demonstrate the random forests superior accuracy achieving rmses of medium gap long gap and extra long gap among physics based models cacc performed best with an rmse of for long gaps these findings highlight the machine learning models performance across all scenarios such models are valuable for ulating ev behavior and analyzing mixed autonomy traffic dynamics in ev integrated environments
system logs are a cornerstone of cybersecurity supporting proactive breach prevention and postincident investigations however analyzing vast amounts of diverse log data remains significantly challenging as high costs lack of inhouse expertise and time constraints make even basic analysis difficult for many organizations this study introduces llmloganalyzer a clusteringbased log analysis chatbot that leverages large language models llms and machine learning ml algorithms to plify and streamline log analysis processes this innovative approach addresses key llm limitations including context window constraints and poor structured text handling capabilities enabling more effective summarization pattern extraction and anomaly detection tasks llmloganalyzer is evaluated across four distinct domain logs and various tasks results demonstrate significant performance improvements over stateoftheart llmbased chatbots including chatgpt chatpdf and notebooklm with consistent gains ranging from to across different tasks the system also exhibits strong robustness achieving a reduction in interquartile range iqr when using rouge scores indicating significantly lower result variability the frameworks effectiveness stems from its modular architecture comprising a router log recognizer log parser and search tools this design enhances llm capabilities for structured text analysis while improving accuracy and robustness making it a valuable resource for both cybersecurity experts and nontechnical users
crossdomain time series forecasting is a valuable task in various web applications despite its rapid advancement achieving effective generalization across heterogeneous time series data remains a significant challenge existing methods have made progress by extending singledomain models yet often fall short when facing domainspecific trend shifts and inconsistent periodic patterns we argue that a key limitation lies in treating temporal series as undifferentiated sequence without explicitly decoupling their inherent structural components to address this we propose onecast a structured and modular forecasting framework that decomposes time series into seasonal and trend components each modeled through tailored generative pathways specifically the seasonal component is captured by a lightweight projection module that reconstructs periodic patterns via interpretable basis functions in parallel the trend component is encoded into discrete tokens at segment level via a semanticaware tokenizer and subsequently inferred through a masked discrete diffusion mechanism the outputs from both branches are combined to produce a final forecast that captures seasonal patterns while tracking domainspecific trends extensive experiments across eight domains demonstrate that onecast mostly outperforms stateoftheart baselines
our study contributes to the scheduling and combinatorial optimization literature with new heuristics discovered by leveraging the power of large language models llms we focus on the singlemachine total tardiness smtt problem which aims to minimize total tardiness by sequencing n jobs on a single processor without preemption given processing times and due dates we develop and benchmark two novel llmdiscovered heuristics the edd challenger eddc and mdd challenger mddc inspired by the wellknown earliest due date edd and modified due date mdd rules in contrast to prior studies that employed pler rulebased heuristics we evaluate our llmdiscovered algorithms using rigorous criteria including optimality gaps and solution time derived from a mixedinteger programming mip formulation of smtt we compare their performance against stateoftheart heuristics and exact methods across various job sizes and jobs for instances with more than jobs exact methods such as mip and dynamic programming become computationally intractable up to jobs eddc improves upon the classic edd rule and another widely used algorithm in the literature mddc consistently outperforms traditional heuristics and remains competitive with exact approaches particularly on larger and more complex instances this study shows that humanllm collaboration can produce scalable highperforming heuristics for nphard constrained combinatorial optimization even under limited resources when effectively configured
shifts in individual movement patterns following disruptive events can reveal changing demands for community resources however predicting such shifts before disruptive events remains challenging for several reasons first measures are lacking for individuals heterogeneous social infrastructure resilience sir which directly influences their movement patterns and commonly used features are often limited or unavailable at scale eg sociodemographic characteristics second the complex interactions between individual movement patterns and spatial contexts have not been sufficiently captured third individuallevel movement may be spatially sparse and not wellsuited to traditional decisionmaking methods for movement predictions this study incorporates individuals sir into a conditioned deep learning model to capture the complex relationships between individual movement patterns and local spatial context using largescale sparse individuallevel data our experiments demonstrate that incorporating individuals sir and spatial context can enhance the models ability to predict postevent individual movement patterns the conditioned model can capture the divergent shifts in movement patterns among individuals who exhibit ilar preevent patterns but differ in sir
traditional llm alignment methods are vulnerable to heterogeneity in human preferences fitting a nave probabilistic model to pairwise comparison data say over promptcompletion pairs yields an inconsistent estimate of the populationaverage utility a canonical measure of social welfare we propose a new method dubbed the sign estimator that provides a ple provably consistent and efficient estimator by replacing crossentropy with binary classification loss in the aggregation step this ple modification recovers consistent ordinal alignment under mild assumptions and achieves the first polynomial finitesample error bounds in this setting in realistic ulations of llm alignment using digital twins the sign estimator substantially reduces preference distortion over a panel of ulated personas cutting angular estimation error by nearly and decreasing disagreement with true population preferences from to compared to standard rlhf our method also compares favorably to panel data heuristics that explicitly model user heterogeneity and require tracking individuallevel preference dataall while maintaining the implementation plicity of existing llm alignment pipelines
we describe a theory and implementation of an intuitionistic decentralized framework for causal discovery using judo calculus which is formally defined as jstable causal inference using jdocalculus in a topos of sheaves in realworld applications from biology to medicine and social science causal effects depend on regime age country dose genotype or lab protocol our proposed judo calculus formalizes this context dependence formally as local truth a causal claim is proven true on a cover of regimes not everywhere at once the lawveretierney modal operator j chooses which regimes are relevant jstability means the claim holds constructively and consistently across that family we describe an algorithmic and implementation framework for judo calculus combining it with standard scorebased constraintbased and gradientbased causal discovery methods we describe experimental results on a range of domains from synthetic to realworld datasets from biology and economics our experimental results show the computational efficiency gained by the decentralized nature of sheaftheoretic causal discovery as well as improved performance over classical causal discovery methods
chainofthought cot reasoning is critical for improving the interpretability and reliability of large visionlanguage models lvlms however existing training algorithms such as sft ppo and grpo may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model to address this challenge we reformulate reasoning in lvlms as posterior inference and propose a scalable training algorithm based on amortized variational inference by leveraging diversityseeking reinforcement learning algorithms we introduce a novel sparse reward function for tokenlevel learning signals that encourage diverse highlikelihood latent cot overcoming deterministic sampling limitations and avoiding reward hacking additionally we implement a bayesian inferencescaling strategy that replaces costly bestofn and beam search with a marginal likelihood to efficiently rank optimal rationales and answers we empirically demonstrate that the proposed method enhances the stateoftheart lvlms on seven reasoning benchmarks in terms of effectiveness generalization and interpretability
agentic ai systems powered by large language models llms and endowed with planning tool use memory and autonomy are emerging as powerful flexible platforms for automation their ability to autonomously execute tasks across web software and physical environments creates new and amplified security risks distinct from both traditional ai safety and conventional software security this survey outlines a taxonomy of threats specific to agentic ai reviews recent benchmarks and evaluation methodologies and discusses defense strategies from both technical and governance perspectives we synthesize current research and highlight open challenges aiming to support the development of securebydesign agent systems
this work investigates the use of digital twins for dynamical system modeling and control integrating physicsbased datadriven and hybrid approaches with both traditional and aidriven controllers using a miniature greenhouse as a test platform four predictive models linear physicsbased modeling pbm long short term memory lstm and hybrid analysis and modeling ham are developed and compared under interpolation and extrapolation scenarios three control strategies model predictive control mpc reinforcement learning rl and large language model llm based control are also implemented to assess tradeoffs in precision adaptability and implementation effort results show that in modeling ham provides the most balanced performance across accuracy generalization and computational efficiency while lstm achieves high precision at greater resource cost among controllers mpc delivers robust and predictable performance rl demonstrates strong adaptability and llmbased controllers offer flexible humanai interaction when coupled with predictive tools
while generative ai rapidly advances in various domains generating truly creative aesthetic and counterintuitive outputs remains a challenge this paper presents an approach to tackle these difficulties in the domain of chess puzzles we start by benchmarking generative ai architectures and then introduce an rl framework with novel rewards based on chess engine search statistics to overcome some of those shortcomings the rewards are designed to enhance a puzzles uniqueness counterintuitiveness diversity and realism our rl approach dramatically increases counterintuitive puzzle generation by x from supervised to surpassing existing dataset rates and the best lichesstrained model our puzzles meet novelty and diversity benchmarks retain aesthetic themes and are rated by human experts as more creative enjoyable and counterintuitive than composed book puzzles even approaching classic compositions our final outcome is a curated booklet of these aigenerated puzzles which is acknowledged for creativity by three worldrenowned experts
agents are rapidly advancing in automating digital work but enterprises face a harder challenge moving beyond prototypes to deployed systems that deliver measurable business value this path is complicated by fragmented frameworks slow development and the absence of standardized evaluation practices generalist agents have emerged as a promising direction excelling on academic benchmarks and offering flexibility across task types applications and modalities yet evidence of their use in production enterprise settings remains limited this paper reports ibms experience developing and piloting the computer using generalist agent cuga which has been opensourced for the community cuga adopts a hierarchical plannerexecutor architecture with strong analytical foundations achieving stateoftheart performance on appworld and webarena beyond benchmarks it was evaluated in a pilot within the businessprocessoutsourcing talent acquisition domain addressing enterprise requirements for scalability auditability safety and governance to support assessment we introduce bpota a task benchmark spanning analytics endpoints in preliminary evaluations cuga approached the accuracy of specialized agents while indicating potential for reducing development time and cost our contribution is twofold presenting early evidence of generalist agents operating at enterprise scale and distilling technical and organizational lessons from this initial pilot we outline requirements and next steps for advancing researchgrade architectures like cuga into robust enterpriseready systems
coordinating multiple autonomous agents in shared environments under decentralized conditions is a longstanding challenge in robotics and artificial intelligence this work addresses the problem of decentralized goal assignment for multiagent path planning where agents independently generate ranked preferences over goals based on structured representations of the environment including grid visualizations and scenario data after this reasoning phase agents exchange their goal rankings and assignments are determined by a fixed deterministic conflictresolution rule eg agent index ordering without negotiation or iterative coordination we systematically compare greedy heuristics optimal assignment and large language model llmbased agents in fully observable gridworld settings our results show that llmbased agents when provided with welldesigned prompts and relevant quantitative information can achieve nearoptimal makespans and consistently outperform traditional heuristics these findings underscore the potential of language models for decentralized goal assignment in multiagent path planning and highlight the importance of information structure in such systems
longhorizon tasks requiring multistep reasoning and dynamic replanning remain challenging for large language models llms sequential prompting methods are prone to context drift loss of goal information and recurrent failure cycles while hierarchical prompting methods often weaken crosslevel continuity or incur substantial runtime overhead we introduce recap recursive contextaware reasoning and planning a hierarchical framework with shared context for reasoning and planning in llms recap combines three key mechanisms i planahead decomposition in which the model generates a full subtask list executes the first item and refines the remainder ii structured reinjection of parent plans maintaining consistent multilevel context during recursive return and iii memoryefficient execution bounding the active prompt so costs scale linearly with task depth together these mechanisms align highlevel goals with lowlevel actions reduce redundant prompting and preserve coherent context updates across recursion experiments demonstrate that recap substantially improves subgoal alignment and success rates on various longhorizon reasoning benchmarks achieving a gain on synchronous robotouille and a improvement on asynchronous robotouille under the strict pass protocol
in nonmedical domains foundation models fms have revolutionized computer vision and language processing through largescale selfsupervised and multimodal learning consequently their rapid adoption in computational pathology was expected to deliver comparable breakthroughs in cancer diagnosis prognostication and multimodal retrieval however recent systematic evaluations reveal fundamental weaknesses low diagnostic accuracy poor robustness geometric instability heavy computational demands and concerning safety vulnerabilities this short paper examines these shortcomings and argues that they stem from deeper conceptual mismatches between the assumptions underlying generic foundation modeling in mainstream ai and the intrinsic complexity of human tissue seven interrelated causes are identified biological complexity ineffective selfsupervision overgeneralization excessive architectural complexity lack of domainspecific innovation insufficient data and a fundamental design flaw related to tissue patch size these findings suggest that current pathology foundation models remain conceptually misaligned with the nature of tissue morphology and call for a fundamental rethinking of the paradigm itself
the rapid advancement of generative ai has raised significant questions regarding its ability to produce creative and novel outputs our recent work investigates this question within the domain of chess puzzles and presents an ai system designed to generate puzzles characterized by aesthetic appeal novelty counterintuitive and unique solutions we briefly discuss our method below and refer the reader to the technical paper for more details to assess our systems creativity we presented a curated booklet of aigenerated puzzles to three worldrenowned experts international master for chess compositions amatzia avni grandmaster jonathan levitt and grandmaster matthew sadler all three are noted authors on chess aesthetics and the evolving role of computers in the game they were asked to select their favorites and explain what made them appealing considering qualities such as their creativity level of challenge or aesthetic design
tandem mass spectrometry enables the identification of unknown compounds in crucial fields such as metabolomics natural product discovery and environmental analysis however current methods rely on database matching from previously observed molecules or on multistep pipelines that require intermediate fragment or fingerprint prediction this makes finding the correct molecule highly challenging particularly for compounds absent from reference databases we introduce a framework that by leveraging testtime tuning enhances the learning of a pretrained transformer model to address this gap enabling endtoend de novo molecular structure generation directly from the tandem mass spectra and molecular formulae bypassing manual annotations and intermediate steps we surpass the defacto stateoftheart approach diffms on two popular benchmarks nplib and massspecgym by and respectively testtime tuning on experimental spectra allows the model to dynamically adapt to novel spectra and the relative performance gain over conventional finetuning is of on massspecgym when predictions deviate from the ground truth the generated molecular candidates remain structurally accurate providing valuable guidance for human interpretation and more reliable identification
multienvironment pomdps mepomdps extend standard pomdps with discrete model uncertainty mepomdps represent a finite set of pomdps that share the same state action and observation spaces but may arbitrarily vary in their transition observation and reward models such models arise for instance when multiple domain experts disagree on how to model a problem the goal is to find a single policy that is robust against any choice of pomdp within the set ie a policy that maximizes the worstcase reward across all pomdps we generalize and expand on existing work in the following way first we show that mepomdps can be generalized to pomdps with sets of initial beliefs which we call adversarialbelief pomdps abpomdps second we show that any arbitrary mepomdp can be reduced to a mepomdp that only varies in its transition and reward functions or only in its observation and reward functions while preserving optimal policies we then devise exact and imate pointbased algorithms to compute robust policies for abpomdps and thus mepomdps we demonstrate that we can compute policies for standard pomdp benchmarks extended to the multienvironment setting
this paper examines the role of artificial intelligence in scientific problemsolving with a focus on its implications for disciplinary creativity drawing on recent work in the philosophy of creativity i distinguish between creative approaches and creative products and introduce the concept of disciplinary creativity the creative application of disciplinespecific expertise to a valued problem within that field through two cases in mathematics i show that while computation can extend disciplinary creativity certain approaches involving ai can serve to displace it this displacement has the potential to alter and perhaps diminish the value of scientific pursuit
we present gametars a generalist game agent trained with a unified scalable action space anchored to humanaligned native keyboardmouse inputs unlike api or guibased approaches this paradigm enables largescale continual pretraining across heterogeneous domains including os web and ulation games gametars is pretrained on over b tokens with diverse trajectories and multimodal data key techniques include a decaying continual loss to reduce causal confusion and an efficient sparsethinking strategy that balances reasoning depth and inference cost experiments show that gametars achieves about times the success rate over the previous sota model on openworld minecraft tasks is close to the generality of fresh humans in unseen web d games and outperforms gpt geminipro and claudesonnet in fps benchmarks scaling results on trainingtime and testtime confirm that the unified action space sustains improvements when scaled to crossgame and multimodal data our results demonstrate that ple scalable action representations combined with largescale pretraining provide a promising path toward generalist agents with broad computeruse abilities
object binding the brains ability to bind the many features that collectively represent an object into a coherent whole is central to human cognition it groups lowlevel perceptual features into highlevel object representations stores those objects efficiently and compositionally in memory and supports human reasoning about individual object instances while prior work often imposes objectcentric attention eg slot attention explicitly to probe these benefits it remains unclear whether this ability naturally emerges in pretrained vision transformers vits intuitively they could recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention motivated by the quadratic nature of selfattention we hypothesize that vits represent whether two patches belong to the same object a property we term issameobject we decode issameobject from patch embeddings across vit layers using a ilarity probe which reaches over accuracy crucially this objectbinding capability emerges reliably in selfsupervised vits dino mae clip but markedly weaker in imagenetsupervised models suggesting that binding is not a trivial architectural artifact but an ability acquired through specific pretraining objectives we further discover that issameobject is encoded in a lowdimensional subspace on top of object features and that this signal actively guides attention ablating issameobject from model activations degrades downstream performance and works against the learning objective implying that emergent object binding naturally serves the pretraining objective our findings challenge the view that vits lack object binding and highlight how symbolic knowledge of which parts belong together emerges naturally in a connectionist system
virtual reality vr games require players to translate highlevel semantic actions into precise device manipulations using controllers and headmounted displays hmds while humans intuitively perform this translation based on common sense and embodied understanding whether large language models llms can effectively replicate this ability remains underexplored this paper introduces a benchmark combobench evaluating llms capability to translate semantic actions into vr device manipulation sequences across scenarios from four popular vr games halflife alyx into the radius moss book ii and vivecraft we evaluate seven llms including gpt gpt gpto geminipro llamab mixtralxb and glmflash compared against annotated ground truth and human performance our results reveal that while topperforming models like geminipro demonstrate strong task decomposition capabilities they still struggle with procedural reasoning and spatial understanding compared to humans performance varies significantly across games suggesting sensitivity to interaction complexity fewshot examples substantially improve performance indicating potential for targeted enhancement of llms vr manipulation capabilities we release all materials at
large language models llms have been shown to perform better when scaffolded into agents with memory tools and feedback beyond this selfevolving agents have emerged but current work largely limits adaptation to prompt rewriting or failure retries therefore we present alitag a selfevolution framework that transforms a generalpurpose agent into a domain expert by systematically generating abstracting and curating model context protocol mcp tools in this framework a generalist agent executes a curated suite of targetdomain tasks and synthesizes candidate mcps from successful trajectories these are then abstracted to parameterized primitives and consolidated into an mcp box at inference time alitag performs retrievalaugmented mcp selection with the help of each tools descriptions and use cases before executing an agent equipped with the mcp executor across several benchmarks gaia pathvqa and humanitys last exam alitag attains strong gains while reducing computation costs on gaia validation it achieves pass and pass establishing a new stateoftheart result while reducing mean tokens per example by imately relative to a strong baseline agent alitag thus provides a principled pathway from generalist capability to reusable domainspecific competence improving both accuracy and efficiency on complex reasoning tasks
reinforcement learning rl has demonstrated significant potential in enhancing the reasoning capabilities of large language models llms however the success of rl for llms heavily relies on humancurated datasets and verifiable rewards which limit their scalability and generality recent selfplay rl methods inspired by the success of the paradigm in games and go aim to enhance llm reasoning capabilities without humanannotated data however their methods primarily depend on a grounded environment for feedback eg a python interpreter or a game engine extending them to general domains remains challenging to address these challenges we propose multiagent evolve mae a framework that enables llms to selfevolve in solving diverse tasks including mathematics reasoning and general knowledge qa the core design of mae is based on a triplet of interacting agents proposer solver judge that are instantiated from a single llm and applies reinforcement learning to optimize their behaviors the proposer generates questions the solver attempts solutions and the judge evaluates both while coevolving experiments on qwenbinstruct demonstrate that mae achieves an average improvement of on multiple benchmarks these results highlight mae as a scalable dataefficient method for enhancing the general reasoning abilities of llms with minimal reliance on humancurated supervision
the rapid adoption of generative artificial intelligence genai technologies has led many organizations to integrate ai into their products and services often without considering user preferences yet public attitudes toward ai use especially in impactful decisionmaking scenarios are underexplored using a largescale twowave survey study nwave nwave representative of the swiss population we examine shifts in public attitudes toward ai before and after the launch of chatgpt we find that the genai boom is significantly associated with reduced public acceptance of ai see figure and increased demand for human oversight in various decisionmaking contexts the proportion of respondents finding ai not acceptable at all increased from to while support for humanonly decisionmaking rose from to these shifts have amplified existing social inequalities in terms of widened educational linguistic and gender gaps postboom our findings challenge industry assumptions about public readiness for ai deployment and highlight the critical importance of aligning technological development with evolving public preferences
realworld tasks require decisions at varying granularities and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a highlevel form of action however current large language model llmbased agents lack this crucial capability to operate fluidly across decision granularities this limitation stems from existing paradigms that enforce a rigid separation between highlevel planning and lowlevel action which impairs dynamic adaptability and limits generalization we propose recode recursive code generation a novel paradigm that addresses this limitation by unifying planning and action within a single code representation in this representation recode treats highlevel plans as abstract placeholder functions which the agent then recursively decomposes into finergrained subfunctions until reaching primitive actions this recursive approach dissolves the rigid boundary between plan and action enabling the agent to dynamically control its decision granularity furthermore the recursive structure inherently generates rich multigranularity training data enabling models to learn hierarchical decisionmaking processes extensive experiments show recode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control the code is available at
as human machine teaming becomes central to paradigms like industry a critical need arises for machines to safely and effectively interpret complex human behaviors a research gap currently exists between techno centric robotic frameworks which often lack nuanced models of human behavior and descriptive behavioral ontologies which are not designed for real time collaborative interpretation this paper addresses this gap by presenting ontopret an ontology for the interpretation of human behavior grounded in cognitive science and a modular engineering methodology ontopret provides a formal machine processable framework for classifying behaviors including task deviations and deceptive actions we demonstrate its adaptability across two distinct use cases manufacturing and gameplay and establish the semantic foundations necessary for advanced reasoning about human intentions
the scope of neural code intelligence is rapidly expanding beyond textbased source code to encompass the rich visual outputs that programs generate this visual dimension is critical for advanced applications like flexible content generation and precise programdriven editing of visualizations however progress has been impeded by the scarcity of highquality multimodal code data a bottleneck stemming from challenges in synthesis and quality assessment to address these challenges we make contributions from both a data and modeling perspective we first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a largescale highquality corpus spanning from standard charts to complex interactive web uis and codedriven animations leveraging this toolkit we construct januscodek the largest multimodal code corpus to date this powers the training of our models januscoder and januscoderv which establish a visualprogrammatic interface for generating code from textual instructions visual inputs or a combination of both our unified model is a departure from existing approaches that build specialized models for isolated tasks extensive experiments on both textcentric and visioncentric coding tasks demonstrate the superior performance of the januscoder series with our b to b scale models approaching or even exceeding the performance of commercial models furthermore extensive analysis provides key insights into harmonizing programmatic logic with its visual expression our code and checkpoints will are available at
designing models that can learn to reason in a systematic way is an important and longstanding challenge in recent years a wide range of solutions have been proposed for the specific case of systematic relational reasoning including neurosymbolic approaches variants of the transformer architecture and specialised graph neural networks however existing benchmarks for systematic relational reasoning focus on an overly plified setting based on the assumption that reasoning can be reduced to composing relational paths in fact this assumption is hardbaked into the architecture of several recent models leading to approaches that can perform well on existing benchmarks but are difficult to generalise to other settings to support further progress in the field of systematic relational reasoning with neural networks we introduce nora a new benchmark which adds several levels of difficulty and requires models to go beyond pathbased reasoning
the rapid advancement of artificial intelligence ai has led to unprecedented computational demands raising significant environmental and ethical concerns this paper critiques the prevailing reliance on largescale static datasets and monolithic training paradigms advocating for a shift toward humaninspired sustainable ai solutions we introduce a novel framework human ai hai which asizes incremental learning carbonaware optimization and humanintheloop collaboration to enhance adaptability efficiency and accountability by drawing parallels with biological cognition and leveraging dynamic architectures hai seeks to balance performance with ecological responsibility we detail the theoretical foundations system design and operational principles that enable ai to learn continuously and contextually while minimizing carbon footprints and human annotation costs our approach addresses pressing challenges in active learning continual adaptation and energyefficient model deployment offering a pathway toward responsible humancentered artificial intelligence
the recent advancement of multimodal large language models mllms is transforming humancomputer interaction hci from surfacelevel exchanges into more nuanced and emotionally intelligent communication to realize this shift emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent furthermore providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust however current mllmbased methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions this inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings to address this we propose a novel approach the emotional rationale verifier erv and an explanation reward our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired videodescription annotations our method significantly improves faithful explanationprediction consistency and explanation emotion accuracy on the mafw and dfew datasets through extensive experiments and human evaluations we show that our approach not only enhances alignment between explanation and prediction but also empowers mllms to deliver emotionally coherent trustworthy interactions marking a key step toward truly humanlike hci systems
this paper establishes a formal equivalence between the architectural classes of modern agentic ai systems and the abstract machines of the chomsky hierarchy we posit that the memory architecture of an ai agent is the definitive feature determining its computational power and that it directly maps it to a corresponding class of automaton specifically we demonstrate that ple reflex agents are equivalent to finite automata hierarchical taskdecomposition agents are equivalent to pushdown automata and agents employing readablewritable memory for reflection are equivalent to tms this automataagent framework provides a principled methodology for rightsizing agent architectures to optimize computational efficiency and cost more critically it creates a direct pathway to formal verification enables the application of mature techniques from automata theory to guarantee agent safety and predictability by classifying agents we can formally delineate the boundary between verifiable systems and those whose behavior is fundamentally undecidable we address the inherent probabilistic nature of llmbased agents by extending the framework to probabilistic automata that allow quantitative risk analysis the paper concludes by outlining an agenda for developing static analysis tools and grammars for agentic frameworks
ai predictive systems are increasingly embedded in decision making pipelines shaping high stakes choices once made solely by humans yet robust decisions under uncertainty still rely on capabilities that current ai lacks domain knowledge not captured by data long horizon context and reasoning grounded in the physical world this gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and ai this work advances this vision by identifying the fundamental principles of human ai collaboration within uncertainty quantification a key component of reliable decision making we introduce human ai collaborative uncertainty quantification a framework that formalizes how an ai model can refine a human experts proposed prediction set with two goals avoiding counterfactual harm ensuring the ai does not degrade correct human judgments and complementarity enabling recovery of correct outcomes the human missed at the population level we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function extending a classical result in conformal prediction building on this insight we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees the online method adapts to distribution shifts including human behavior evolving through interaction with ai a phenomenon we call human to ai adaptation experiments across image classification regression and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone achieving higher coverage and smaller set sizes across various conditions
enterprises need access decisions that satisfy least privilege comply with regulations and remain auditable we present a policy aware controller that uses a large language model llm to interpret natural language requests against written policies and metadata not raw data the system implemented with google gemini flash executes a sixstage reasoning framework context interpretation user validation data classification business purpose test compliance mapping and risk synthesis with early hard policy gates and deny by default it returns approve deny conditional together with cited controls and a machine readable rationale we evaluate on fourteen canonical cases across seven scenario families using a privacy preserving benchmark results show exact decision match improving from to after applying policy gates deny recall rising to false approval rate on mustdeny families dropping to and functional appropriateness and compliance adherence at expert ratings of rationale quality are high and median latency is under one minute these findings indicate that policy constrained llm reasoning combined with explicit gates and audit trails can translate human readable policies into safe compliant and traceable machine decisions
this work is a commentary of the article href ai survival stories a taxonomic analysis of ai existential risk by cappelen goldstein and hawthorne it is not just a commentary though but a useful reminder of the philosophical limitations of saylinear models of risk the article will focus on the model employed by the authors first i discuss some differences between standard swiss cheese models and this one i then argue that in a situation of epistemic indifference the probability of pd is higher than what one might first suggest given the structural relationships between layers i then distinguish between risk and uncertainty and argue that any estimation of pd is structurally affected by two kinds of uncertainty option uncertainty and statespace uncertainty incorporating these dimensions of uncertainty into our qualitative discussion on ai existential risk can provide a better understanding of the likeliness of pd
the growing intersection of cybersecurity and law creates a complex information space where traditional legal research tools struggle to deal with nuanced connections between cases statutes and technical vulnerabilities this knowledge divide hinders collaboration between legal experts and cybersecurity professionals to address this important gap this work provides a first step towards intelligent systems capable of navigating the increasingly intricate cyberlegal domain we demonstrate promising initial results on multilingual tasks
deep q networks dqn have shown remarkable success in various reinforcement learning tasks however their reliance on associative learning often leads to the acquisition of spurious correlations hindering their problemsolving capabilities in this paper we introduce a novel approach to integrate causal principles into dqns leveraging the peace probabilistic easy variational causal effect formula for estimating causal effects by incorporating causal reasoning during training our proposed framework enhances the dqns understanding of the underlying causal structure of the environment thereby mitigating the influence of confounding factors and spurious correlations we demonstrate that integrating dqns with causal capabilities significantly enhances their problemsolving capabilities without compromising performance experimental results on standard benchmark environments showcase that our approach outperforms conventional dqns highlighting the effectiveness of causal reasoning in reinforcement learning overall our work presents a promising avenue for advancing the capabilities of deep reinforcement learning agents through principled causal inference
autobidding is crucial in facilitating online advertising by automatically providing bids for advertisers while previous work has made great efforts to model bidding environments for better ad performance it has limitations in generalizability across environments since these models are typically tailored for specific bidding scenarios to this end we approach the scenarioindependent principles through a unified function that estimates the achieved effect under specific bids such as budget consumption gross merchandise volume gmv page views etc then we propose a bidding foundation model bidx to learn this fundamental function from data in various scenarios our bidx is built over uniform series embeddings that encode heterogeneous data through tailored embedding methods to capture complex intervariable and dynamic temporal dependencies in bidding data we propose two attention mechanisms separately treating embeddings of different variables and embeddings at different times as attention tokens for representation learning on top of the learned variable and temporal representations a variableaware fusion module is used to perform adaptive bidding outcome prediction to model the unique bidding data distribution we devise a zeroinflated projection module to incorporate the estimated nonzero probability into its value prediction which makes up a joint optimization objective containing classification and regression the objective is proven to converge to the zeroinflated distribution our model has been deployed on the ad platform in taobao one of the worlds largest ecommerce platforms offline evaluation on eight datasets exhibits bidxs superiority compared to various baselines and its generality across different scenarios bidx increased gmv by and roi by in online ab tests paving the way for bidding foundation model in computational advertising
data pipelines are essential in stream processing as they enable the efficient collection processing and delivery of realtime data supporting rapid data analysis in this paper we present autostreampipe a novel framework that employs large language models llms to automate the design generation and deployment of stream processing pipelines autostreampipe bridges the semantic gap between highlevel user intent and platformspecific implementations across distributed stream processing systems for structured multiagent reasoning by integrating a hypergraph of thoughts hgot as an extended version of got autostreampipe combines resilient execution strategies advanced query analysis and hgot to deliver pipelines with good accuracy experimental evaluations on diverse pipelines demonstrate that autostreampipe significantly reduces development time x and error rates x as measured by a novel errorfree score efs compared to llm codegeneration methods
opinions are central to almost all human activities and are key influencers of our behaviors in current times due to growth of social networking website and increase in number of ecommerce site huge amount of opinions are now available on web given a set of evaluative statements that contain opinions or sentiments about an entity opinion mining aims to extract attributes and components of the object that have been commented on in each statement and to determine whether the comments are positive negative or neutral while lot of research recently has been done in field of opinion mining and some of it dealing with ranking of entities based on review or opinion set classifying opinions into finer granularity level and then ranking entities has never been done before in this paper method for opinion mining from statements at a deeper level of granularity is proposed this is done by using fuzzy logic reasoning after which entities are ranked as per this information
adaptive agent design offers a way to improve humanai collaboration on timesensitive tasks in rapidly changing environments in such cases to ensure the human maintains an accurate understanding of critical task elements an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively given that human attention represents a zerosum cognitive resource where focus on one message diminishes awareness of other or upcoming information we introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication formalised as bayesian reference resolution using the rational speech act rsa modelling framework to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment the agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how priorguided interpretation of messages will influence attention to the interface and subsequent belief update across several timesteps out to a fixed horizon in a comparison to baseline methods we show that this effectiveness depends crucially on combining multistep planning with a realistic model of user awareness as the first application of rsa for communication in a dynamic environment and for humanai interaction in general we establish theoretical foundations for pragmatic communication in humanagent teams highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents
cnot gates are fundamental to quantum computing as they facilitate entanglement a crucial resource for quantum algorithms certain classes of quantum circuits are constructed exclusively from cnot gates given their widespread use it is imperative to minimise the number of cnot gates employed this problem known as cnot minimisation remains an open challenge with its computational complexity yet to be fully characterised in this work we introduce a novel reinforcement learning approach to address this task instead of training multiple reinforcement learning agents for different circuit sizes we use a single agent up to a fixed size m matrices of sizes different from m are preprocessed using either embedding or gaussian striping to assess the efficacy of our approach we trained an agent with m and evaluated it on matrices of size n that range from to the results we obtained show that our method overperforms the stateoftheart algorithm as the value of n increases
recent advances in datadriven approaches such as neural operators nos have shown substantial efficacy in reducing the solution time for integrated circuit ic thermal ulations however a limitation of these approaches is requiring a large amount of highfidelity training data such as chip parameters and temperature distributions thereby incurring significant computational costs to address this challenge we propose a novel algorithm for the generation of ic thermal ulation data named block krylov and operator action blockoa which ultaneously accelerates the data generation process and enhances the precision of generated data blockoa is specifically designed for ic applications initially we use the block krylov algorithm based on the structure of the heat equation to quickly obtain a few basic solutions then we combine them to get numerous temperature distributions that satisfy the physical constraints finally we apply heat operators on these functions to determine the heat source distributions efficiently generating precise data points theoretical analysis shows that the time complexity of blockoa is one order lower than the existing method experimental results further validate its efficiency showing that blockoa achieves a fold speedup in generating thermal ulation data for chips with varying physical parameters and ic structures even with just of the generation time datadriven approaches trained on the data generated by blockoa exhibits comparable performance to that using the existing method
while several high profile video games have served as testbeds for deep reinforcement learning drl this technique has rarely been employed by the game industry for crafting authentic ai behaviors previous research focuses on training superhuman agents with large models which is impractical for game studios with limited resources aiming for humanlike agents this paper proposes a sampleefficient drl method tailored for training and finetuning agents in industrial settings such as the video game industry our method improves sample efficiency of valuebased drl by leveraging precollected data and increasing network plasticity we evaluate our method training a goalkeeper agent in ea sports fc one of the bestselling football ulations today our agent outperforms the games builtin ai by in ball saving rate ablation studies show that our method trains agents faster compared to standard drl methods finally qualitative evaluation from domain experts indicates that our approach creates more humanlike gameplay compared to handcrafted agents as a testament to the impact of the approach the method has been adopted for use in the most recent release of the series
we introduce a novel dropin modification to monte carlo tree searchs mcts decision policy that we call aupo comparisons based on a range of ippc benchmark problems show that aupo clearly outperforms mcts aupo is an automatic action abstraction algorithm that solely relies on reward distribution statistics acquired during the mcts thus unlike other automatic abstraction algorithms aupo requires neither access to transition probabilities nor does aupo require a directed acyclic search graph to build its abstraction allowing aupo to detect symmetric actions that stateoftheart frameworks like asap struggle with when the resulting symmetric states are far apart in state space furthermore as aupo only affects the decision policy it is not mutually exclusive with other abstraction techniques that only affect the tree search
learning diverse skills without handcrafted reward functions could accelerate reinforcement learning in downstream tasks however existing skill discovery methods focus solely on maximizing the diversity of skills without considering human preferences which leads to undesirable behaviors and possibly dangerous skills for instance a cheetah robot trained using previous methods learns to roll in all directions to maximize skill diversity whereas we would prefer it to run without flipping or entering hazardous areas in this work we propose a foundation model guided fog skill discovery method which incorporates human intentions into skill discovery through foundation models specifically fog extracts a score function from foundation models to evaluate states based on human intentions assigning higher values to desirable states and lower to undesirable ones these scores are then used to reweight the rewards of skill discovery algorithms by optimizing the reweighted skill discovery rewards fog successfully learns to eliminate undesirable behaviors such as flipping or rolling and to avoid hazardous areas in both statebased and pixelbased tasks interestingly we show that fog can discover skills involving behaviors that are difficult to define interactive visualisations are available from
scientific large language models scillms have emerged as a promising frontier for accelerating biological discovery however these models face a fundamental challenge when processing raw biomolecular sequences the tokenization dilemma whether treating sequences as a specialized language risking the loss of functional motif information or as a separate modality introducing formidable alignment challenges current strategies fundamentally limit their reasoning capacity we challenge this sequencecentric paradigm by positing that a more effective strategy is to provide scillms with highlevel structured context derived from established bioinformatics tools thereby bypassing the need to interpret lowlevel noisy sequence data directly through a systematic comparison of leading scillms on biological reasoning tasks we tested three input modes sequenceonly contextonly and a combination of both our findings are striking the contextonly approach consistently and substantially outperforms all other modes even more revealing the inclusion of the raw sequence alongside its highlevel context consistently degrades performance indicating that raw sequences act as informational noise even for models with specialized tokenization schemes these results suggest that the primary strength of existing scillms lies not in their nascent ability to interpret biomolecular syntax from scratch but in their profound capacity for reasoning over structured humanreadable knowledge therefore we argue for reframing scillms not as sequence decoders but as powerful reasoning engines over expert knowledge this work lays the foundation for a new class of hybrid scientific ai agents repositioning the developmental focus from direct sequence interpretation towards highlevel knowledge synthesis the code is available at
generating highquality code remains a challenge for large language models llms for the evolution of reasoning models on this task reward models are a necessary intermediate step these models judge outcomes or intermediate steps decoderonly transformer models can be turned into reward models by introducing a regression layer and supervised finetuning while it is known that reflection capabilities generally increase with the size of a model we want to investigate whether stateoftheart small language models like the phi family can be turned into usable reward models blending the consideration of process rewards and outcome rewards targeting this goal we construct a dataset of code samples with correctness labels derived from the apps coding challenge benchmark we then train a valuehead model to estimate the success probability of intermediate outputs our evaluation shows that small llms are capable of serving as effective reward models or code evaluation critics successfully identifying correct solutions among multiple candidates using this critic we achieve over a improvement in the search capability of the most accurate code out of multiple generations
driven by the dual principles of smart education and artificial intelligence technology the online education model has rapidly emerged as an important component of the education industry cognitive diagnostic technology can utilize students learning data and feedback information in educational evaluation to accurately assess their ability level at the knowledge level however while massive amounts of information provide abundant data resources they also bring about complexity in feature extraction and scarcity of disciplinary data in crossdisciplinary fields traditional cognitive diagnostic methods still face many challenges given the differences in knowledge systems cognitive structures and data characteristics between different disciplines this paper conducts indepth research on neural network cognitive diagnosis and knowledge association neural network cognitive diagnosis and proposes an innovative crossdisciplinary cognitive diagnosis method tlcd this method combines deep learning techniques and transfer learning strategies to enhance the performance of the model in the target discipline by utilizing the common features of the main discipline the experimental results show that the crossdisciplinary cognitive diagnosis model based on deep learning performs better than the basic model in crossdisciplinary cognitive diagnosis tasks and can more accurately evaluate students learning situation
artificial intelligence is undergoing a profound transition from a computational instrument to an autonomous originator of scientific knowledge this emerging paradigm the ai scientist is architected to emulate the complete scientific workflowfrom initial hypothesis generation to the final synthesis of publishable findingsthereby promising to fundamentally reshape the pace and scale of discovery however the rapid and unstructured proliferation of these systems has created a fragmented research landscape obscuring overarching methodological principles and developmental trends this survey provides a systematic and comprehensive synthesis of this domain by introducing a unified sixstage methodological framework that deconstructs the endtoend scientific process into literature review idea generation experimental preparation experimental execution scientific writing and paper generation through this analytical lens we chart the fields evolution from early foundational modules to integrated closedloop systems and finally to the current frontier of scalability impact and humanai collaboration present by rigorously synthesizing these developments this survey not only clarifies the current state of autonomous science but also provides a critical roadmap for overcoming remaining challenges in robustness and governance ultimately guiding the next generation of systems toward becoming trustworthy and indispensable partners in human scientific inquiry
recent studies demonstrate that diffusion planners benefit from sparsestep planning over singlestep planning training models to skip steps in their trajectories helps capture longterm dependencies without additional or memory computational cost however predicting excessively sparse plans degrades performance we hypothesize this temporal density threshold is nonuniform across a temporal horizon and that certain parts of a planned trajectory should be more densely planned we propose mixed density diffuser mdd a diffusion planner where the densities throughout the horizon are tunable hyperparameters mdd achieves a new sota across the mazed franka kitchen and antmaze drl task domains
large language models llms have demonstrated promising performance in generating diagnostic conclusions from imaging findings thereby supporting radiology reporting trainee education and quality control however systematic guidance on how to optimize prompt design across different clinical contexts remains underexplored moreover a comprehensive and standardized framework for assessing the trustworthiness of llmgenerated radiology reports is yet to be established this study aims to enhance the trustworthiness of llmgenerated liver mri reports by introducing a multidimensional credibility assessment mdca framework and providing guidance on institutionspecific prompt optimization the proposed framework is applied to evaluate and compare the performance of several advanced llms including kimikinstruct qwenbabinstruct deepseekv and bytedanceseedossbinstruct using the siliconflow platform
profilexai is a model and domainagnostic framework that couples posthoc explainers shap lime anchor with retrieval augmented llms to produce explanations for different types of users the system indexes a multimodal knowledge base selects an explainer per instance via quantitative criteria and generates grounded narratives with chatenabled prompting on heart disease and thyroid cancer datasets we evaluate fidelity robustness parsimony token use and perceived quality no explainer dominates lime achieves the best fidelityrobustness tradeoff infidelity le l on heart disease anchor yields the sparsest lowtoken rules shap attains the highest satisfaction barx profile conditioning stabilizes tokens sigma le and maintains positive ratings across profiles barxge with domain experts at enabling efficient and trustworthy explanations
recently semantically constrained adversarial examples semanticae which are directly generated from natural language instructions have become a promising avenue for future research due to their flexible attacking forms to generate semanticaes current methods fall short of satisfactory attacking ability as the key underlying factors of semantic uncertainty in human instructions such as referring diversity descriptive incompleteness and boundary ambiguity have not been fully investigated to tackle the issues this paper develops a multidimensional instruction uncertainty reduction insur framework to generate more satisfactory semanticae ie transferable adaptive and effective specifically in the dimension of the sampling method we propose the residualdriven attacking direction stabilization to alleviate the unstable adversarial optimization caused by the diversity of language references by coarsely predicting the languageguided sampling process the optimization process will be stabilized by the designed resadvddim sampler therefore releasing the transferable and robust adversarial capability of multistep diffusion models in task modeling we propose the contextencoded attacking scenario constraint to supplement the missing knowledge from incomplete human instructions guidance masking and renderer integration are proposed to regulate the constraints of dd semanticae activating stronger scenarioadapted attacks moreover in the dimension of generator evaluation we propose the semanticabstracted attacking evaluation enhancement by clarifying the evaluation boundary facilitating the development of more effective semanticae generators extensive experiments demonstrate the superiority of the transfer attack performance of insur moreover we realize the referencefree generation of semantically constrained d adversarial examples for the first time
in wireless communication systems efficient and adaptive resource allocation plays a crucial role in enhancing overall quality of service qos while centralized multiagent reinforcement learning marl frameworks rely on a central coordinator for policy training and resource scheduling they suffer from scalability issues and privacy risks in contrast the distributed training with decentralized execution dtde paradigm enables distributed learning and decisionmaking but it struggles with nonstationarity and limited interagent cooperation which can severely degrade system performance to overcome these challenges we propose the multiagent conditional diffusion model planner macdmp for decentralized communication resource management built upon the modelbased reinforcement learning mbrl paradigm macdmp employs diffusion models dms to capture environment dynamics and plan future trajectories while an inverse dynamics model guides action generation thereby alleviating the sample inefficiency and slow convergence of conventional dtde methods moreover to imate largescale agent interactions a meanfield mf mechanism is introduced as an assistance to the classifier in dms this design mitigates interagent nonstationarity and enhances cooperation with minimal communication overhead in distributed settings we further theoretically establish an upper bound on the distributional imation error introduced by the mfbased diffusion generation guaranteeing convergence stability and reliable modeling of multiagent stochastic dynamics extensive experiments demonstrate that macdmp consistently outperforms existing marl baselines in terms of average reward and qos metrics showcasing its scalability and practicality for realworld wireless network optimization
next pointofinterest poi recommendation is a critical task in modern locationbased social networks lbsns aiming to model the complex decisionmaking process of human mobility to provide personalized recommendations for a users next checkin location existing poi recommendation models predominantly based on graph neural networks and sequential models have been extensively studied however these models face a fundamental limitation they struggle to ultaneously capture the inherent hierarchical structure of spatial choices and the dynamics and irregular shifts of userspecific temporal contexts to overcome this limitation we propose gtrmamba a novel framework for crossmanifold conditioning and routing gtrmamba leverages the distinct advantages of different mathematical spaces for different tasks it models the static treelike preference hierarchies in hyperbolic geometry while routing the dynamic sequence updates to a novel mamba layer in the computationally stable and efficient euclidean tangent space this process is coordinated by a crossmanifold channel that fuses spatiotemporal information to explicitly steer the state space model ssm enabling flexible adaptation to contextual changes extensive experiments on three realworld datasets demonstrate that gtrmamba consistently outperforms stateoftheart baseline models in next poi recommendation
generalization across agentic toolcalling environments remains a key unsolved challenge in developing reliable agentic reasoning systems while large language models llms demonstrate strong performance on isolated benchmarks their ability to transfer reasoning strategies and coordinate tools across diverse domains is poorly understood in this work we conduct a largescale evaluation of stateoftheart llms on multiple toolcalling benchmarksbfcl v taubench taubench and acebenchand introduce maven math physics adversarial verification evaluation network a new out of distribution ood benchmark designed to stresstest multistep reasoning through explicit verification and adversarial task composition our results show that most current models achieve below accuracy on maven revealing a significant generalization gap across tooluse settings to address this we present the corethink agentic reasoner a framework that augments llms with a lightweight symbolic reasoning layer for structured decomposition and adaptive tool orchestration without additional training it generalizes across all benchmarks achieving stateoftheart performance with improvements over existing baselines at roughly onetenth the computational cost
cognitive studies and artificial intelligence have developed distinct models for various inferential mechanisms categorization induction abduction causal inference contrast merge yet both natural and artificial views on cognition lack apparently a unifying framework this paper formulates a speculative answer attempting to respond to this gap to postulate on higherlevel activation processes from a material perspective we consider inferential mechanisms informed by symbolic ai modelling techniques through the plistic lenses of electronic circuits based
