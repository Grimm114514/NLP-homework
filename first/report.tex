% \documentclass[11pt, a4paper]{article} % 不再使用 article
\documentclass[11pt, a4paper]{ctexart} % <-- 改用 ctexart 文档类

% --- 1. 通用设置与页面边距 ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}

% --- 2. 字体与中文设置 ---
% ctexart 已经自动处理好了中文字体 (通常是宋体/黑体)，
% 我们不需要再加载 fontspec 或 babel 了。

% --- 3. 常用工具包 ---
\usepackage{amsmath}      % AMS 数学公式包
\usepackage{booktabs}     % 优化表格（三线表）
\usepackage{graphicx}     % 插入图片
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=cyan]{hyperref} % 超链接包
\usepackage{fancyhdr} % 页眉页脚宏包

% --- 4. 封面信息 ---
% ctexart 会自动处理中文标题
\title{自然语言处理第一次作业（A）}
\author{作者：张宗桪 \\ 学号：2023K8009991013}
\date{\today} % 使用当前日期

% --- 5. 文档开始 ---
\begin{document}

% --- 封面 ---
\maketitle
\thispagestyle{empty} % 封面不显示页码
\newpage % 封面后强制换页

% --- 目录 ---
\pagestyle{plain} % 目录页使用简单页眉
\tableofcontents
\newpage % 目录后强制换页

% --- 正文 ---
\pagestyle{fancy} % 正文使用自定义页眉页脚
\fancyhf{} % 清空默认设置
\fancyfoot[C]{\thepage} % 页码居中底部
\section{样本的爬取过程}

本次实验利用 Python 语言的 Scrapy 框架进行网络数据的爬取。Scrapy 是一个为了爬取网站数据、提取结构性数据而编写的高效、异步的应用框架，它允许开发者通过定义“爬虫”（Spiders）来实现复杂的抓取逻辑。

本实验的核心爬取逻辑由两个 Spider 类（'arxiv\_spider.py' 和 'sina\_tech.py'）定义，它们分别针对 $arXiv$ 和新浪新闻这两个数据源。

\subsection{$arXiv$ 论文数据爬取 (arxiv\_spider.py)}

$arXiv$ 数据源的爬取目标是获取计算机科学（CS）领域（包括人工智能 `cs.AI`、计算语言学 `cs.CL` 和计算机视觉 `cs.CV`）的最新论文文本：

\begin{enumerate}
    \item \textbf{起始点 (\texttt{parse} 方法):} 爬虫从 \texttt{start\_urls} 中定义的 \texttt{/recent} 列表页开始。在此页面，它使用 CSS 选择器 (\texttt{response.css}) 查找所有指向论文摘要页（\texttt{/abs/...}）的链接，并使用 \texttt{response.follow} 生成新的请求，将响应(response)交由 \texttt{parse\_abstract} 方法处理。

    \item \textbf{摘要爬取 (\texttt{parse\_abstract} 方法)} 
    \begin{itemize}
        \item 爬虫执行如下方案：仅提取当前页面的论文摘要。摘要文本位于 \texttt{<blockquote>} 标签中。
    \end{itemize}

    \item \textbf{内容提取 (\texttt{parse\_html\_paper} / \texttt{parse\_abstract}):} 在 \texttt{parse\_abstract} 方法中提取摘要内容。
\end{enumerate}

\subsection{新浪新闻数据爬取 (sina\_tech.py)}

\texttt{sina\_tech.py} 爬虫的逻辑相对直接。它从首页开始，在 \texttt{parse} 方法中使用 XPath (\texttt{response.xpath}) 筛选出所有符合新浪文章 URL 特征（包含 \texttt{/doc-} 和 \texttt{.shtml}）的链接。随后，它跟进这些链接，并在 \texttt{parse\_article} 方法中，使用 XPath 精确提取文章的标题、发布时间、来源和正文等结构化数据。

\subsection{核心爬取配置 (settings.py)}

为了确保爬取过程的高效、稳定和友好，在 \texttt{settings.py} 文件中进行如下参数设置：

\begin{enumerate}
    \item \textbf{友好性策略：} 为了避免对目标服务器造成过大压力，组合使用 \texttt{ROBOTSTXT\_OBEY = True}（严格遵守网站的 \texttt{robots.txt} 爬取协议）和 \texttt{DOWNLOAD\_DELAY = 1}（设置 1 秒的下载延迟）。
    \item \textbf{身份伪装：} 将 \texttt{USER\_AGENT} 设置为通用的 Chrome 浏览器标识，以模拟正常的用户访问，这是爬虫反屏蔽的常见策略。
    \item \textbf{数据编码：} 鉴于新浪新闻包含大量中文文本，明确设置 \texttt{FEED\_EXPORT\_ENCODING = "utf-8"}。
\end{enumerate}
\section{样本的清洗}
数据清洗是本实验至关重要的一步，其目的是从爬取原始文本中提取出干净、可用的纯文本内容。我们主要使用了 Python 的 BeautifulSoup 库和正则表达式 (re) 库。

\subsection{中文处理}
对于新浪新闻的中文文本，我们进行了以下清洗步骤：
\begin{enumerate}
    \item \textbf{移除特定噪音：} 移除特定的广告（如 炒股就看...）、系统残留（如 Flash Player...）和免责声明等。
    \item \textbf{移除 URL 和邮件：} 移除多种格式的作者、来源、责编等信息（如 \texttt{(来源：...)} 或 \texttt{(责编|记者...)}）。
    \item \textbf{去除无效字符：} 去除多余的空白符、换行符和特殊控制字符。
    \item \textbf{统一编码：} 所有文本统一保存为 UTF-8 编码。
\end{enumerate}

\subsection{英文处理}
对于 $arXiv$ 论文的英文文本，清洗步骤包括：
\begin{enumerate}
    \item \textbf{移除元数据：} 移除特定的 \texttt{\textbackslash s*} 标签。
    \item \textbf{文本标准化：} 将所有文本转换为小写，使用 \texttt{cleaned\_line.lower()}。
    \item \textbf{移除格式化残留：} 移除如 \texttt{textbf} 或 \texttt{textit} 这样的格式前缀。
    \item \textbf{严格字符过滤：} 移除所有非字母和非空白字符（即数字和标点）。
    \item \textbf{规范化空白：} 将多个连续空白压缩为单个空格。
    \item \textbf{去重与写入：} 使用 \texttt{set} 集合 (\texttt{seen\_lines}) 检查，跳过已存在的行。
\end{enumerate}

最终，我们得到了三个不同规模的中文和英文语料库：
\begin{itemize}
    \item \textbf{小规模样本：} 中文约 150KB 文本，英文约700KB。
    \item \textbf{中规模样本：} 中文约 250KB 文本，英文约900KB。
    \item \textbf{大规模样本：} 中文约 350KB 文本，英文约1500KB。
\end{itemize}

\section{英语字母和单词或汉字的概率和熵}
在此部分，我们分别对中文和英文样本进行信息熵的计算。信息熵的计算公式为：
$$ H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i) $$
其中 $P(x_i)$ 是一个符号（字母、单词或汉字）出现的概率。

\subsection{中文}
对于中文，我们以“汉字”为单位进行统计。

\subsubsection{小规模样本}
（此处分析小规模中文样本的汉字频率和信息熵...）

\subsubsection{中规模样本}
（此处分析中规模中文样本的汉字频率和信息熵...）

\subsubsection{大规模样本}
（此处分析大规模中文样本的汉字频率和信息熵...）

\subsection{英文}
对于英文，我们分别以“字母”（Letter）和“单词”（Word）为单位进行统计。

\subsubsection{小规模样本}
\paragraph{字母熵：} （分析...）
\paragraph{单词熵：} （分析...）

\subsubsection{中规模样本}
\paragraph{字母熵：} （分析...）
\paragraph{单词熵：} （分析...）

\subsubsection{大规模样本}
\paragraph{字母熵：} （分析...）
\paragraph{单词熵：} （分析...）

\section{验证齐夫定律 (Zipf's Law)}
齐夫定律指出，在一个大型语料库中，任意单词的出现频率 $f$ 与其在频率表中的排名 $r$ 成反比，即：
$$ f \propto \frac{1}{r} $$
取对数后，$\log(f)$ 和 $\log(r)$ 应呈线性关系。我们仅对英文样本进行单词级别的验证。

\subsection{小规模样本}
（分析...）

\subsection{中规模样本}
（分析...）

\subsection{大规模样本}
（分析...）
我们在大规模样本上的对数-对数坐标图（见图 \ref{fig:zipf_plot}）清晰地展示了这一线性关系，从而验证了齐夫定律。

% 这是一个图片占位符示例
\begin{figure}[htbp]
  \centering
  % 因为我们不能加载外部图片，所以使用一个方框作为占位符
  \framebox{\parbox{0.8\textwidth}{\centering
    \vspace{5cm}
    \textbf{图 1：大规模英文样本的 Zipf 定律拟合图} \\
    \small (\texttt{log(rank)} vs \texttt{log(frequency)})
    \vspace{5cm}
  }}
  \caption{Zipf 定律对数-对数图}
  \label{fig:zipf_plot}
\end{figure}

% --- 5. 文档结束 ---
\end{document}

