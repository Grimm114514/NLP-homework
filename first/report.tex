\documentclass[11pt, a4paper]{ctexart} % <-- 改用 ctexart 文档类

% --- 1. 通用设置与页面边距 ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}

% --- 2. 字体与中文设置 ---
% ctexart 已经自动处理好了中文字体 (通常是宋体/黑体)，
% 我们不需要再加载 fontspec 或 babel 了。

% --- 3. 常用工具包 ---
\usepackage{amsmath}      % AMS 数学公式包
\usepackage{booktabs}     % 优化表格（三线表）
\usepackage{graphicx}     % 插入图片
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=cyan]{hyperref} % 超链接包
\usepackage{fancyhdr} % 页眉页脚宏包
\usepackage{float} % 在导言区加载宏包

% --- 4. 封面信息 ---
% ctexart 会自动处理中文标题
\title{自然语言处理第一次作业（A）}
\author{作者：张宗桪 \\ 学号：2023K8009991013}
\date{\today} % 使用当前日期

% --- 5. 文档开始 ---
\begin{document}

% --- 封面 ---
\maketitle
\thispagestyle{empty} % 封面不显示页码
\newpage % 封面后强制换页

% --- 目录 ---
\pagestyle{plain} % 目录页使用简单页眉
\tableofcontents
\newpage % 目录后强制换页

% --- 正文 ---
\pagestyle{fancy} % 正文使用自定义页眉页脚
\fancyhf{} % 清空默认设置
\fancyfoot[C]{\thepage} % 页码居中底部
\setlength{\parskip}{1em} % 设置段落间距
\setlength{\parindent}{2em} % 设置段落缩进
\setlength{\baselineskip}{1.5em} % 设置行间距
\renewcommand{\baselinestretch}{1.5} % 设置行距为1.5倍行距
\section{样本的爬取过程}
本次实验使用 Python 的 Scrapy 框架构建两个定制爬虫，分别采集英文科研论文摘要与中文科技新闻文本语料。本实验的主要采集逻辑由两个 Spider 类（\texttt{arxiv\_spider.py} 与 \texttt{sina\_tech.py}）实现，对应两个异质数据源：学术论文摘要与新闻报道。

\subsection{\texorpdfstring{$arXiv$}{arXiv} 论文数据爬取 (arxiv\_spider.py)}

采集目标：获取计算机科学领域近期论文摘要，限定在人工智能 (\texttt{cs.AI})、计算语言学 (\texttt{cs.CL}) 与计算机视觉 (\texttt{cs.CV}) 等分类下的最新条目。整体流程概述如下：

\begin{enumerate}
       \item 入口解析 (\texttt{parse}): 从 \texttt{start\_urls} 指向的 \texttt{/recent} 页面启动，利用 CSS 选择器提取指向具体论文摘要页（\texttt{/abs/...}）的链接，并通过 \texttt{response.follow} 派发至摘要解析回调。
       \item 摘要抽取 (\texttt{parse\_abstract}): 在摘要页中定位 \texttt{<blockquote>} 标签并提取其纯文本内容，只抓取摘要。
       \item 结构规整: 在抽取后进行基础清洗，以便后续英文文本规范化与统计处理。
\end{enumerate}

\subsection{新浪新闻数据爬取 (sina\_tech.py)}

该 Spider 针对新浪科技频道最新页面的资讯类内容：
\begin{enumerate}
       \item 链接发现: 在首页使用 XPath 表达式筛选包含特征片段（\texttt{/doc-} 与 \texttt{.shtml}）的文章 URL，规避导航与多媒体类非正文页面。
       \item 正文解析 (\texttt{parse\_article}): 对每篇文章提取标题、发布时间、来源与主体段落等字段，确保生成结构化条目；解析时通过 XPath 精准定位，减少对页面样式变动的敏感性。
\end{enumerate}

\subsection{核心爬取配置 (settings.py)}

在 \texttt{settings.py} 中配置关键参数：
\begin{enumerate}
       \item 访问友好策略: \texttt{ROBOTSTXT\_OBEY = True} 强制遵守目标站点的爬取协议； \texttt{DOWNLOAD\_DELAY = 1} 设置 1 秒请求延迟，减少被限速或封禁风险。
       \item User-Agent 伪装: 设置模拟常见 Chrome 浏览器标识的 \texttt{USER\_AGENT}，提升页面响应一致性并减少触发反爬策略的概率。
       \item 编码统一: 明确 \texttt{FEED\_EXPORT\_ENCODING = "utf-8"}，保证中文文本在导出阶段不出现乱码，方便后续跨平台处理与统计。
\end{enumerate}

\section{样本的清洗}

本实验针对中英文语料的语言特性差异与噪音模式，分别设计差异化清洗策略，主要依托 Python 的 BeautifulSoup 库（HTML 结构解析）与正则表达式库（模式匹配与文本规整）实现。

清洗流程为：首先移除明显的非文本元素与格式标记，其次针对语言特性进行精细化过滤，最后统一编码与空白符。

\subsection{中文文本处理流程}
针对新浪科技新闻的中文语料，采取如下精细化清洗策略：
\begin{enumerate}
    \item 结构化噪音过滤： 使用正则表达式移除网页特有的冗余信息，包括：
    \begin{itemize}
        \item 商业广告植入（如"炒股就看金融界"等营销文案）
        \item 技术残留标识（如"Flash Player"、"JavaScript"等系统提示）
        \item 法律免责条款与版权声明
    \end{itemize}
    \item 元信息剔除： 移除新闻生产流程中的编辑元数据，具体包括：
    \begin{itemize}
        \item 来源标注：\texttt{(来源：[机构名称])}
        \item 责编信息：\texttt{(责编|记者：[姓名])}
        \item 时间戳与 URL 链接
    \end{itemize}
    \item 字符规范化： 清理格式控制符与异常字符：
    \begin{itemize}
        \item 移除零宽字符、制表符及非打印控制字符
        \item 压缩连续空白为单一空格
    \end{itemize}
\end{enumerate}

\subsection{英文文本处理流程}
$arXiv$ 论文摘要的英文文本清洗更注重学术写作规范化与统计一致性：
\begin{enumerate}
    \item LaTeX 残留清除： 移除论文排版过程中的格式化标记：
    \begin{itemize}
        \item 移除反斜杠转义序列（\texttt{\textbackslash s*} 等）
        \item 清理 \texttt{\textbackslash textbf}、\texttt{\textbackslash textit} 等字体控制命令
    \end{itemize}
    \item 大小写标准化： 将全文转换为小写，消除因首字母大写、专有名词等导致的词频统计偏差。
    \item 字符集约束： 
    \begin{itemize}
        \item 仅保留英文字母（a-z）与空白符
        \item 移除数字、标点符号与特殊符号
    \end{itemize}
    \item 空白符规整： 将连续空白字符（空格、制表符）压缩为单一空格，避免词间距不一致影响切分。
    \item 去重优化： 采用集合数据结构（\texttt{set}）记录已处理行，避免重复内容对词频统计造成偏差。
\end{enumerate}

\subsection{语料库构建结果}
经过上述差异化预处理，最终构建了六个分层语料库，形成中英双语、三级规模的矩阵化数据集：
\begin{itemize}
    \item 小规模语料： 中文 $\approx$ 150KB，英文 $\approx$ 700KB
    \item 中等规模语料： 中文 $\approx$ 250KB，英文 $\approx$ 900KB
    \item 大规模语料： 中文 $\approx$ 350KB，英文 $\approx$ 1.5MB
\end{itemize}

\section{符号概率分布与信息熵分析}

信息熵的数学定义为：
$$ H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i) $$
其中 $P(x_i)$ 表示符号 $x_i$（汉字或英文单词）在语料中的出现概率，$n$ 为不重复符号的总数。熵值以比特（bit）为单位，数值越高表示符号分布越均匀，文本的不确定性与信息密度越大。

本实验对中英文采用不同的统计单元：中文以"汉字"为原子符号进行字级统计，英文以"单词"为基本单元进行词级统计。
\subsection{中文汉字熵分析}

\subsubsection{结果}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_100cn_cleaned}
    \caption{小规模样本的汉字频率和信息熵}
    \label{fig:small_scale_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_150cn_cleaned}
    \caption{中规模样本的汉字频率和信息熵}
    \label{fig:medium_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_200cn_cleaned}
    \caption{大规模样本的汉字频率和信息熵}
    \label{fig:large_scale}
\end{figure}

\subsubsection{横向对比}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_comparison_chart}
    \caption{横向对比}
    \label{fig:comparison}
\end{figure}
\paragraph{分析}

根据图表数据分析，\texttt{100cn\_cleaned.txt}、\texttt{150cn\_cleaned.txt} 和 \texttt{200cn\_cleaned.txt} 三个文本文件的汉字信息熵变化呈现先升后降（9.5298 $\to$ 9.5574 $\to$ 9.5265）的非线性趋势。

下面我们分阶段进行详细分析：

\paragraph{第一阶段：从 100cn 到 150cn}
\begin{enumerate}
    \item 数据表现：在此阶段，总汉字数（55,052 $\to$ 81,843）、独立汉字数（2,105 $\to$ 2,318）和信息熵（9.5298 $\to$ 9.5574）均呈现增长。
    
    \item 分布变化：这是一个符合预期的增长。文本规模扩大，带来了 213 个新的独立汉字，引入了更多样化的词汇和表达。关键证据是，最常用字“的”的概率反而轻微下降了（从 2.7301\% $\to$ 2.6710\%）。
    
    \item 结论：当一个文本库在扩展时，如果新加入的内容主题多样，会引入新的词汇，同时稀释最常见词汇的占比。这种多样化使得汉字分布更趋于均匀，系统的不确定性随之增加，因此信息熵上升。
\end{enumerate}
                                               
\paragraph{第二阶段：从 150cn 到 200cn}
\begin{enumerate}
    \item 数据表现：在此阶段，总汉字数（81,843 $\to$ 108,021）和独立汉字数（2,318 $\to$ 2,478）依然在增加，但信息熵却反常下降了（从 9.5574 $\to$ 9.5265）。
    
    \item 分布变化：熵下降的直接原因是汉字概率分布变得更不均匀。最显著的证据是：
    \begin{itemize}
    \item 排名第一的汉字“的”，其出现概率从 2.6710\% 显著飙升至 3.1309\%。
        \item 排名第二的“在”，概率也从 0.8431\% 上升到 0.9693\%。
    \end{itemize}
    
    \item 理论解释：根据信息熵的定义 $H = -\sum p(x) \log p(x)$，当某个单一事件（如“的”字）的概率 $p$ 显著提高时，它对总熵的贡献会减小，导致整体系统的不确定性（熵）降低。换言之，当“的”字出现得如此频繁，系统就变得“更可预测”了。
    
    \item 主题偏移：词频表（Top 20）的变化揭示了熵下降的根本原因——“主题偏移”。
    \begin{itemize}
        \item \texttt{150cn} 的高频词包括“行”（第9）、“生”（第13）、“学”（第20）。
        \item \texttt{200cn} 中，这些词跌出前20，取而代之的是一组主题高度集中的新词：“资”、“理”、“场”、“产”。
    \end{itemize}
    
    \item 结论：这表明从 150cn 到 200cn 新增的文本内容，大概率是高度集中在“商业”、“经济”或“管理”领域。这种高度专业化、主题趋同的文本，不仅引入了自己领域的高频词，也导致了“的”、“在”等通用字的用法频率激增。这种现象使得整个文本的汉字分布变得更加倾斜和不均，最终导致了信息熵的下降，尽管文本的总量和词汇量仍在增加。
\end{enumerate}


\subsection{英文}
对于英文，我们以“单词”（Word）为单位进行统计。

\subsubsection{结果}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_100000en}
    \caption{小规模样本的单词频率和信息熵}
    \label{fig:small_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_150000en}
    \caption{中规模样本的单词频率和信息熵}
    \label{fig:medium_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_200000en}
    \caption{大规模样本的单词频率和信息熵}
    \label{fig:large_scale}
\end{figure}

\subsubsection{横向对比}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=18cm]{word_comparison_chart}
    \caption{横向对比}
    \label{fig:comparison}
\end{figure}
\paragraph{分析}
基于三组英文样本（100K/150K/200K）的统计结果，可以得到以下观察与结论：
\begin{enumerate}
    \item 信息熵： 单词信息熵 10.1900 $\to$ 10.2136 $\to$ 10.3102，稳步上升，说明词频分布整体更加均匀，即不确定性增加。
    \item 高频词结构： Top-20 长期由功能词 and, the, a, to, of, in 等主导。随着规模扩大，model, llm, reasoning, method, data, task 等领域词逐步进入Top-20，显示样本主题对 AI、LLM 等研究语域的偏向。
    \item 头尾部变化： 头部功能词的相对占比基本稳定或略有下降，而尾部长尾扩展更明显——这是熵上升的主要来源。
\end{enumerate}

综上，随着语料规模的扩大，英文样本呈现出“熵上升、主题更加凸显”的特征；这符合语言统计的一般规律，也反映了采样来源对语域的影响。

\section{验证齐夫定律 (Zipf's Law)}
齐夫定律指出，在一个大型语料库中，任意单词的出现频率 $f$ 与其在频率表中的排名 $r$ 成反比，即：
$$ f \propto \frac{1}{r} $$
取对数后，$\log(f)$ 和 $\log(r)$ 应呈线性关系。我们仅对英文样本进行单词级别的验证。

\paragraph{验证方法}
使用 Python 实现验证逻辑：首先统计单词频率并按频率降序排列。然后计算词频与排名的对数值，通过 \texttt{numpy.polyfit} 进行线性拟合，获得斜率参数。

\subsection{小规模样本}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{zipf_verification_100000en.png}
    \caption{小规模样本的 Zipf 定律对数拟合图}
    \label{fig:zipf_small_scale_new}
\end{figure}

\subsection{中规模样本}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{zipf_verification_150000en.png}
    \caption{中规模样本的 Zipf 定律对数拟合图}
    \label{fig:zipf_medium_scale_new}
\end{figure}

\subsection{大规模样本}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{zipf_verification_200000en.png}
    \caption{大规模样本的 Zipf 定律对数拟合图}
    \label{fig:zipf_large_scale_new}
\end{figure}

\paragraph{分析}
从对数坐标的拟合结果看，三个规模的语料库均表现出词频与排名近似线性相关的特性，这与齐夫定律的预测一致。具体而言，100K、150K 和 200K 词汇语料的回归斜率分别为 -1.171 ($R^2=0.9677$)、-1.211 ($R^2=0.9682$) 和 -1.269 ($R^2=0.9686$)。

实验结论与偏差分析：
\begin{enumerate}
    \item 基本符合理论： 所有样本的拟合直线斜率都接近理论值 -1，同时 $R^2$ 均接近 1，表明词频与排名的对数关系具有较高的线性相关性。
    \item 高频词区域： 在所有图中，曲线的头部（高频词区域）都明显高于拟合的直线。这主要是因为大量的冠词、介词等功能词（如 "the", "of", "a"）占据了频率的顶端，其出现频率远超普通内容词，导致了分布的陡峭化。
    \item 低频词区域： 曲线的尾部出现了明显的向下弯曲和“台阶”状结构。这种现象源于以下几个原因：
    \begin{itemize}
    \item 有限样本： 在有限的语料中，许多稀有词只出现了一次或极少数次，导致在对数坐标尾部形成离散的、水平的“台阶”。
    \item 数据离散化： 排名越靠后，词频越低，频率值的变化不再是连续的，这种离散效应在对数图上被放大。
    \end{itemize}
\end{enumerate}

% --- 5. 文档结束 ---
\end{document}

