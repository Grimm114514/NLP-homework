\documentclass[11pt, a4paper]{ctexart} % <-- 改用 ctexart 文档类

% --- 1. 通用设置与页面边距 ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}

% --- 2. 字体与中文设置 ---
% ctexart 已经自动处理好了中文字体 (通常是宋体/黑体)，
% 我们不需要再加载 fontspec 或 babel 了。

% --- 3. 常用工具包 ---
\usepackage{amsmath}      % AMS 数学公式包
\usepackage{booktabs}     % 优化表格（三线表）
\usepackage{graphicx}     % 插入图片
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=cyan]{hyperref} % 超链接包
\usepackage{fancyhdr} % 页眉页脚宏包
\usepackage{float} % 在导言区加载宏包

% --- 4. 封面信息 ---
% ctexart 会自动处理中文标题
\title{自然语言处理第一次作业（A）}
\author{作者：张宗桪 \\ 学号：2023K8009991013}
\date{\today} % 使用当前日期

% --- 5. 文档开始 ---
\begin{document}

% --- 封面 ---
\maketitle
\thispagestyle{empty} % 封面不显示页码
\newpage % 封面后强制换页

% --- 目录 ---
\pagestyle{plain} % 目录页使用简单页眉
\tableofcontents
\newpage % 目录后强制换页

% --- 正文 ---
\pagestyle{fancy} % 正文使用自定义页眉页脚
\fancyhf{} % 清空默认设置
\fancyfoot[C]{\thepage} % 页码居中底部
\section{样本的爬取过程}
本次实验使用 Python 的 Scrapy 框架构建两个定制爬虫，分别采集英文科研论文摘要（$arXiv$）与中文科技新闻（新浪科技频道）文本语料本实验的主要采集逻辑由两个 Spider 类（`arxiv_spider.py` 与 `sina_tech.py`）实现，对应两个异质数据源：学术论文摘要与新闻报道。二者在页面结构复杂度、语料语言特征及更新节奏方面具有显著差异，因而在解析策略与节流配置上采取了区分化设计。
\subsection{$arXiv$ 论文数据爬取 (arxiv\_spider.py)}

采集目标：获取计算机科学领域近期论文摘要，限定在人工智能 (`cs.AI`)、计算语言学 (`cs.CL`) 与计算机视觉 (`cs.CV`) 等分类下的最新条目。整体流程概述如下：

\begin{enumerate}
    \item \textbf{入口解析 (\texttt{parse}):} 从 \texttt{start\_urls} 指向的 \texttt{/recent} 页面启动，利用 CSS 选择器提取指向具体论文摘要页（\texttt{/abs/...}）的链接，并通过 \texttt{response.follow} 派发至摘要解析回调。
    \item \textbf{摘要抽取 (\texttt{parse\_abstract}):} 在摘要页中定位 \texttt{<blockquote>} 标签并提取其纯文本内容，只抓取摘要。
    \item \textbf{结构规整:} 在抽取后进行基础清洗，以便后续英文文本规范化与统计处理。
\end{enumerate}

\subsection{新浪新闻数据爬取 (sina\_tech.py)}

该 Spider 针对新浪科技频道最新页面的资讯类内容。与论文摘要的层次化链接结构不同，新闻页在首页已暴露大量正文链接，因此策略更强调批量筛选与字段精准抽取：
\begin{enumerate}
    \item \textbf{链接发现:} 在首页使用 XPath 表达式筛选包含特征片段（\texttt{/doc-} 与 \texttt{.shtml}）的文章 URL，规避导航与多媒体类非正文页面。
    \item \textbf{正文解析 (\texttt{parse\_article}):} 对每篇文章提取标题、发布时间、来源与主体段落等字段，确保生成结构化条目；解析时通过 XPath 精准定位，减少对页面样式变动的敏感性。
    \item \textbf{异常与冗余控制:} 对为空标题或缺失正文的响应进行跳过；避免重复抓取同一链接（依赖 Scrapy 默认去重机制）。
\end{enumerate}

\subsection{核心爬取配置 (settings.py)}

为提升采集的遵从性、稳定性与文本可用性，在 `settings.py` 中配置关键参数：
\begin{enumerate}
    \item \textbf{访问友好策略:} \texttt{ROBOTSTXT\_OBEY = True} 强制遵守目标站点的爬取协议； \texttt{DOWNLOAD\_DELAY = 1} 设置 1 秒请求延迟，降低瞬时并发压力，减少被限速或封禁风险。
    \item \textbf{User-Agent 伪装:} 设置模拟常见 Chrome 浏览器标识的 \texttt{USER\_AGENT}，提升页面响应一致性并减少触发反爬策略的概率。
    \item \textbf{编码统一:} 明确 \texttt{FEED\_EXPORT\_ENCODING = "utf-8"}，保证中文文本在导出阶段不出现乱码，方便后续跨平台处理与统计。
\end{enumerate}

上述双源并行抓取策略在结构复杂度与内容粒度上实现互补：英文侧强调学术摘要的领域聚焦与可控泛化性；中文侧强调新闻更新的时效性与语域多样性。二者结合为后续的信息熵与齐夫分布验证提供跨语言、跨体裁的对比基底。
\section{样本的清洗}
数据清洗是本实验至关重要的一步，其目的是从爬取原始文本中提取出干净、可用的纯文本内容。我们主要使用了 Python 的 BeautifulSoup 库和正则表达式 (re) 库。

\subsection{中文处理}
对于新浪新闻的中文文本，我们进行了以下清洗步骤：
\begin{enumerate}
    \item \textbf{移除特定噪音：} 移除特定的广告（如 炒股就看...）、系统残留（如 Flash Player...）和免责声明等。
    \item \textbf{移除 URL 和邮件：} 移除多种格式的作者、来源、责编等信息（如 \texttt{(来源：...)} 或 \texttt{(责编|记者...)}）。
    \item \textbf{去除无效字符：} 去除多余的空白符、换行符和特殊控制字符。
    \item \textbf{统一编码：} 所有文本统一保存为 UTF-8 编码。
\end{enumerate}

\subsection{英文处理}
对于 $arXiv$ 论文的英文文本，清洗步骤包括：
\begin{enumerate}
    \item \textbf{移除元数据：} 移除特定的 \texttt{\textbackslash s*} 标签。
    \item \textbf{文本标准化：} 使用 \texttt{cleaned\_line.lower()}将所有文本转换为小写。
    \item \textbf{移除格式化残留：} 移除如 \texttt{textbf} 或 \texttt{textit} 这样的格式前缀。
    \item \textbf{严格字符过滤：} 移除所有非字母和非空白字符（即数字和标点）。
    \item \textbf{规范化空白：} 将多个连续空白压缩为单个空格。
    \item \textbf{去重与写入：} 使用 \texttt{set} 集合 (\texttt{seen\_lines}) 检查，跳过已存在的行。
\end{enumerate}

最终，我们得到了三个不同规模的中文和英文语料库：
\begin{itemize}
    \item \textbf{小规模样本：} 中文约 150KB 文本，英文约700KB。
    \item \textbf{中规模样本：} 中文约 250KB 文本，英文约900KB。
    \item \textbf{大规模样本：} 中文约 350KB 文本，英文约1500KB。
\end{itemize}

\section{英语单词或汉字的概率和熵}
在此部分，我们分别对中文和英文样本进行信息熵的计算。信息熵的计算公式为：
$$ H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i) $$
其中 $P(x_i)$ 是一个符号（单词或汉字）出现的概率。

\subsection{中文}
对于中文，我们以“汉字”为单位进行统计。

\subsubsection{结果}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_100cn_cleaned}
    \caption{小规模样本的汉字频率和信息熵}
    \label{fig:small_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_150cn_cleaned}
    \caption{中规模样本的汉字频率和信息熵}
    \label{fig:medium_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_200cn_cleaned}
    \caption{大规模样本的汉字频率和信息熵}
    \label{fig:large_scale}
\end{figure}

\subsubsection{横向对比}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_comparison_chart}
    \caption{横向对比}
    \label{fig:comparison}
\end{figure}
\paragraph{分析}

根据图表数据分析，\texttt{100cn\_cleaned.txt}、\texttt{150cn\_cleaned.txt} 和 \texttt{200cn\_cleaned.txt} 三个文本文件的汉字信息熵变化呈现先升后降（9.5298 $\to$ 9.5574 $\to$ 9.5265）的非线性趋势。

这一现象清晰地表明，影响信息熵的核心因素是文本的“主题内容”及其决定的“汉字概率分布”，而非文本规模本身。下面我们分阶段进行详细论证：

\paragraph{第一阶段：从 100cn 到 150cn（熵的合理增长）}
\begin{enumerate}
    \item \textbf{数据表现}：在此阶段，总汉字数（55,052 $\to$ 81,843）、独立汉字数（2,105 $\to$ 2,318）和信息熵（9.5298 $\to$ 9.5574）均呈现增长。
    
    \item \textbf{分布变化}：这是一个符合预期的增长。文本规模扩大，带来了 213 个新的独立汉字，引入了更多样化的词汇和表达。关键证据是，最常用字“的”的概率反而\textit{轻微下降}了（从 2.7301\% $\to$ 2.6710\%）。
    
    \item \textbf{结论}：当一个文本库在扩展时，如果新加入的内容主题多样，会引入新的词汇（独立汉字增加），同时稀释最常见词汇的占比。这种多样化使得汉字分布更趋于均匀，系统的不确定性随之增加，因此信息熵上升。
\end{enumerate}
                                               
\paragraph{第二阶段：从 150cn 到 200cn（熵的反常下降）}
\begin{enumerate}
    \item \textbf{数据表现}：在此阶段，总汉字数（81,843 $\to$ 108,021）和独立汉字数（2,318 $\to$ 2,478）依然在增加，但信息熵却\textit{反常下降}了（从 9.5574 $\to$ 9.5265）。
    
    \item \textbf{分布变化}：熵下降的直接原因是汉字概率分布变得更不均匀。最显著的证据是：
    \begin{itemize}
        \item 排名第一的汉字“的”，其出现概率从 2.6710\% \textbf{显著飙升}至 3.1309\%。
        \item 排名第二的“在”，概率也从 0.8431\% 上升到 0.9693\%。
    \end{itemize}
    
    \item \textbf{理论解释}：根据信息熵的定义 $H = -\sum p(x) \log p(x)$，当某个单一事件（如“的”字）的概率 $p$ 显著提高时，它对总熵的贡献会减小，导致整体系统的不确定性（熵）降低。换言之，当“的”字出现得如此频繁，系统就变得“更可预测”了。
    
    \item \textbf{主题偏移}：词频表（Top 20）的变化揭示了熵下降的根本原因——“主题偏移”。
    \begin{itemize}
        \item \texttt{150cn} 的高频词包括“行”（第9）、“生”（第13）、“学”（第20）。
        \item \texttt{200cn} 中，这些词跌出前20，取而代之的是一组主题高度集中的新词：“资”、“理”、“场”、“产”。
    \end{itemize}
    
    \item \textbf{结论}：这表明从 150cn 到 200cn 新增的文本内容，大概率是高度集中在“商业”、“经济”或“管理”领域。这种高度专业化、主题趋同的文本，不仅引入了自己领域的高频词，也导致了“的”、“在”等通用字的用法频率激增。这种“主题收敛”（Topical Convergence）使得整个文本的汉字分布变得更加倾斜和不均，最终导致了信息熵的下降，尽管文本的总量和词汇量（独立汉字数）仍在增加。
\end{enumerate}

\noindent
结论：\texttt{200cn} 文本的主题专一化，导致了其汉字使用概率分布的集中。这种集中化对信息熵的拉低效应，超过了因独立汉字数增加而带来的推高效应，最终导致了熵值的净下降。
\subsection{英文}
对于英文，我们以“单词”（Word）为单位进行统计。

\subsubsection{结果}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_100000en}
    \caption{小规模样本的单词频率和信息熵}
    \label{fig:small_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_150000en}
    \caption{中规模样本的单词频率和信息熵}
    \label{fig:medium_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_200000en}
    \caption{大规模样本的单词频率和信息熵}
    \label{fig:large_scale}
\end{figure}

\subsubsection{横向对比}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=18cm]{word_comparison_chart}
    \caption{横向对比}
    \label{fig:comparison}
\end{figure}
\paragraph{分析}
基于三组英文样本（100K/150K/200K）的统计结果（见 Top-N 表与对比柱状图），可以得到以下观察与结论：
\begin{enumerate}
    \item \textbf{信息熵：} 单词信息熵约为 \textbf{10.1900} $\to$ \textbf{10.2136} $\to$ \textbf{10.3102}，稳步上升，说明词频分布整体更加均匀、长尾词汇占比提升，即不确定性增加。
    \item \textbf{高频词结构：} Top-20 长期由功能词 \textit{and, the, a, to, of, in} 等主导。随着规模扩大，\textit{model, llm, reasoning, method, data, task} 等领域词逐步进入Top-20，显示样本主题对 AI、LLM 等研究语域的偏向。
    \item \textbf{头尾部变化：} 头部功能词的相对占比基本\textit{稳定或略有下降}，而尾部长尾扩展更明显——这是熵上升的主要来源。结合 Zipf 分析，头部略高与尾部台阶状是未做停用词处理与有限样本离散效应的共同结果。
\end{enumerate}

综上，随着语料规模的扩大，英文样本呈现出“\textit{熵上升、主题更加凸显（AI/LLM 相关词汇增多）}”的特征；这既符合语言统计的一般规律，也反映了采样来源对语域的影响。

\section{验证齐夫定律 (Zipf's Law)}
齐夫定律指出，在一个大型语料库中，任意单词的出现频率 $f$ 与其在频率表中的排名 $r$ 成反比，即：
$$ f \propto \frac{1}{r} $$
取对数后，$\log(f)$ 和 $\log(r)$ 应呈线性关系。我们仅对英文样本进行单词级别的验证。

\paragraph{验证方法}
使用 Python 实现验证逻辑：首先统计单词频率并按频率降序排列。然后计算词频与排名的对数值，通过 \texttt{numpy.polyfit} 进行线性拟合，获得斜率参数。

\subsection{小规模样本}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{zipf_verification_100000en.png}
    \caption{小规模样本的 Zipf 定律对数拟合图}
    \label{fig:zipf_small_scale_new}
\end{figure}

\subsection{中规模样本}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{zipf_verification_150000en.png}
    \caption{中规模样本的 Zipf 定律对数拟合图}
    \label{fig:zipf_medium_scale_new}
\end{figure}

\subsection{大规模样本}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{zipf_verification_200000en.png}
    \caption{大规模样本的 Zipf 定律对数拟合图}
    \label{fig:zipf_large_scale_new}
\end{figure}

\paragraph{分析}
从对数坐标的拟合结果看，三个规模的语料库均表现出词频与排名近似线性相关的特性，这与齐夫定律的预测一致。具体而言，100K、150K 和 200K 词汇语料的回归斜率分别为 -1.171 ($R^2=0.9677$)、-1.211 ($R^2=0.9682$) 和 -1.269 ($R^2=0.9686$)。

\textbf{实验结论与偏差分析：}
\begin{enumerate}
    \item \textbf{普遍符合 Zipf 分布：} 所有样本的拟合直线斜率都接近理论值 -1，同时 $R^2$ 均接近 1，表明词频与排名的对数关系具有较高的线性相关性。
    \item \textbf{高频词区域（头部偏离）：} 在所有图中，曲线的头部（高频词区域）都明显高于拟合的直线。这主要是因为语料未经预处理，大量的冠词、介词等功能词（如 "the", "of", "a"）占据了频率的顶端，其出现频率远超普通内容词，导致了分布的陡峭化。
    \item \textbf{低频词区域（尾部弯曲与台阶）：} 曲线的尾部出现了明显的向下弯曲和“台阶”状结构。这种现象源于以下几个原因：
    \begin{itemize}
        \item \textbf{有限样本效应：} 在有限的语料中，许多稀有词只出现了一次或极少数次，导致在对数坐标尾部形成离散的、水平的“台阶”。
        \item \textbf{词形未归一：} 实验未进行词形还原，导致同一词根的不同形式被计为不同词条，分散了词频。
        \item \textbf{数据离散化：} 排名越靠后，词频越低，频率值的变化不再是连续的，这种离散效应在对数图上被放大。
    \end{itemize}
\end{enumerate}

% --- 5. 文档结束 ---
\end{document}

