
\documentclass[11pt, a4paper]{ctexart} % <-- 改用 ctexart 文档类

% --- 1. 通用设置与页面边距 ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}

% --- 2. 字体与中文设置 ---
% ctexart 已经自动处理好了中文字体 (通常是宋体/黑体)，
% 我们不需要再加载 fontspec 或 babel 了。

% --- 3. 常用工具包 ---
\usepackage{amsmath}      % AMS 数学公式包
\usepackage{booktabs}     % 优化表格（三线表）
\usepackage{graphicx}     % 插入图片
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=cyan]{hyperref} % 超链接包
\usepackage{fancyhdr} % 页眉页脚宏包
\usepackage{float} % 在导言区加载宏包

% --- 4. 封面信息 ---
% ctexart 会自动处理中文标题
\title{自然语言处理第一次作业（A）}
\author{作者：张宗桪 \\ 学号：2023K8009991013}
\date{\today} % 使用当前日期

% --- 5. 文档开始 ---
\begin{document}

% --- 封面 ---
\maketitle
\thispagestyle{empty} % 封面不显示页码
\newpage % 封面后强制换页

% --- 目录 ---
\pagestyle{plain} % 目录页使用简单页眉
\tableofcontents
\newpage % 目录后强制换页

% --- 正文 ---
\pagestyle{fancy} % 正文使用自定义页眉页脚
\fancyhf{} % 清空默认设置
\fancyfoot[C]{\thepage} % 页码居中底部
\section{样本的爬取过程}

本次实验利用 Python 语言的 Scrapy 框架进行网络数据的爬取。Scrapy 是一个为了爬取网站数据、提取结构性数据而编写的高效、异步的应用框架，它允许开发者通过定义“爬虫”（Spiders）来实现复杂的抓取逻辑。

本实验的核心爬取逻辑由两个 Spider 类（'arxiv\_spider.py' 和 'sina\_tech.py'）定义，它们分别针对 $arXiv$ 和新浪新闻这两个数据源。

\subsection{$arXiv$ 论文数据爬取 (arxiv\_spider.py)}

$arXiv$ 数据源的爬取目标是获取计算机科学（CS）领域（包括人工智能 `cs.AI`、计算语言学 `cs.CL` 和计算机视觉 `cs.CV`）的最新论文文本：

\begin{enumerate}
    \item \textbf{起始点 (\texttt{parse} 方法):} 爬虫从 \texttt{start\_urls} 中定义的 \texttt{/recent} 列表页开始。在此页面，它使用 CSS 选择器 (\texttt{response.css}) 查找所有指向论文摘要页（\texttt{/abs/...}）的链接，并使用 \texttt{response.follow} 生成新的请求，将响应(response)交由 \texttt{parse\_abstract} 方法处理。

    \item \textbf{摘要爬取 (\texttt{parse\_abstract} 方法)} 
    \begin{itemize}
        \item 爬虫执行如下方案：仅提取当前页面的论文摘要。摘要文本位于 \texttt{<blockquote>} 标签中。
    \end{itemize}

    \item \textbf{内容提取 (\texttt{parse\_html\_paper} / \texttt{parse\_abstract}):} 在 \texttt{parse\_abstract} 方法中提取摘要内容。
\end{enumerate}

\subsection{新浪新闻数据爬取 (sina\_tech.py)}

\texttt{sina\_tech.py} 爬虫的逻辑相对直接。它从首页开始，在 \texttt{parse} 方法中使用 XPath (\texttt{response.xpath}) 筛选出所有符合新浪文章 URL 特征（包含 \texttt{/doc-} 和 \texttt{.shtml}）的链接。随后，它跟进这些链接，并在 \texttt{parse\_article} 方法中，使用 XPath 精确提取文章的标题、发布时间、来源和正文等结构化数据。

\subsection{核心爬取配置 (settings.py)}

为了确保爬取过程的高效、稳定和友好，在 \texttt{settings.py} 文件中进行如下参数设置：

\begin{enumerate}
    \item \textbf{友好性策略：} 为了避免对目标服务器造成过大压力，组合使用 \texttt{ROBOTSTXT\_OBEY = True}（严格遵守网站的 \texttt{robots.txt} 爬取协议）和 \texttt{DOWNLOAD\_DELAY = 1}（设置 1 秒的下载延迟）。
    \item \textbf{身份伪装：} 将 \texttt{USER\_AGENT} 设置为通用的 Chrome 浏览器标识，以模拟正常的用户访问，这是爬虫反屏蔽的常见策略。
    \item \textbf{数据编码：} 鉴于新浪新闻包含大量中文文本，明确设置 \texttt{FEED\_EXPORT\_ENCODING = "utf-8"}。
\end{enumerate}
\section{样本的清洗}
数据清洗是本实验至关重要的一步，其目的是从爬取原始文本中提取出干净、可用的纯文本内容。我们主要使用了 Python 的 BeautifulSoup 库和正则表达式 (re) 库。

\subsection{中文处理}
对于新浪新闻的中文文本，我们进行了以下清洗步骤：
\begin{enumerate}
    \item \textbf{移除特定噪音：} 移除特定的广告（如 炒股就看...）、系统残留（如 Flash Player...）和免责声明等。
    \item \textbf{移除 URL 和邮件：} 移除多种格式的作者、来源、责编等信息（如 \texttt{(来源：...)} 或 \texttt{(责编|记者...)}）。
    \item \textbf{去除无效字符：} 去除多余的空白符、换行符和特殊控制字符。
    \item \textbf{统一编码：} 所有文本统一保存为 UTF-8 编码。
\end{enumerate}

\subsection{英文处理}
对于 $arXiv$ 论文的英文文本，清洗步骤包括：
\begin{enumerate}
    \item \textbf{移除元数据：} 移除特定的 \texttt{\textbackslash s*} 标签。
    \item \textbf{文本标准化：} 使用 \texttt{cleaned\_line.lower()}将所有文本转换为小写。
    \item \textbf{移除格式化残留：} 移除如 \texttt{textbf} 或 \texttt{textit} 这样的格式前缀。
    \item \textbf{严格字符过滤：} 移除所有非字母和非空白字符（即数字和标点）。
    \item \textbf{规范化空白：} 将多个连续空白压缩为单个空格。
    \item \textbf{去重与写入：} 使用 \texttt{set} 集合 (\texttt{seen\_lines}) 检查，跳过已存在的行。
\end{enumerate}

最终，我们得到了三个不同规模的中文和英文语料库：
\begin{itemize}
    \item \textbf{小规模样本：} 中文约 150KB 文本，英文约700KB。
    \item \textbf{中规模样本：} 中文约 250KB 文本，英文约900KB。
    \item \textbf{大规模样本：} 中文约 350KB 文本，英文约1500KB。
\end{itemize}

\section{英语单词或汉字的概率和熵}
在此部分，我们分别对中文和英文样本进行信息熵的计算。信息熵的计算公式为：
$$ H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i) $$
其中 $P(x_i)$ 是一个符号（单词或汉字）出现的概率。

\subsection{中文}
对于中文，我们以“汉字”为单位进行统计。

\subsubsection{结果}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_100cn_cleaned}
    \caption{小规模样本的汉字频率和信息熵}
    \label{fig:small_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_150cn_cleaned}
    \caption{中规模样本的汉字频率和信息熵}
    \label{fig:medium_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_top_n_200cn_cleaned}
    \caption{大规模样本的汉字频率和信息熵}
    \label{fig:large_scale}
\end{figure}

\subsubsection{横向对比}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth, height=15cm]{hanzi_comparison_chart}
    \caption{横向对比}
    \label{fig:comparison}
\end{figure}
\paragraph{分析}
根据图表数据分析，\texttt{100cn\_cleaned.txt}、\texttt{150cn\_cleaned.txt} 和 \texttt{200cn\_cleaned.txt} 三个文本文件的汉字信息熵变化呈现先升后降（9.5298 $\to$ 9.5574 $\to$ 9.5265）的非线性趋势。

这一现象清晰地表明，影响信息熵的核心因素是文本的“主题内容”及其决定的“汉字概率分布”，而非文本规模本身。下面我们分阶段进行详细论证：

\paragraph{第一阶段：从 100cn 到 150cn（熵的合理增长）}
\begin{enumerate}
    \item \textbf{数据表现}：在此阶段，总汉字数（55,052 $\to$ 81,843）、独立汉字数（2,105 $\to$ 2,318）和信息熵（9.5298 $\to$ 9.5574）均呈现增长。
    
    \item \textbf{分布变化}：这是一个符合预期的增长。文本规模扩大，带来了 213 个新的独立汉字，引入了更多样化的词汇和表达。关键证据是，最常用字“的”的概率反而\textit{轻微下降}了（从 2.7301\% $\to$ 2.6710\%）。
    
    \item \textbf{结论}：当一个文本库在扩展时，如果新加入的内容主题多样，会引入新的词汇（独立汉字增加），同时稀释最常见词汇的占比。这种多样化使得汉字分布更趋于均匀，系统的不确定性随之增加，因此信息熵上升。
\end{enumerate}
                                               
\paragraph{第二阶段：从 150cn 到 200cn（熵的反常下降）}
\begin{enumerate}
    \item \textbf{数据表现}：在此阶段，总汉字数（81,843 $\to$ 108,021）和独立汉字数（2,318 $\to$ 2,478）依然在增加，但信息熵却\textit{反常下降}了（从 9.5574 $\to$ 9.5265）。
    
    \item \textbf{分布变化}：熵下降的直接原因是汉字概率分布变得更不均匀。最显著的证据是：
    \begin{itemize}
        \item 排名第一的汉字“的”，其出现概率从 2.6710\% \textbf{显著飙升}至 3.1309\%。
        \item 排名第二的“在”，概率也从 0.8431\% 上升到 0.9693\%。
    \end{itemize}
    
    \item \textbf{理论解释}：根据信息熵的定义 $H = -\sum p(x) \log p(x)$，当某个单一事件（如“的”字）的概率 $p$ 显著提高时，它对总熵的贡献会减小，导致整体系统的不确定性（熵）降低。换言之，当“的”字出现得如此频繁，系统就变得“更可预测”了。
    
    \item \textbf{主题偏移}：词频表（Top 20）的变化揭示了熵下降的根本原因——“主题偏移”。
    \begin{itemize}
        \item \texttt{150cn} 的高频词包括“行”（第9）、“生”（第13）、“学”（第20）。
        \item \texttt{200cn} 中，这些词跌出前20，取而代之的是一组主题高度集中的新词：“资”、“理”、“场”、“产”。
    \end{itemize}
    
    \item \textbf{结论}：这表明从 150cn 到 200cn 新增的文本内容，大概率是高度集中在“商业”、“经济”或“管理”领域。这种高度专业化、主题趋同的文本，不仅引入了自己领域的高频词，也导致了“的”、“在”等通用字的用法频率激增。这种“主题收敛”（Topical Convergence）使得整个文本的汉字分布变得更加倾斜和不均，最终导致了信息熵的下降，尽管文本的总量和词汇量（独立汉字数）仍在增加。
\end{enumerate}

\noindent
结论：\texttt{200cn} 文本的主题专一化，导致了其汉字使用概率分布的集中。这种集中化对信息熵的拉低效应，超过了因独立汉字数增加而带来的推高效应，最终导致了熵值的净下降。
\subsection{英文}
对于英文，我们以“单词”（Word）为单位进行统计。

\subsubsection{结果}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_100000en}
    \caption{小规模样本的单词频率和信息熵}
    \label{fig:small_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_150000en}
    \caption{中规模样本的单词频率和信息熵}
    \label{fig:medium_scale}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=15cm]{word_top_n_200000en}
    \caption{大规模样本的单词频率和信息熵}
    \label{fig:large_scale}
\end{figure}

\subsubsection{横向对比}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=18cm]{word_comparison_chart}
    \caption{横向对比}
    \label{fig:comparison}
\end{figure}
\paragraph{分析}

\section{验证齐夫定律 (Zipf's Law)}
齐夫定律指出，在一个大型语料库中，任意单词的出现频率 $f$ 与其在频率表中的排名 $r$ 成反比，即：
$$ f \propto \frac{1}{r} $$
取对数后，$\log(f)$ 和 $\log(r)$ 应呈线性关系。我们仅对英文样本进行单词级别的验证。

\subsection{小规模样本}
（分析...）

\subsection{中规模样本}
（分析...）

\subsection{大规模样本}
（分析...）
我们在大规模样本上的对数-对数坐标图（见图 \ref{fig:zipf_plot}）清晰地展示了这一线性关系，从而验证了齐夫定律。

% 这是一个图片占位符示例
\begin{figure}[htbp]
  \centering
  % 因为我们不能加载外部图片，所以使用一个方框作为占位符
  \framebox{\parbox{0.8\textwidth}{\centering
    \vspace{5cm}
    \textbf{图 1：大规模英文样本的 Zipf 定律拟合图} \\
    \small (\texttt{log(rank)} vs \texttt{log(frequency)})
    \vspace{5cm}
  }}
  \caption{Zipf 定律对数-对数图}
  \label{fig:zipf_plot}
\end{figure}

% --- 5. 文档结束 ---
\end{document}

