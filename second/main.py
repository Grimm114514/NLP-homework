import numpy as np
import random
import json
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# --- 1. å¯¼å…¥æ¨¡å— ---
from data import load_data, build_vocab_and_dataset
from training import train_all_models, load_checkpoint
from FNN import FNNModel
from RNN import RNNModel
from LSTM import LSTMModel
from train import train_model

# --- 2. ç›¸ä¼¼åº¦è®¡ç®—ä¸åˆ†æå·¥å…·å‡½æ•° ---

def get_cosine_similarity(vec1, vec2):
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0: return 0.0
    return np.dot(vec1, vec2) / (norm1 * norm2)

def analyze_embeddings(model_name, embeddings, vocab, idx_to_word, test_words):
    print(f"\n======== æ¨¡å‹åˆ†ææŠ¥å‘Š: {model_name} ========")
    for word in test_words:
        if word not in vocab: continue
        word_idx = vocab[word]
        word_vec = embeddings[word_idx]
        
        sim_scores = []
        for i in range(len(vocab)):
            if i == word_idx or i == 0: continue
            other_vec = embeddings[i]
            score = get_cosine_similarity(word_vec, other_vec)
            sim_scores.append((i, score))
        
        sim_scores.sort(key=lambda x: x[1], reverse=True)
        top_5 = sim_scores[:5] # åªçœ‹å‰5ä¸ªï¼ŒèŠ‚çœç¯‡å¹…
        
        result_str = ", ".join([f"{idx_to_word[idx]}({score:.2f})" for idx, score in top_5])
        print(f"[{word}] -> {result_str}")

def visualize_embeddings(all_embeddings, vocab, idx_to_word, num_words=50):
    """ä½¿ç”¨PCAå°†è¯å‘é‡é™ç»´åˆ°2Då¹¶å¯è§†åŒ–"""
    plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
    plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    
    num_models = len(all_embeddings)
    fig, axes = plt.subplots(1, num_models, figsize=(8*num_models, 6))
    if num_models == 1:
        axes = [axes]
    
    for idx, (model_name, embeddings) in enumerate(all_embeddings.items()):
        # ä½¿ç”¨PCAé™ç»´åˆ°2D
        pca = PCA(n_components=2)
        embeddings_2d = pca.fit_transform(embeddings[:num_words])
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        axes[idx].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6, s=100)
        
        # æ ‡æ³¨è¯è¯­
        for i in range(min(num_words, len(embeddings_2d))):
            word = idx_to_word.get(i, f'word_{i}')
            axes[idx].annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), 
                             fontsize=8, alpha=0.7)
        
        axes[idx].set_title(f'{model_name} è¯å‘é‡å¯è§†åŒ– (PCA)', fontsize=14)
        axes[idx].set_xlabel('PCAç»´åº¦1')
        axes[idx].set_ylabel('PCAç»´åº¦2')
        axes[idx].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('word_embeddings_visualization.png', dpi=150, bbox_inches='tight')
    print("\nğŸ“Š è¯å‘é‡å¯è§†åŒ–å·²ä¿å­˜è‡³: word_embeddings_visualization.png")
    plt.show()

def visualize_similarity_matrix(embeddings, vocab, idx_to_word, model_name, num_words=30):
    """ç»˜åˆ¶è¯å‘é‡ç›¸ä¼¼åº¦çŸ©é˜µçƒ­å›¾"""
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    selected_embeddings = embeddings[:num_words]
    similarity_matrix = np.zeros((num_words, num_words))
    
    for i in range(num_words):
        for j in range(num_words):
            similarity_matrix[i, j] = get_cosine_similarity(selected_embeddings[i], selected_embeddings[j])
    
    # ç»˜åˆ¶çƒ­å›¾
    plt.figure(figsize=(12, 10))
    plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')
    plt.colorbar(label='ä½™å¼¦ç›¸ä¼¼åº¦')
    
    # è®¾ç½®æ ‡ç­¾
    words = [idx_to_word.get(i, f'word_{i}') for i in range(num_words)]
    plt.xticks(range(num_words), words, rotation=90, fontsize=8)
    plt.yticks(range(num_words), words, fontsize=8)
    
    plt.title(f'{model_name} è¯å‘é‡ç›¸ä¼¼åº¦çŸ©é˜µ', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'{model_name}_similarity_matrix.png', dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š {model_name} ç›¸ä¼¼åº¦çŸ©é˜µå·²ä¿å­˜è‡³: {model_name}_similarity_matrix.png")
    plt.show()

def save_embeddings_to_json(all_embeddings, idx_to_word, filename='word_embeddings.json'):
    """å°†æ‰€æœ‰æ¨¡å‹çš„è¯å‘é‡ä¿å­˜åˆ°JSONæ–‡ä»¶"""
    output_data = {}
    
    for model_name, embeddings in all_embeddings.items():
        model_data = {}
        for idx, word in idx_to_word.items():
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            vector = embeddings[idx].tolist()
            model_data[word] = {
                'index': idx,
                'vector': vector
            }
        output_data[model_name] = model_data
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ è¯å‘é‡å·²ä¿å­˜è‡³: {filename}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_words = len(idx_to_word)
    vector_dim = len(embeddings[0])
    print(f"   åŒ…å« {len(all_embeddings)} ä¸ªæ¨¡å‹")
    print(f"   æ¯ä¸ªæ¨¡å‹ {total_words} ä¸ªè¯ï¼Œæ¯ä¸ªè¯å‘é‡ç»´åº¦: {vector_dim}")

def save_embeddings_compact(all_embeddings, idx_to_word, filename='word_embeddings_compact.json'):
    """ä¿å­˜ç´§å‡‘æ ¼å¼çš„è¯å‘é‡ï¼ˆåªä¿å­˜è¯å’Œå‘é‡ï¼‰"""
    output_data = {}
    
    for model_name, embeddings in all_embeddings.items():
        # ç®€åŒ–æ ¼å¼ï¼š{word: [vector]}
        model_data = {
            word: embeddings[idx].tolist() 
            for idx, word in idx_to_word.items()
        }
        output_data[model_name] = model_data
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ç´§å‡‘æ ¼å¼è¯å‘é‡å·²ä¿å­˜è‡³: {filename}")


# --- 3. ä¸»ç¨‹åº ---

if __name__ == "__main__":
    # é…ç½®
    FILE_PATH = 'cleaned_corpus.txt'
    MODEL_DIR = './model'
    VOCAB_SIZE = 1000
    EMBED_DIM = 10
    HIDDEN_DIM = 64
    CONTEXT_SIZE = 3
    EPOCHS = 5000
    LR = 0.01

    print("Step 1: å¤„ç†æ•°æ®...")
    raw_text = load_data(FILE_PATH)
    train_X, train_y, vocab, idx_to_word = build_vocab_and_dataset(raw_text, VOCAB_SIZE, CONTEXT_SIZE)
    print(f"è¯è¡¨å¤§å°: {len(vocab)}, æ ·æœ¬æ•°: {len(train_X)}")
    
    # æ˜¾ç¤ºè¯è¡¨ï¼ˆå‰50ä¸ªè¯ï¼‰
    print("\n[è¯è¡¨é¢„è§ˆ] å‰50ä¸ªé«˜é¢‘è¯:")
    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])[:50]
    vocab_preview = ", ".join([f"{word}({idx})" for word, idx in sorted_vocab])
    print(vocab_preview)

    print("\nStep 2: è®­ç»ƒæ‰€æœ‰æ¨¡å‹...")
    all_embeddings = train_all_models(
        train_X, train_y, 
        len(vocab), EMBED_DIM, HIDDEN_DIM, CONTEXT_SIZE,
        EPOCHS, LR, MODEL_DIR
    )

    # === æ˜¾ç¤ºè¯å‘é‡ ===
    print("\nStep 3: æ˜¾ç¤ºè¯å‘é‡...")
    for model_name, embeds in all_embeddings.items():
        print(f"\n[{model_name}æ¨¡å‹] è¯å‘é‡ç»´åº¦: {embeds.shape}")
        # æ˜¾ç¤ºå‰10ä¸ªè¯çš„è¯å‘é‡
        print(f"å‰10ä¸ªè¯çš„è¯å‘é‡:")
        for i in range(min(10, len(embeds))):
            word = idx_to_word[i]
            vec_str = np.array2string(embeds[i], precision=3, suppress_small=True, max_line_width=100)
            print(f"  {word:15s} -> {vec_str}")
    
    # === ç›¸ä¼¼åº¦åˆ†æä¸ç»“æœå¯¹æ¯” ===
    print("\nStep 4: è®¡ç®—è¯å‘é‡ç›¸ä¼¼åº¦å¹¶åˆ†æ...")
    valid_words = [w for w in vocab.keys() if w != "<UNK>"]
    test_words = random.sample(valid_words, min(10, len(valid_words)))
    
    for model_name, embeds in all_embeddings.items():
        analyze_embeddings(model_name, embeds, vocab, idx_to_word, test_words)

    # === ä¿å­˜è¯å‘é‡åˆ°JSON ===
    print("\nStep 5: ä¿å­˜è¯å‘é‡åˆ°JSONæ–‡ä»¶...")
    save_embeddings_to_json(all_embeddings, idx_to_word, 'word_embeddings.json')
    save_embeddings_compact(all_embeddings, idx_to_word, 'word_embeddings_compact.json')
    
    # === å¯è§†åŒ– ===
    print("\nStep 6: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # 1. è¯å‘é‡æ•£ç‚¹å›¾ï¼ˆPCAé™ç»´ï¼‰
    visualize_embeddings(all_embeddings, vocab, idx_to_word, num_words=50)
    
    # 2. ç›¸ä¼¼åº¦çŸ©é˜µçƒ­å›¾
    for model_name, embeds in all_embeddings.items():
        visualize_similarity_matrix(embeds, vocab, idx_to_word, model_name, num_words=30)
    
    # === æ¼”ç¤ºï¼šå¦‚ä½•åŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹ ===
    print("\nStep 7: [æ¼”ç¤º] åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹...")
    loaded_lstm = LSTMModel(len(vocab), EMBED_DIM, HIDDEN_DIM)
    load_success = load_checkpoint(loaded_lstm, "lstm_model", folder=MODEL_DIR)
    
    if load_success:
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥è¿›è¡Œç›¸ä¼¼åº¦åˆ†ææˆ–é¢„æµ‹ä»»åŠ¡ã€‚")