import requests
from bs4 import BeautifulSoup
import time
import os
import re

# === é…ç½®åŒºåŸŸ ===
# ç›®æ ‡è¯­è¨€: 'en' (è‹±è¯­) æˆ– 'zh' (ä¸­æ–‡)
# å»ºè®®é€‰ 'en'ï¼Œå› ä¸ºä½ çš„ data.py ç›®å‰åªæ”¯æŒè‹±è¯­åˆ†è¯
LANG = 'en' 

# ç›®æ ‡æ–‡ä»¶å¤§å° (å•ä½: MB)
# ä½œä¸šè¦æ±‚ä¸é«˜ï¼Œçˆ¬ 2MB - 5MB çº¯æ–‡æœ¬å°±è¶³å¤Ÿè®­ç»ƒå‡ºä¸é”™çš„æ•ˆæœäº†
TARGET_SIZE_MB = 2 

# è¾“å‡ºæ–‡ä»¶å
OUTPUT_FILE = 'corpus.txt'

# =================

def get_random_article_url(lang):
    """è·å–ç»´åŸºç™¾ç§‘éšæœºæ¡ç›®çš„URL"""
    return f"https://{lang}.wikipedia.org/wiki/Special:Random"

def clean_text(text):
    """ç®€å•çš„æ–‡æœ¬æ¸…æ´—"""
    # å»é™¤å¼•ç”¨æ ‡è®°ï¼Œå¦‚ [1], [2]
    text = re.sub(r'\[\d+\]', '', text)
    # å»é™¤å¤šä½™çš„ç©ºç™½
    text = text.strip()
    return text

def crawl_wikipedia():
    print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–ç»´åŸºç™¾ç§‘ ({LANG})ï¼Œç›®æ ‡å¤§å°: {TARGET_SIZE_MB} MB...")
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: {OUTPUT_FILE}\n")

    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ¸…ç©º
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("")

    current_size = 0
    article_count = 0
    target_bytes = TARGET_SIZE_MB * 1024 * 1024

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    while current_size < target_bytes:
        try:
            url = get_random_article_url(LANG)
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ ‡é¢˜
                title = soup.find('h1', {'id': 'firstHeading'}).text
                
                # æå–æ­£æ–‡ (ç»´åŸºç™¾ç§‘çš„æ­£æ–‡éƒ½åœ¨ <p> æ ‡ç­¾é‡Œ)
                paragraphs = soup.find_all('p')
                content = []
                for p in paragraphs:
                    text = clean_text(p.get_text())
                    if len(text) > 50: # å¿½ç•¥å¤ªçŸ­çš„æ®µè½
                        content.append(text)
                
                full_text = " ".join(content) + "\n"
                
                # åªæœ‰å½“æå–åˆ°æœ‰æ•ˆå†…å®¹æ—¶æ‰å†™å…¥
                if len(full_text) > 200:
                    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
                        f.write(full_text)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    file_size = os.path.getsize(OUTPUT_FILE)
                    current_size = file_size
                    article_count += 1
                    
                    print(f"[{article_count}] å·²çˆ¬å–: {title[:20]:<20}... | å½“å‰å¤§å°: {file_size/1024:.2f} KB")
                
            else:
                print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {response.status_code}")

            # ç¤¼è²Œçˆ¬è™«ï¼Œé˜²æ­¢è¢«å° IP
            time.sleep(1.0)

        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            time.sleep(2)

    print(f"\nâœ… çˆ¬å–å®Œæˆï¼æ€»å…±çˆ¬å–äº† {article_count} ç¯‡æ–‡ç« ã€‚")
    print(f"ğŸ“„ æ–‡ä»¶å·²ä¿å­˜ä¸º {OUTPUT_FILE}ï¼Œå¤§å°: {current_size/1024/1024:.2f} MB")
    print("ğŸš€ ç°åœ¨ä½ å¯ä»¥è¿è¡Œ main.py äº†ï¼")

if __name__ == "__main__":
    # ä½ éœ€è¦å®‰è£… requests å’Œ beautifulsoup4
    # pip install requests beautifulsoup4
    crawl_wikipedia()